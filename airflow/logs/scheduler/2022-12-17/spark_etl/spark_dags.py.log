[2022-12-17T03:25:50.748+0000] {processor.py:154} INFO - Started process (PID=206) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:25:50.750+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:25:50.787+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:25:50.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:25:51.565+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:25:52.762+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:25:52.761+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:25:53.759+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:25:53.758+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:25:54.310+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.721 seconds
[2022-12-17T03:26:05.112+0000] {processor.py:154} INFO - Started process (PID=216) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:05.114+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:26:05.170+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:05.150+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:06.050+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:08.334+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:08.333+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:26:09.218+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:09.217+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:26:10.323+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 5.330 seconds
[2022-12-17T03:26:20.962+0000] {processor.py:154} INFO - Started process (PID=226) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:20.965+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:26:20.970+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:20.969+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:21.699+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:23.380+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:23.379+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:26:23.803+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:23.802+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:26:24.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.236 seconds
[2022-12-17T03:26:34.948+0000] {processor.py:154} INFO - Started process (PID=244) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:34.965+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:26:34.993+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:34.992+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:35.533+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:36.613+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:36.612+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:26:36.981+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:36.980+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:26:37.313+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.523 seconds
[2022-12-17T03:26:47.611+0000] {processor.py:154} INFO - Started process (PID=256) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:47.615+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:26:47.619+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:47.618+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:47.754+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:48.420+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:26:58.691+0000] {processor.py:154} INFO - Started process (PID=264) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:58.736+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:26:58.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:58.742+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:58.891+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:26:59.181+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:59.180+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:26:59.327+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:26:59.326+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:26:59.503+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.859 seconds
[2022-12-17T03:27:10.357+0000] {processor.py:154} INFO - Started process (PID=274) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:10.360+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:27:10.364+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:10.363+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:10.505+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:11.457+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:11.456+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:27:11.780+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:11.775+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:27:12.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.942 seconds
[2022-12-17T03:27:22.799+0000] {processor.py:154} INFO - Started process (PID=291) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:22.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:27:22.856+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:22.827+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:23.861+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:24.623+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:24.622+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:27:25.250+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:25.249+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:27:25.846+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.164 seconds
[2022-12-17T03:27:36.845+0000] {processor.py:154} INFO - Started process (PID=303) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:36.856+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:27:36.861+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:36.860+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:37.024+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:37.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:37.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:27:37.719+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:37.718+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:27:38.121+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.304 seconds
[2022-12-17T03:27:48.544+0000] {processor.py:154} INFO - Started process (PID=313) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:48.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:27:48.632+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:48.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:48.998+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:49.236+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:49.235+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:27:49.377+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:49.376+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:27:49.601+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.098 seconds
[2022-12-17T03:27:59.834+0000] {processor.py:154} INFO - Started process (PID=323) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:59.846+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:27:59.854+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:27:59.853+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:27:59.970+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:00.210+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:00.209+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:28:00.395+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:00.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:28:00.577+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.770 seconds
[2022-12-17T03:28:11.203+0000] {processor.py:154} INFO - Started process (PID=339) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:11.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:28:11.258+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:11.256+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:11.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:12.080+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:12.061+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:28:12.395+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:12.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:28:12.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.440 seconds
[2022-12-17T03:28:22.760+0000] {processor.py:154} INFO - Started process (PID=351) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:22.806+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:28:22.811+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:22.810+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:22.905+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:23.065+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:23.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:28:23.203+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:23.202+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:28:23.338+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.592 seconds
[2022-12-17T03:28:33.655+0000] {processor.py:154} INFO - Started process (PID=359) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:33.670+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:28:33.681+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:33.680+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:33.815+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:34.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:34.545+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:28:34.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:34.733+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:28:34.861+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.247 seconds
[2022-12-17T03:28:45.189+0000] {processor.py:154} INFO - Started process (PID=369) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:45.193+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:28:45.198+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:45.197+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:45.440+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:45.617+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:45.616+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:28:45.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:45.793+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:28:45.946+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.777 seconds
[2022-12-17T03:28:56.428+0000] {processor.py:154} INFO - Started process (PID=386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:56.469+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:28:56.479+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:56.474+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:56.671+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:28:56.918+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:56.917+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:28:57.139+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:28:57.138+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:28:57.408+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.070 seconds
[2022-12-17T03:29:07.764+0000] {processor.py:154} INFO - Started process (PID=393) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:07.768+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:29:07.772+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:07.771+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:07.859+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:08.021+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:08.020+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:29:08.191+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:08.190+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:29:08.385+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.635 seconds
[2022-12-17T03:29:18.965+0000] {processor.py:154} INFO - Started process (PID=403) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:18.969+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:29:18.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:18.972+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:19.063+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:19.206+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:19.205+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:29:19.421+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:19.420+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:29:19.621+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.671 seconds
[2022-12-17T03:29:30.555+0000] {processor.py:154} INFO - Started process (PID=416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:30.573+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:29:30.577+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:30.576+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:30.786+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:31.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:31.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:29:31.530+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:31.529+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:29:31.757+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.222 seconds
[2022-12-17T03:29:42.184+0000] {processor.py:154} INFO - Started process (PID=434) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:42.223+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:29:42.227+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:42.226+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:42.459+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:43.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:43.002+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:29:43.131+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:43.130+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:29:43.253+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.100 seconds
[2022-12-17T03:29:53.750+0000] {processor.py:154} INFO - Started process (PID=444) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:53.795+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:29:53.799+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:53.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:53.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:29:54.026+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:54.025+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:29:54.158+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:29:54.157+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:29:54.309+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.575 seconds
[2022-12-17T03:30:05.229+0000] {processor.py:154} INFO - Started process (PID=454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:05.290+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:30:05.294+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:05.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:05.494+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:05.877+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:05.876+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:30:06.268+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:06.267+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:30:06.403+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.221 seconds
[2022-12-17T03:30:16.920+0000] {processor.py:154} INFO - Started process (PID=471) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:16.987+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:30:16.991+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:16.990+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:17.358+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:19.130+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:19.129+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:30:19.478+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:19.478+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:30:20.068+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.224 seconds
[2022-12-17T03:30:30.820+0000] {processor.py:154} INFO - Started process (PID=481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:30.851+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:30:30.854+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:30.853+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:31.111+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:31.334+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:31.333+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:30:31.473+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:31.472+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:30:32.080+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.308 seconds
[2022-12-17T03:30:42.681+0000] {processor.py:154} INFO - Started process (PID=491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:42.709+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:30:42.713+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:42.712+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:42.803+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:43.655+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:30:54.343+0000] {processor.py:154} INFO - Started process (PID=503) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:54.404+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:30:54.408+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:54.407+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:54.575+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:30:54.778+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:54.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:30:55.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:30:55.147+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:30:55.461+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.187 seconds
[2022-12-17T03:31:05.917+0000] {processor.py:154} INFO - Started process (PID=520) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:05.940+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:31:05.949+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:05.948+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:06.241+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:07.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:07.304+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:31:07.923+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:07.906+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:31:08.321+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.442 seconds
[2022-12-17T03:31:18.957+0000] {processor.py:154} INFO - Started process (PID=532) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:19.005+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:31:19.009+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:19.008+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:19.092+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:19.889+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:31:30.275+0000] {processor.py:154} INFO - Started process (PID=542) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:30.326+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:31:30.330+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:30.329+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:30.415+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:30.742+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:30.741+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:31:30.867+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:30.866+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:31:31.000+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.739 seconds
[2022-12-17T03:31:41.228+0000] {processor.py:154} INFO - Started process (PID=552) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:41.232+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:31:41.235+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:41.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:41.320+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:41.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:41.465+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:31:41.602+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:41.602+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:31:41.751+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.537 seconds
[2022-12-17T03:31:52.091+0000] {processor.py:154} INFO - Started process (PID=570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:52.138+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:31:52.141+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:31:52.141+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:52.335+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:31:53.386+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:32:03.589+0000] {processor.py:154} INFO - Started process (PID=580) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:03.593+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:32:03.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:03.597+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:03.684+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:03.835+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:03.834+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:32:03.953+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:03.953+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:32:04.157+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.582 seconds
[2022-12-17T03:32:14.472+0000] {processor.py:154} INFO - Started process (PID=590) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:14.476+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:32:14.479+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:14.478+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:14.595+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:14.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:14.804+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:32:14.940+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:14.934+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:32:15.456+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.998 seconds
[2022-12-17T03:32:25.830+0000] {processor.py:154} INFO - Started process (PID=607) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:25.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:32:25.844+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:25.843+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:26.028+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:26.307+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:26.305+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:32:26.579+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:26.578+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:32:26.870+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.070 seconds
[2022-12-17T03:32:37.152+0000] {processor.py:154} INFO - Started process (PID=618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:37.182+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:32:37.186+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:37.185+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:37.273+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:37.752+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:32:48.155+0000] {processor.py:154} INFO - Started process (PID=628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:48.219+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:32:48.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:48.222+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:48.461+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:48.861+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:48.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:32:49.235+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:49.234+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:32:49.379+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.262 seconds
[2022-12-17T03:32:59.876+0000] {processor.py:154} INFO - Started process (PID=638) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:32:59.920+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:32:59.929+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:32:59.926+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:00.093+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:00.474+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:00.473+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:33:00.890+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:00.889+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:33:01.143+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.286 seconds
[2022-12-17T03:33:11.977+0000] {processor.py:154} INFO - Started process (PID=656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:12.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:33:12.049+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:12.038+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:12.598+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:13.215+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:13.214+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:33:13.735+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:13.734+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:33:14.032+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.185 seconds
[2022-12-17T03:33:24.962+0000] {processor.py:154} INFO - Started process (PID=666) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:24.982+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:33:24.990+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:24.989+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:25.288+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:26.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:26.068+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:33:26.376+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:26.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:33:26.568+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.640 seconds
[2022-12-17T03:33:37.247+0000] {processor.py:154} INFO - Started process (PID=676) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:37.265+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:33:37.273+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:37.272+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:37.502+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:38.653+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:33:49.228+0000] {processor.py:154} INFO - Started process (PID=686) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:49.257+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:33:49.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:33:49.263+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:49.623+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:33:50.667+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:34:01.347+0000] {processor.py:154} INFO - Started process (PID=704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:01.394+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:34:01.403+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:01.398+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:02.041+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:03.098+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:03.097+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:34:03.351+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:03.350+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:34:03.573+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.256 seconds
[2022-12-17T03:34:14.209+0000] {processor.py:154} INFO - Started process (PID=714) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:14.236+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:34:14.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:14.239+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:14.502+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:15.419+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:15.418+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:34:15.555+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:15.553+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:34:15.692+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.499 seconds
[2022-12-17T03:34:26.680+0000] {processor.py:154} INFO - Started process (PID=724) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:26.697+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:34:26.701+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:26.700+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:26.868+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:27.011+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:27.010+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:34:27.129+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:27.128+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:34:27.248+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.584 seconds
[2022-12-17T03:34:37.646+0000] {processor.py:154} INFO - Started process (PID=734) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:37.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:34:37.681+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:37.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:37.800+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:38.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:38.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:34:38.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:38.579+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:34:38.999+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.420 seconds
[2022-12-17T03:34:49.451+0000] {processor.py:154} INFO - Started process (PID=752) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:49.478+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:34:49.490+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:49.489+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:49.654+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:34:49.801+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:49.800+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:34:50.151+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:34:50.150+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:34:50.509+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.086 seconds
[2022-12-17T03:35:00.946+0000] {processor.py:154} INFO - Started process (PID=762) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:00.969+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:35:00.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:00.972+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:01.057+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:01.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:01.224+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:35:01.349+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:01.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:35:01.499+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.569 seconds
[2022-12-17T03:35:11.778+0000] {processor.py:154} INFO - Started process (PID=772) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:11.829+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:35:11.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:11.832+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:11.913+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:12.044+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:12.043+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:35:12.171+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:12.170+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:35:12.314+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.550 seconds
[2022-12-17T03:35:22.540+0000] {processor.py:154} INFO - Started process (PID=782) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:22.566+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:35:22.570+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:22.569+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:22.676+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:22.858+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:22.857+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:35:23.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:23.002+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:35:23.115+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.589 seconds
[2022-12-17T03:35:33.625+0000] {processor.py:154} INFO - Started process (PID=799) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:33.680+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:35:33.696+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:33.690+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:33.949+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:34.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:34.743+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:35:34.950+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:34.949+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:35:35.145+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.560 seconds
[2022-12-17T03:35:45.500+0000] {processor.py:154} INFO - Started process (PID=809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:45.525+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:35:45.538+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:45.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:45.636+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:45.790+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:45.789+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:35:45.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:45.899+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:35:46.046+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.574 seconds
[2022-12-17T03:35:56.421+0000] {processor.py:154} INFO - Started process (PID=819) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:56.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:35:56.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:56.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:56.600+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:35:56.996+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:56.994+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:35:57.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:35:57.165+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:35:57.281+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.874 seconds
[2022-12-17T03:36:07.756+0000] {processor.py:154} INFO - Started process (PID=836) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:07.809+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:36:07.821+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:07.820+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:08.010+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:08.356+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:08.355+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:36:08.559+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:08.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:36:08.717+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.014 seconds
[2022-12-17T03:36:19.109+0000] {processor.py:154} INFO - Started process (PID=847) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:19.133+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:36:19.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:19.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:19.224+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:19.387+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:19.386+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:36:19.519+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:19.519+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:36:19.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.567 seconds
[2022-12-17T03:36:29.964+0000] {processor.py:154} INFO - Started process (PID=857) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:30.015+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:36:30.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:30.018+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:30.119+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:30.255+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:30.254+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:36:30.385+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:30.384+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:36:30.540+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.598 seconds
[2022-12-17T03:36:40.791+0000] {processor.py:154} INFO - Started process (PID=867) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:40.842+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:36:40.846+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:40.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:40.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:41.127+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:41.126+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:36:41.281+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:41.281+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:36:41.412+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.637 seconds
[2022-12-17T03:36:51.728+0000] {processor.py:154} INFO - Started process (PID=884) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:51.732+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:36:51.736+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:51.735+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:51.968+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:36:52.728+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:52.727+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:36:52.872+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:36:52.872+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:36:53.059+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.346 seconds
[2022-12-17T03:37:03.293+0000] {processor.py:154} INFO - Started process (PID=894) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:03.314+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:37:03.318+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:03.317+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:03.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:04.634+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:04.633+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:37:04.772+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:04.767+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:37:04.964+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.687 seconds
[2022-12-17T03:37:15.317+0000] {processor.py:154} INFO - Started process (PID=904) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:15.347+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:37:15.350+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:15.349+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:15.456+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:15.623+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:15.622+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:37:15.760+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:15.759+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:37:15.873+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.578 seconds
[2022-12-17T03:37:26.152+0000] {processor.py:154} INFO - Started process (PID=914) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:26.195+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:37:26.199+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:26.198+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:26.301+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:27.079+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:37:37.466+0000] {processor.py:154} INFO - Started process (PID=932) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:37.497+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:37:37.509+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:37.508+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:37.648+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:38.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:38.037+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:37:38.347+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:38.346+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:37:38.626+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.226 seconds
[2022-12-17T03:37:49.000+0000] {processor.py:154} INFO - Started process (PID=942) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:49.032+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:37:49.036+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:49.035+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:49.134+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:37:49.494+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:49.493+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:37:49.633+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:37:49.632+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:37:49.807+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.828 seconds
[2022-12-17T03:38:00.140+0000] {processor.py:154} INFO - Started process (PID=952) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:00.169+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:38:00.173+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:00.172+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:00.346+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:00.690+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:00.689+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:38:00.848+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:00.848+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:38:00.974+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.886 seconds
[2022-12-17T03:38:11.557+0000] {processor.py:154} INFO - Started process (PID=968) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:11.621+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:38:11.648+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:11.648+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:12.058+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:12.301+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:12.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:38:12.543+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:12.542+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:38:12.761+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.267 seconds
[2022-12-17T03:38:23.332+0000] {processor.py:154} INFO - Started process (PID=979) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:23.360+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:38:23.374+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:23.373+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:23.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:23.919+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:23.918+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:38:24.049+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:24.048+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:38:24.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.890 seconds
[2022-12-17T03:38:34.709+0000] {processor.py:154} INFO - Started process (PID=989) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:34.728+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:38:34.733+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:34.732+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:34.875+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:35.107+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:35.106+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:38:35.266+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:35.266+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:38:35.368+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.684 seconds
[2022-12-17T03:38:45.668+0000] {processor.py:154} INFO - Started process (PID=999) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:45.754+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:38:45.758+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:45.757+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:46.012+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:46.309+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:46.308+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:38:46.504+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:46.503+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:38:46.627+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.091 seconds
[2022-12-17T03:38:57.084+0000] {processor.py:154} INFO - Started process (PID=1018) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:57.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:38:57.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:57.215+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:57.794+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:38:58.901+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:58.899+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:38:59.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:38:59.062+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:38:59.262+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.227 seconds
[2022-12-17T03:39:09.431+0000] {processor.py:154} INFO - Started process (PID=1028) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:09.454+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:39:09.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:09.457+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:09.681+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:10.403+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:10.394+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:39:10.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:10.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:39:10.682+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.269 seconds
[2022-12-17T03:39:21.094+0000] {processor.py:154} INFO - Started process (PID=1038) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:21.122+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:39:21.131+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:21.129+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:21.254+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:22.663+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:22.662+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:39:22.780+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:22.779+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:39:22.893+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.843 seconds
[2022-12-17T03:39:33.108+0000] {processor.py:154} INFO - Started process (PID=1056) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:33.133+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:39:33.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:33.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:33.306+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:33.510+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:33.508+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:39:33.683+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:33.682+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:39:33.893+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.802 seconds
[2022-12-17T03:39:44.327+0000] {processor.py:154} INFO - Started process (PID=1066) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:44.358+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:39:44.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:44.361+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:44.462+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:45.312+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:39:55.713+0000] {processor.py:154} INFO - Started process (PID=1076) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:55.762+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:39:55.767+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:39:55.766+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:55.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:39:57.128+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:40:07.302+0000] {processor.py:154} INFO - Started process (PID=1086) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:07.352+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:40:07.359+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:07.358+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:07.457+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:08.578+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:08.577+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:40:08.714+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:08.713+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:40:08.820+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.533 seconds
[2022-12-17T03:40:19.265+0000] {processor.py:154} INFO - Started process (PID=1105) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:19.319+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:40:19.326+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:19.325+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:19.666+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:20.142+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:20.141+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:40:20.341+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:20.340+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:40:20.487+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.242 seconds
[2022-12-17T03:40:30.598+0000] {processor.py:154} INFO - Started process (PID=1115) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:30.666+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:40:30.671+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:30.670+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:30.757+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:30.942+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:30.941+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:40:31.190+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:31.189+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:40:31.320+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.737 seconds
[2022-12-17T03:40:41.502+0000] {processor.py:154} INFO - Started process (PID=1125) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:41.525+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:40:41.530+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:41.529+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:41.627+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:41.776+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:41.775+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:40:41.894+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:41.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:40:41.998+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.514 seconds
[2022-12-17T03:40:52.347+0000] {processor.py:154} INFO - Started process (PID=1142) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:52.373+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:40:52.405+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:52.404+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:52.533+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:40:53.970+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:53.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:40:54.300+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:40:54.294+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:40:54.429+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.119 seconds
[2022-12-17T03:41:04.868+0000] {processor.py:154} INFO - Started process (PID=1153) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:04.915+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:41:04.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:04.921+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:05.012+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:05.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:05.869+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:41:06.303+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:06.302+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:41:06.496+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.645 seconds
[2022-12-17T03:41:17.120+0000] {processor.py:154} INFO - Started process (PID=1163) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:17.148+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:41:17.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:17.151+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:17.259+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:17.611+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:17.610+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:41:17.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:17.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:41:17.937+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.832 seconds
[2022-12-17T03:41:28.182+0000] {processor.py:154} INFO - Started process (PID=1173) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:28.228+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:41:28.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:28.232+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:28.315+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:29.193+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:29.191+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:41:29.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:29.309+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:41:29.421+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.257 seconds
[2022-12-17T03:41:39.832+0000] {processor.py:154} INFO - Started process (PID=1191) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:39.907+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:41:39.919+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:39.914+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:40.021+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:40.183+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:40.182+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:41:40.335+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:40.334+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:41:40.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.866 seconds
[2022-12-17T03:41:51.099+0000] {processor.py:154} INFO - Started process (PID=1201) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:51.154+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:41:51.160+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:51.158+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:51.248+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:41:51.450+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:51.449+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:41:51.564+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:41:51.563+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:41:51.673+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.588 seconds
[2022-12-17T03:42:02.331+0000] {processor.py:154} INFO - Started process (PID=1211) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:02.379+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:42:02.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:02.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:02.467+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:02.607+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:02.606+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:42:02.727+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:02.726+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:42:02.878+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.561 seconds
[2022-12-17T03:42:13.123+0000] {processor.py:154} INFO - Started process (PID=1229) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:13.150+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:42:13.158+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:13.157+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:13.255+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:13.416+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:13.415+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:42:13.594+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:13.593+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:42:13.802+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.696 seconds
[2022-12-17T03:42:24.168+0000] {processor.py:154} INFO - Started process (PID=1240) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:24.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:42:24.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:24.224+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:24.326+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:24.477+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:24.476+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:42:24.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:24.588+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:42:24.695+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.542 seconds
[2022-12-17T03:42:35.358+0000] {processor.py:154} INFO - Started process (PID=1250) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:35.362+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:42:35.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:35.365+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:35.449+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:35.709+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:35.709+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:42:35.901+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:35.901+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:42:36.071+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.728 seconds
[2022-12-17T03:42:46.304+0000] {processor.py:154} INFO - Started process (PID=1260) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:46.349+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:42:46.362+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:46.361+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:46.459+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:46.599+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:46.598+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:42:46.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:46.709+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:42:46.811+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.522 seconds
[2022-12-17T03:42:57.154+0000] {processor.py:154} INFO - Started process (PID=1279) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:57.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:42:57.196+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:57.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:57.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:42:57.462+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:57.461+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:42:57.586+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:42:57.585+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:42:57.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.604 seconds
[2022-12-17T03:43:08.127+0000] {processor.py:154} INFO - Started process (PID=1289) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:08.155+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:43:08.159+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:08.158+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:08.249+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:09.008+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:09.007+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:43:09.126+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:09.125+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:43:09.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.127 seconds
[2022-12-17T03:43:19.497+0000] {processor.py:154} INFO - Started process (PID=1299) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:19.524+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:43:19.539+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:19.538+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:19.662+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:19.822+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:19.821+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:43:19.933+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:19.933+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:43:20.077+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.598 seconds
[2022-12-17T03:43:30.316+0000] {processor.py:154} INFO - Started process (PID=1309) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:30.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:43:30.376+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:30.375+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:30.458+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:31.173+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:43:41.887+0000] {processor.py:154} INFO - Started process (PID=1327) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:41.891+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:43:41.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:41.898+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:42.064+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:42.270+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:42.269+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:43:42.507+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:42.506+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:43:42.716+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.877 seconds
[2022-12-17T03:43:52.992+0000] {processor.py:154} INFO - Started process (PID=1337) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:52.995+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:43:52.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:52.998+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:53.080+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:43:53.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:53.209+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:43:53.356+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:43:53.355+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:43:53.466+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.489 seconds
[2022-12-17T03:44:03.901+0000] {processor.py:154} INFO - Started process (PID=1347) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:03.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:44:03.980+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:03.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:04.128+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:04.258+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:04.257+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:44:04.498+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:04.497+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:44:04.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.777 seconds
[2022-12-17T03:44:14.914+0000] {processor.py:154} INFO - Started process (PID=1362) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:14.929+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:44:14.933+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:14.932+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:15.148+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:15.337+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:15.335+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:44:15.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:15.466+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:44:15.744+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.867 seconds
[2022-12-17T03:44:26.281+0000] {processor.py:154} INFO - Started process (PID=1374) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:26.285+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:44:26.289+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:26.288+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:26.375+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:27.565+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:44:37.972+0000] {processor.py:154} INFO - Started process (PID=1384) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:37.977+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:44:37.981+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:37.980+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:38.074+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:38.256+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:38.255+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:44:38.406+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:38.406+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:44:38.565+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.611 seconds
[2022-12-17T03:44:48.903+0000] {processor.py:154} INFO - Started process (PID=1394) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:48.907+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:44:48.911+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:48.910+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:48.998+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:49.140+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:49.140+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:44:49.276+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:49.275+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:44:49.422+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-17T03:44:59.549+0000] {processor.py:154} INFO - Started process (PID=1413) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:59.608+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:44:59.616+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:44:59.615+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:44:59.821+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:00.191+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:00.190+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:45:00.329+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:00.328+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:45:00.529+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.995 seconds
[2022-12-17T03:45:10.982+0000] {processor.py:154} INFO - Started process (PID=1423) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:10.986+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:45:10.990+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:10.989+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:11.080+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:11.216+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:11.215+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:45:11.328+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:11.327+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:45:11.465+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.498 seconds
[2022-12-17T03:45:21.691+0000] {processor.py:154} INFO - Started process (PID=1433) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:21.695+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:45:21.700+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:21.699+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:21.793+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:21.929+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:21.928+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:45:22.051+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:22.050+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:45:22.188+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.512 seconds
[2022-12-17T03:45:32.966+0000] {processor.py:154} INFO - Started process (PID=1443) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:32.995+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:45:32.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:32.998+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:33.100+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:33.323+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:33.322+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:45:33.434+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:33.434+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:45:33.574+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.623 seconds
[2022-12-17T03:45:44.085+0000] {processor.py:154} INFO - Started process (PID=1460) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:44.131+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:45:44.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:44.136+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:44.339+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:44.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:44.658+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:45:44.866+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:44.865+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:45:45.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.157 seconds
[2022-12-17T03:45:55.563+0000] {processor.py:154} INFO - Started process (PID=1470) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:55.588+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:45:55.592+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:55.592+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:55.740+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:45:56.400+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:56.397+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:45:56.562+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:45:56.561+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:45:56.685+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.138 seconds
[2022-12-17T03:46:07.011+0000] {processor.py:154} INFO - Started process (PID=1480) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:07.015+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:46:07.020+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:07.018+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:07.109+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:07.500+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:07.499+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:46:07.623+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:07.622+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:46:07.850+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.854 seconds
[2022-12-17T03:46:18.254+0000] {processor.py:154} INFO - Started process (PID=1497) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:18.273+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:46:18.286+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:18.285+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:18.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:18.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:18.801+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:46:19.046+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:19.046+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:46:19.298+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.084 seconds
[2022-12-17T03:46:29.518+0000] {processor.py:154} INFO - Started process (PID=1506) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:29.522+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:46:29.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:29.526+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:29.610+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:29.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:29.792+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:46:29.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:29.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:46:30.160+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.657 seconds
[2022-12-17T03:46:40.505+0000] {processor.py:154} INFO - Started process (PID=1516) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:40.524+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:46:40.528+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:40.527+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:40.627+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:40.934+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:40.933+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:46:41.186+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:41.185+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:46:41.408+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.919 seconds
[2022-12-17T03:46:51.651+0000] {processor.py:154} INFO - Started process (PID=1526) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:51.654+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:46:51.658+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:51.657+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:51.770+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:46:51.941+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:51.940+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:46:52.077+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:46:52.076+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:46:52.237+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.604 seconds
[2022-12-17T03:47:02.619+0000] {processor.py:154} INFO - Started process (PID=1544) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:02.648+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:47:02.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:02.651+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:02.868+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:03.041+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:03.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:47:03.260+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:03.259+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:47:03.489+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.918 seconds
[2022-12-17T03:47:13.884+0000] {processor.py:154} INFO - Started process (PID=1553) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:13.922+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:47:13.930+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:13.929+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:14.043+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:14.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:14.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:47:14.382+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:14.381+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:47:14.528+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-17T03:47:24.865+0000] {processor.py:154} INFO - Started process (PID=1561) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:24.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:47:24.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:24.877+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:25.012+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:25.156+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:25.155+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:47:25.265+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:25.264+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:47:25.420+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.599 seconds
[2022-12-17T03:47:35.708+0000] {processor.py:154} INFO - Started process (PID=1571) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:35.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:47:35.716+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:35.715+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:35.902+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:36.904+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:36.903+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:47:37.039+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:37.038+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:47:37.217+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.527 seconds
[2022-12-17T03:47:47.659+0000] {processor.py:154} INFO - Started process (PID=1588) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:47.674+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:47:47.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:47:47.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:47.792+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:47:49.308+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:48:00.168+0000] {processor.py:154} INFO - Started process (PID=1598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:00.194+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:48:00.200+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:00.198+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:00.285+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:00.776+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:00.775+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:48:00.901+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:00.900+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:48:01.049+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.895 seconds
[2022-12-17T03:48:11.565+0000] {processor.py:154} INFO - Started process (PID=1608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:11.575+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:48:11.579+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:11.578+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:11.692+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:12.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:12.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:48:12.465+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:12.465+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:48:12.679+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.139 seconds
[2022-12-17T03:48:22.954+0000] {processor.py:154} INFO - Started process (PID=1626) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:22.978+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:48:22.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:22.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:23.092+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:23.265+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:23.264+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:48:23.438+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:23.437+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:48:23.628+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.691 seconds
[2022-12-17T03:48:33.910+0000] {processor.py:154} INFO - Started process (PID=1636) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:33.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:48:33.997+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:33.996+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:34.084+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:34.707+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:34.706+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:48:34.819+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:34.818+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:48:34.935+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.042 seconds
[2022-12-17T03:48:45.169+0000] {processor.py:154} INFO - Started process (PID=1646) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:45.195+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:48:45.200+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:45.198+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:45.346+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:45.895+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:45.894+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:48:46.007+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:46.006+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:48:46.146+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.024 seconds
[2022-12-17T03:48:56.447+0000] {processor.py:154} INFO - Started process (PID=1656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:56.577+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:48:56.585+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:56.584+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:56.901+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:48:57.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:57.242+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:48:57.506+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:48:57.505+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:48:57.664+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.254 seconds
[2022-12-17T03:49:08.068+0000] {processor.py:154} INFO - Started process (PID=1675) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:08.133+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:49:08.138+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:08.137+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:08.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:08.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:08.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:49:08.782+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:08.778+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:49:09.020+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.966 seconds
[2022-12-17T03:49:19.305+0000] {processor.py:154} INFO - Started process (PID=1685) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:19.358+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:49:19.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:19.362+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:19.445+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:19.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:19.579+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:49:19.691+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:19.690+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:49:19.834+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.544 seconds
[2022-12-17T03:49:30.141+0000] {processor.py:154} INFO - Started process (PID=1695) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:30.162+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:49:30.167+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:30.166+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:30.249+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:30.385+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:30.384+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:49:30.496+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:30.496+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:49:30.659+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.533 seconds
[2022-12-17T03:49:40.851+0000] {processor.py:154} INFO - Started process (PID=1713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:40.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:49:40.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:40.871+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:41.126+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:42.229+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:49:52.759+0000] {processor.py:154} INFO - Started process (PID=1723) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:52.789+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:49:52.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:49:52.792+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:52.879+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:49:53.655+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:50:04.039+0000] {processor.py:154} INFO - Started process (PID=1733) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:04.043+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:50:04.047+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:04.046+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:04.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:04.259+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:04.258+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:50:04.375+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:04.374+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:50:04.507+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.482 seconds
[2022-12-17T03:50:15.054+0000] {processor.py:154} INFO - Started process (PID=1743) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:15.119+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:50:15.123+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:15.122+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:15.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:15.692+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:15.691+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:50:15.822+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:15.821+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:50:15.934+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.918 seconds
[2022-12-17T03:50:26.470+0000] {processor.py:154} INFO - Started process (PID=1761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:26.475+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:50:26.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:26.526+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:26.848+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:27.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:27.451+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:50:27.832+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:27.831+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:50:28.444+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.002 seconds
[2022-12-17T03:50:38.796+0000] {processor.py:154} INFO - Started process (PID=1771) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:38.909+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:50:38.921+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:38.920+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:39.200+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:40.340+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:40.339+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:50:40.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:40.483+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:50:40.628+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.885 seconds
[2022-12-17T03:50:50.891+0000] {processor.py:154} INFO - Started process (PID=1781) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:50.952+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:50:50.964+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:50.964+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:51.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:50:51.932+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:51.922+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:50:52.076+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:50:52.075+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:50:52.197+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.379 seconds
[2022-12-17T03:51:02.507+0000] {processor.py:154} INFO - Started process (PID=1791) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:02.536+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:51:02.548+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:02.547+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:02.789+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:04.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:04.164+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:51:04.286+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:04.286+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:51:04.427+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.969 seconds
[2022-12-17T03:51:15.478+0000] {processor.py:154} INFO - Started process (PID=1809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:15.608+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:51:15.621+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:15.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:16.039+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:18.499+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:18.498+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:51:18.932+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:18.932+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:51:19.093+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.731 seconds
[2022-12-17T03:51:29.744+0000] {processor.py:154} INFO - Started process (PID=1819) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:29.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:51:29.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:29.783+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:30.134+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:30.970+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:30.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:51:31.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:31.120+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:51:31.228+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.590 seconds
[2022-12-17T03:51:42.021+0000] {processor.py:154} INFO - Started process (PID=1829) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:42.045+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:51:42.049+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:42.048+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:42.363+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:42.678+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:42.677+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:51:42.825+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:42.824+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:51:42.943+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.023 seconds
[2022-12-17T03:51:53.290+0000] {processor.py:154} INFO - Started process (PID=1839) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:53.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:51:53.344+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:53.343+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:53.446+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:51:53.630+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:53.629+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:51:53.860+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:51:53.855+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:51:54.262+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.989 seconds
[2022-12-17T03:52:04.933+0000] {processor.py:154} INFO - Started process (PID=1857) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:04.989+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:52:04.992+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:04.991+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:05.512+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:07.635+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:07.634+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:52:07.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:07.877+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:52:08.074+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.215 seconds
[2022-12-17T03:52:18.660+0000] {processor.py:154} INFO - Started process (PID=1867) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:18.664+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:52:18.667+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:18.666+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:18.762+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:19.410+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:19.409+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:52:19.678+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:19.677+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:52:19.939+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.297 seconds
[2022-12-17T03:52:30.233+0000] {processor.py:154} INFO - Started process (PID=1877) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:30.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:52:30.249+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:30.248+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:30.343+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:30.506+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:30.505+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:52:30.650+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:30.650+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:52:30.793+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.582 seconds
[2022-12-17T03:52:41.074+0000] {processor.py:154} INFO - Started process (PID=1887) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:41.120+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:52:41.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:41.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:41.214+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:42.411+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:52:52.885+0000] {processor.py:154} INFO - Started process (PID=1905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:52.917+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:52:52.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:52.921+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:53.133+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:52:53.721+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:53.720+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:52:53.913+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:52:53.912+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:52:54.123+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.284 seconds
[2022-12-17T03:53:04.423+0000] {processor.py:154} INFO - Started process (PID=1915) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:04.464+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:53:04.472+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:04.471+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:04.554+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:05.164+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:05.163+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:53:05.282+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:05.281+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:53:05.423+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.015 seconds
[2022-12-17T03:53:15.620+0000] {processor.py:154} INFO - Started process (PID=1925) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:15.670+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:53:15.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:15.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:15.757+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:16.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:16.309+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:53:16.521+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:16.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:53:16.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.111 seconds
[2022-12-17T03:53:26.991+0000] {processor.py:154} INFO - Started process (PID=1935) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:27.040+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:53:27.044+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:27.043+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:27.133+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:27.301+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:27.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:53:27.449+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:27.448+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:53:27.575+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.600 seconds
[2022-12-17T03:53:37.771+0000] {processor.py:154} INFO - Started process (PID=1953) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:37.799+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:53:37.808+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:37.802+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:37.959+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:38.279+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:38.278+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:53:38.594+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:38.593+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:53:38.754+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.009 seconds
[2022-12-17T03:53:49.126+0000] {processor.py:154} INFO - Started process (PID=1963) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:49.157+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:53:49.162+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:49.161+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:49.246+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:49.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:49.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:53:49.502+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:49.501+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:53:49.636+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.524 seconds
[2022-12-17T03:53:59.892+0000] {processor.py:154} INFO - Started process (PID=1973) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:53:59.940+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:53:59.945+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:53:59.944+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:00.044+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:00.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:00.221+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:54:00.423+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:00.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:54:00.527+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.651 seconds
[2022-12-17T03:54:10.871+0000] {processor.py:154} INFO - Started process (PID=1990) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:10.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:54:10.932+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:10.931+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:11.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:11.473+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:11.472+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:54:11.599+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:11.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:54:11.766+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.914 seconds
[2022-12-17T03:54:22.261+0000] {processor.py:154} INFO - Started process (PID=2001) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:22.300+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:54:22.304+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:22.303+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:22.393+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:22.533+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:22.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:54:22.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:22.658+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:54:22.830+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.595 seconds
[2022-12-17T03:54:33.094+0000] {processor.py:154} INFO - Started process (PID=2011) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:33.127+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:54:33.131+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:33.130+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:33.236+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:33.449+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:33.448+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:54:33.599+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:33.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:54:33.747+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.671 seconds
[2022-12-17T03:54:44.333+0000] {processor.py:154} INFO - Started process (PID=2021) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:44.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:54:44.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:44.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:44.824+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:45.359+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:45.357+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:54:45.577+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:45.576+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:54:45.820+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.549 seconds
[2022-12-17T03:54:56.283+0000] {processor.py:154} INFO - Started process (PID=2039) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:56.396+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:54:56.400+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:56.399+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:56.840+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:54:57.907+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:57.905+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:54:58.248+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:54:58.242+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:54:58.549+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.344 seconds
[2022-12-17T03:55:09.314+0000] {processor.py:154} INFO - Started process (PID=2049) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:09.358+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:55:09.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:09.362+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:09.555+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:10.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:10.013+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:55:10.265+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:10.264+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:55:10.480+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.206 seconds
[2022-12-17T03:55:20.741+0000] {processor.py:154} INFO - Started process (PID=2059) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:20.793+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:55:20.798+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:20.797+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:20.946+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:21.924+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:21.923+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:55:22.187+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:22.182+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:55:22.428+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.711 seconds
[2022-12-17T03:55:32.747+0000] {processor.py:154} INFO - Started process (PID=2069) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:32.750+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:55:32.755+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:32.754+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:32.875+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:33.141+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:33.140+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:55:33.298+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:33.294+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:55:33.544+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.844 seconds
[2022-12-17T03:55:44.416+0000] {processor.py:154} INFO - Started process (PID=2087) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:44.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:55:44.445+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:44.445+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:44.705+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:55:45.374+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:45.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:55:46.006+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:55:46.005+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:55:46.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.926 seconds
[2022-12-17T03:56:42.200+0000] {processor.py:154} INFO - Started process (PID=172) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:56:42.222+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:56:42.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:56:42.225+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:56:42.429+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:56:43.529+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:56:43.527+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:56:43.774+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:56:43.773+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:56:43.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.741 seconds
[2022-12-17T03:56:54.321+0000] {processor.py:154} INFO - Started process (PID=185) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:56:54.349+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:56:54.364+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:56:54.358+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:56:54.472+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:56:55.472+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:57:06.138+0000] {processor.py:154} INFO - Started process (PID=196) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:06.194+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:57:06.198+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:06.197+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:06.369+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:07.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:07.136+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:57:07.284+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:07.279+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:57:07.433+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.384 seconds
[2022-12-17T03:57:17.890+0000] {processor.py:154} INFO - Started process (PID=215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:17.906+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:57:17.910+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:17.909+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:18.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:19.088+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:57:29.467+0000] {processor.py:154} INFO - Started process (PID=225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:29.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:57:29.500+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:29.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:29.596+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:29.841+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:29.841+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:57:29.979+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:29.978+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:57:30.126+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.673 seconds
[2022-12-17T03:57:40.274+0000] {processor.py:154} INFO - Started process (PID=235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:40.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:57:40.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:40.304+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:40.435+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:41.229+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:41.228+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:57:41.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:41.400+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:57:41.569+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.318 seconds
[2022-12-17T03:57:51.985+0000] {processor.py:154} INFO - Started process (PID=252) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:52.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:57:52.021+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:52.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:52.225+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:57:52.738+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:52.734+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:57:53.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:57:53.190+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:57:53.496+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.547 seconds
[2022-12-17T03:58:04.325+0000] {processor.py:154} INFO - Started process (PID=261) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:04.341+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:58:04.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:04.356+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:04.981+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:05.864+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:05.859+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:58:06.336+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:06.335+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:58:06.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.663 seconds
[2022-12-17T03:58:17.757+0000] {processor.py:154} INFO - Started process (PID=271) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:17.766+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:58:17.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:17.769+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:18.132+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:18.453+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:18.452+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:58:18.679+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:18.678+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:58:18.898+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.232 seconds
[2022-12-17T03:58:29.226+0000] {processor.py:154} INFO - Started process (PID=281) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:29.252+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:58:29.267+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:29.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:29.614+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:30.178+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:30.177+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:58:30.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:30.481+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:58:30.725+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.585 seconds
[2022-12-17T03:58:41.109+0000] {processor.py:154} INFO - Started process (PID=291) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:41.122+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:58:41.135+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:41.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:41.532+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:42.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:42.263+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:58:42.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:42.766+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:58:43.513+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.479 seconds
[2022-12-17T03:58:54.278+0000] {processor.py:154} INFO - Started process (PID=309) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:54.297+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:58:54.309+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:58:54.308+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:54.620+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:58:55.779+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:59:06.430+0000] {processor.py:154} INFO - Started process (PID=319) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:06.457+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:59:06.461+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:06.460+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:06.570+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:06.993+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:06.992+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:59:07.313+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:07.312+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:59:07.621+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.251 seconds
[2022-12-17T03:59:18.021+0000] {processor.py:154} INFO - Started process (PID=329) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:18.038+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:59:18.057+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:18.056+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:18.338+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:19.933+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T03:59:30.762+0000] {processor.py:154} INFO - Started process (PID=348) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:30.778+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:59:30.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:30.793+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:31.368+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:32.638+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:32.637+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:59:32.983+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:32.978+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:59:33.279+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.620 seconds
[2022-12-17T03:59:43.801+0000] {processor.py:154} INFO - Started process (PID=359) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:43.818+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:59:43.826+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:43.825+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:44.215+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:45.998+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:45.997+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:59:46.324+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:46.319+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:59:46.653+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.960 seconds
[2022-12-17T03:59:57.123+0000] {processor.py:154} INFO - Started process (PID=369) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:57.136+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T03:59:57.145+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:57.144+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:57.263+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T03:59:58.703+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:58.693+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T03:59:59.128+0000] {logging_mixin.py:137} INFO - [2022-12-17T03:59:59.123+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T03:59:59.348+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.249 seconds
[2022-12-17T04:00:09.576+0000] {processor.py:154} INFO - Started process (PID=379) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:09.601+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:00:09.609+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:09.608+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:09.710+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:10.182+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:10.181+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:00:10.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:10.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:00:10.540+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.978 seconds
[2022-12-17T04:00:20.976+0000] {processor.py:154} INFO - Started process (PID=397) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:21.017+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:00:21.026+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:21.025+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:21.253+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:22.792+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:22.791+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:00:23.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:23.014+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:00:23.241+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.288 seconds
[2022-12-17T04:00:33.735+0000] {processor.py:154} INFO - Started process (PID=407) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:33.763+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:00:33.767+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:33.766+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:33.851+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:34.506+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:34.505+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:00:34.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:34.664+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:00:34.792+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.070 seconds
[2022-12-17T04:00:45.039+0000] {processor.py:154} INFO - Started process (PID=417) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:45.088+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:00:45.092+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:45.091+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:45.180+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:45.914+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:45.913+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:00:46.042+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:46.041+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:00:46.172+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.146 seconds
[2022-12-17T04:00:56.294+0000] {processor.py:154} INFO - Started process (PID=427) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:56.345+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:00:56.350+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:56.349+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:56.437+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:00:57.323+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:57.322+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:00:57.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:00:57.451+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:00:57.599+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.318 seconds
[2022-12-17T04:01:08.050+0000] {processor.py:154} INFO - Started process (PID=444) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:08.079+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:01:08.083+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:08.082+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:08.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:08.579+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:08.577+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:01:08.780+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:08.779+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:01:08.970+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.941 seconds
[2022-12-17T04:01:19.385+0000] {processor.py:154} INFO - Started process (PID=454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:19.396+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:01:19.406+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:19.405+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:19.505+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:19.751+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:19.750+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:01:19.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:19.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:01:20.087+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.720 seconds
[2022-12-17T04:01:30.347+0000] {processor.py:154} INFO - Started process (PID=464) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:30.377+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:01:30.382+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:30.381+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:30.484+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:30.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:30.898+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:01:31.040+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:31.039+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:01:31.172+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.843 seconds
[2022-12-17T04:01:41.456+0000] {processor.py:154} INFO - Started process (PID=481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:41.481+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:01:41.485+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:41.484+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:41.603+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:41.865+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:41.864+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:01:42.143+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:42.142+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:01:42.287+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.846 seconds
[2022-12-17T04:01:52.754+0000] {processor.py:154} INFO - Started process (PID=492) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:52.790+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:01:52.799+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:52.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:52.915+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:01:53.138+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:53.137+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:01:53.274+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:01:53.273+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:01:53.382+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.642 seconds
[2022-12-17T04:02:04.048+0000] {processor.py:154} INFO - Started process (PID=502) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:04.071+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:02:04.077+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:04.076+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:04.163+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:04.433+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:04.432+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:02:04.595+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:04.594+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:02:04.735+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.701 seconds
[2022-12-17T04:02:14.996+0000] {processor.py:154} INFO - Started process (PID=512) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:15.019+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:02:15.023+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:15.022+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:15.111+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:15.560+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:15.560+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:02:15.699+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:15.698+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:02:15.828+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.846 seconds
[2022-12-17T04:02:26.292+0000] {processor.py:154} INFO - Started process (PID=530) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:26.310+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:02:26.314+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:26.313+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:26.497+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:28.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:28.147+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:02:28.347+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:28.346+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:02:28.473+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.226 seconds
[2022-12-17T04:02:38.795+0000] {processor.py:154} INFO - Started process (PID=540) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:38.819+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:02:38.823+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:38.822+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:38.913+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:39.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:39.150+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:02:39.293+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:39.292+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:02:39.423+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.644 seconds
[2022-12-17T04:02:49.578+0000] {processor.py:154} INFO - Started process (PID=550) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:49.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:02:49.877+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:49.876+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:49.971+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:02:50.606+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:50.605+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:02:50.748+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:02:50.747+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:02:50.888+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.324 seconds
[2022-12-17T04:03:01.399+0000] {processor.py:154} INFO - Started process (PID=567) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:01.402+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:03:01.406+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:01.405+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:01.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:02.160+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:02.159+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:03:02.629+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:02.628+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:03:02.949+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.606 seconds
[2022-12-17T04:03:13.773+0000] {processor.py:154} INFO - Started process (PID=578) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:13.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:03:13.806+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:13.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:14.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:14.588+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:14.587+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:03:14.954+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:14.953+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:03:15.337+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.629 seconds
[2022-12-17T04:03:25.831+0000] {processor.py:154} INFO - Started process (PID=588) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:25.834+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:03:25.846+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:25.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:26.008+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:26.382+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:26.381+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:03:26.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:26.547+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:03:26.725+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.927 seconds
[2022-12-17T04:03:37.126+0000] {processor.py:154} INFO - Started process (PID=598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:37.168+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:03:37.172+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:37.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:37.261+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:37.577+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:37.576+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:03:37.909+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:37.908+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:03:38.018+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.918 seconds
[2022-12-17T04:03:48.640+0000] {processor.py:154} INFO - Started process (PID=615) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:48.670+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:03:48.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:48.673+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:48.954+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:03:49.498+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:49.497+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:03:49.912+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:03:49.908+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:03:50.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.632 seconds
[2022-12-17T04:04:01.081+0000] {processor.py:154} INFO - Started process (PID=626) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:01.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:04:01.123+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:01.122+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:01.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:02.943+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:04:13.707+0000] {processor.py:154} INFO - Started process (PID=636) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:13.715+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:04:13.719+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:13.718+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:13.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:14.654+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:14.653+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:04:14.850+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:14.849+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:04:14.996+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.316 seconds
[2022-12-17T04:04:25.385+0000] {processor.py:154} INFO - Started process (PID=646) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:25.393+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:04:25.397+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:25.396+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:25.547+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:25.869+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:25.868+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:04:26.355+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:26.354+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:04:26.523+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.160 seconds
[2022-12-17T04:04:36.977+0000] {processor.py:154} INFO - Started process (PID=663) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:36.997+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:04:37.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:37.014+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:37.500+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:38.395+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:04:49.642+0000] {processor.py:154} INFO - Started process (PID=674) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:49.654+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:04:49.661+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:49.660+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:49.885+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:04:51.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:51.769+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:04:52.370+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:04:52.369+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:04:52.918+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.333 seconds
[2022-12-17T04:05:04.034+0000] {processor.py:154} INFO - Started process (PID=684) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:04.052+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:05:04.056+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:04.055+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:04.197+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:04.661+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:04.660+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:05:04.967+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:04.966+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:05:05.241+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.227 seconds
[2022-12-17T04:05:15.841+0000] {processor.py:154} INFO - Started process (PID=694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:15.889+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:05:15.897+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:15.896+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:16.214+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:16.707+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:16.706+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:05:16.950+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:16.949+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:05:17.206+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.430 seconds
[2022-12-17T04:05:27.947+0000] {processor.py:154} INFO - Started process (PID=711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:27.964+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:05:27.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:27.993+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:28.509+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:30.705+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:30.704+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:05:31.708+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:31.686+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:05:32.383+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.533 seconds
[2022-12-17T04:05:43.746+0000] {processor.py:154} INFO - Started process (PID=722) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:43.838+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:05:43.854+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:43.853+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:44.307+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:45.265+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:45.264+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:05:46.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:46.164+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:05:46.439+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.864 seconds
[2022-12-17T04:05:57.017+0000] {processor.py:154} INFO - Started process (PID=732) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:57.032+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:05:57.042+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:57.041+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:57.302+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:05:58.863+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:58.858+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:05:59.378+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:05:59.377+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:05:59.657+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.735 seconds
[2022-12-17T04:06:10.258+0000] {processor.py:154} INFO - Started process (PID=742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:10.285+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:06:10.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:06:10.309+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:10.898+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:12.774+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:06:23.913+0000] {processor.py:154} INFO - Started process (PID=759) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:23.964+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:06:23.977+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:06:23.976+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:24.404+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:26.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:06:26.626+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:06:27.630+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:06:27.629+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:06:28.501+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 4.696 seconds
[2022-12-17T04:06:39.519+0000] {processor.py:154} INFO - Started process (PID=770) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:39.538+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:06:39.584+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:06:39.569+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:40.429+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:42.897+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:06:54.737+0000] {processor.py:154} INFO - Started process (PID=780) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:54.768+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:06:54.816+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:06:54.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:55.576+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:06:58.260+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:06:58.242+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:06:59.683+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:06:59.682+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:07:00.945+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 6.336 seconds
[2022-12-17T04:07:12.687+0000] {processor.py:154} INFO - Started process (PID=790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:12.702+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:07:12.730+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:07:12.729+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:13.647+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:16.263+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:07:27.926+0000] {processor.py:154} INFO - Started process (PID=808) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:27.965+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:07:27.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:07:27.985+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:28.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:29.960+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:07:29.939+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:07:30.786+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:07:30.785+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:07:31.433+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.558 seconds
[2022-12-17T04:07:42.128+0000] {processor.py:154} INFO - Started process (PID=818) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:42.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:07:42.199+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:07:42.191+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:42.432+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:43.742+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:07:54.771+0000] {processor.py:154} INFO - Started process (PID=828) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:54.783+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:07:54.819+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:07:54.817+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:54.978+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:07:57.190+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:07:57.189+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:07:57.970+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:07:57.969+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:07:58.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.528 seconds
[2022-12-17T04:08:08.621+0000] {processor.py:154} INFO - Started process (PID=841) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:08.629+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:08:08.639+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:08.638+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:08.808+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:09.337+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:09.336+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:08:09.518+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:09.517+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:08:09.752+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.180 seconds
[2022-12-17T04:08:20.496+0000] {processor.py:154} INFO - Started process (PID=859) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:20.505+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:08:20.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:20.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:20.993+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:22.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:22.205+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:08:22.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:22.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:08:23.537+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.116 seconds
[2022-12-17T04:08:34.086+0000] {processor.py:154} INFO - Started process (PID=868) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:34.177+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:08:34.181+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:34.180+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:34.383+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:35.075+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:35.062+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:08:35.495+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:35.494+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:08:35.674+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.647 seconds
[2022-12-17T04:08:46.184+0000] {processor.py:154} INFO - Started process (PID=878) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:46.195+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:08:46.221+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:46.209+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:46.554+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:47.606+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:08:58.374+0000] {processor.py:154} INFO - Started process (PID=889) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:58.390+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:08:58.394+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:58.393+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:58.642+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:08:59.853+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:08:59.852+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:09:00.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:00.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:09:01.117+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.826 seconds
[2022-12-17T04:09:11.674+0000] {processor.py:154} INFO - Started process (PID=908) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:11.678+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:09:11.681+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:11.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:11.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:12.342+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:12.341+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:09:12.604+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:12.603+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:09:13.036+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.414 seconds
[2022-12-17T04:09:23.896+0000] {processor.py:154} INFO - Started process (PID=918) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:23.899+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:09:23.903+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:23.902+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:23.995+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:24.397+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:24.395+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:09:24.535+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:24.534+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:09:24.657+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.779 seconds
[2022-12-17T04:09:35.384+0000] {processor.py:154} INFO - Started process (PID=928) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:35.413+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:09:35.417+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:35.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:35.521+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:36.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:36.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:09:36.232+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:36.231+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:09:36.562+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.195 seconds
[2022-12-17T04:09:47.068+0000] {processor.py:154} INFO - Started process (PID=946) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:47.109+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:09:47.117+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:47.116+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:47.374+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:47.730+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:47.729+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:09:48.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:48.037+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:09:48.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.713 seconds
[2022-12-17T04:09:58.981+0000] {processor.py:154} INFO - Started process (PID=956) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:59.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:09:59.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:59.034+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:59.135+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:09:59.846+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:09:59.845+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:10:00.009+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:00.008+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:10:00.158+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.190 seconds
[2022-12-17T04:10:10.420+0000] {processor.py:154} INFO - Started process (PID=966) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:10.424+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:10:10.428+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:10.427+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:10.513+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:10.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:10.760+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:10:10.929+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:10.928+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:10:11.109+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.703 seconds
[2022-12-17T04:10:21.373+0000] {processor.py:154} INFO - Started process (PID=976) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:21.376+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:10:21.380+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:21.379+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:21.476+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:21.860+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:21.846+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:10:22.097+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:22.096+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:10:22.228+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.868 seconds
[2022-12-17T04:10:32.633+0000] {processor.py:154} INFO - Started process (PID=994) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:32.684+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:10:32.703+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:32.702+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:32.973+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:33.705+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:33.704+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:10:33.890+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:33.889+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:10:34.085+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.467 seconds
[2022-12-17T04:10:44.330+0000] {processor.py:154} INFO - Started process (PID=1004) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:44.334+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:10:44.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:44.337+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:44.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:44.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:44.968+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:10:45.100+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:45.098+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:10:45.244+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.928 seconds
[2022-12-17T04:10:55.590+0000] {processor.py:154} INFO - Started process (PID=1014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:55.601+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:10:55.610+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:55.609+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:55.749+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:10:56.699+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:56.698+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:10:56.863+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:10:56.856+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:10:57.052+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.485 seconds
[2022-12-17T04:11:07.389+0000] {processor.py:154} INFO - Started process (PID=1032) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:07.393+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:11:07.397+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:07.396+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:07.493+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:08.408+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:08.407+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:11:08.621+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:08.621+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:11:08.794+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-17T04:11:19.180+0000] {processor.py:154} INFO - Started process (PID=1043) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:19.201+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:11:19.206+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:19.205+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:19.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:20.179+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:11:30.641+0000] {processor.py:154} INFO - Started process (PID=1053) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:30.749+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:11:30.755+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:30.754+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:30.885+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:31.158+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:31.157+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:11:31.351+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:31.350+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:11:31.474+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.848 seconds
[2022-12-17T04:11:41.786+0000] {processor.py:154} INFO - Started process (PID=1063) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:41.829+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:11:41.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:41.833+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:41.976+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:42.244+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:42.243+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:11:42.404+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:42.403+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:11:42.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.771 seconds
[2022-12-17T04:11:53.048+0000] {processor.py:154} INFO - Started process (PID=1081) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:53.076+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:11:53.083+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:53.082+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:53.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:11:53.692+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:53.691+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:11:53.997+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:11:53.996+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:11:54.248+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.228 seconds
[2022-12-17T04:12:04.616+0000] {processor.py:154} INFO - Started process (PID=1091) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:04.620+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:12:04.624+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:04.623+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:04.710+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:05.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:05.400+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:12:05.627+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:05.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:12:05.816+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.214 seconds
[2022-12-17T04:12:16.084+0000] {processor.py:154} INFO - Started process (PID=1101) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:16.113+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:12:16.117+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:16.116+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:16.227+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:17.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:17.674+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:12:17.835+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:17.834+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:12:17.969+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.899 seconds
[2022-12-17T04:12:28.468+0000] {processor.py:154} INFO - Started process (PID=1118) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:28.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:12:28.522+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:28.521+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:28.831+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:29.667+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:29.666+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:12:29.917+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:29.917+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:12:30.157+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.737 seconds
[2022-12-17T04:12:40.586+0000] {processor.py:154} INFO - Started process (PID=1129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:40.591+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:12:40.595+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:40.594+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:40.686+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:41.991+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:12:52.500+0000] {processor.py:154} INFO - Started process (PID=1139) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:52.555+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:12:52.559+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:52.558+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:52.687+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:12:53.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:53.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:12:53.370+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:12:53.370+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:12:53.485+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.999 seconds
[2022-12-17T04:13:03.666+0000] {processor.py:154} INFO - Started process (PID=1149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:03.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:13:03.716+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:03.715+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:03.812+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:04.135+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:04.134+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:13:04.273+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:04.272+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:13:04.441+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.793 seconds
[2022-12-17T04:13:15.145+0000] {processor.py:154} INFO - Started process (PID=1167) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:15.189+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:13:15.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:15.200+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:15.333+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:16.626+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:13:26.967+0000] {processor.py:154} INFO - Started process (PID=1177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:26.987+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:13:26.991+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:26.990+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:27.092+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:27.891+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:27.890+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:13:28.043+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:28.042+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:13:28.180+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.241 seconds
[2022-12-17T04:13:38.434+0000] {processor.py:154} INFO - Started process (PID=1187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:38.488+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:13:38.492+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:38.491+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:38.588+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:38.833+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:38.832+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:13:39.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:39.033+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:13:39.244+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.824 seconds
[2022-12-17T04:13:49.721+0000] {processor.py:154} INFO - Started process (PID=1203) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:49.757+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:13:49.765+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:49.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:49.940+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:13:50.237+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:50.236+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:13:50.451+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:13:50.450+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:13:50.600+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.949 seconds
[2022-12-17T04:14:01.241+0000] {processor.py:154} INFO - Started process (PID=1215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:01.248+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:14:01.251+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:01.250+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:01.357+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:01.998+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:01.997+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:14:02.130+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:02.130+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:14:02.282+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.065 seconds
[2022-12-17T04:14:12.533+0000] {processor.py:154} INFO - Started process (PID=1225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:12.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:14:12.541+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:12.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:12.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:13.255+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:13.254+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:14:13.395+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:13.395+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:14:13.519+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.004 seconds
[2022-12-17T04:14:23.804+0000] {processor.py:154} INFO - Started process (PID=1235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:23.853+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:14:23.858+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:23.856+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:23.948+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:24.235+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:24.234+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:14:24.419+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:24.418+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:14:24.530+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.742 seconds
[2022-12-17T04:14:34.897+0000] {processor.py:154} INFO - Started process (PID=1254) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:34.923+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:14:34.936+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:34.930+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:35.106+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:35.533+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:35.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:14:35.723+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:35.722+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:14:36.037+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.156 seconds
[2022-12-17T04:14:46.538+0000] {processor.py:154} INFO - Started process (PID=1264) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:46.569+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:14:46.575+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:46.574+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:46.663+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:46.901+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:46.900+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:14:47.054+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:47.053+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:14:47.222+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.697 seconds
[2022-12-17T04:14:57.541+0000] {processor.py:154} INFO - Started process (PID=1274) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:57.545+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:14:57.548+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:57.547+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:57.640+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:14:57.868+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:57.863+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:14:58.095+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:14:58.094+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:14:58.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.715 seconds
[2022-12-17T04:15:08.503+0000] {processor.py:154} INFO - Started process (PID=1284) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:08.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:15:08.510+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:08.509+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:08.611+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:09.161+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:15:19.613+0000] {processor.py:154} INFO - Started process (PID=1302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:19.617+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:15:19.621+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:19.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:19.906+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:20.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:20.804+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:15:21.071+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:21.070+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:15:21.349+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.754 seconds
[2022-12-17T04:15:32.097+0000] {processor.py:154} INFO - Started process (PID=1312) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:32.100+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:15:32.104+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:32.103+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:32.203+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:32.700+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:15:42.960+0000] {processor.py:154} INFO - Started process (PID=1322) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:42.990+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:15:42.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:42.993+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:43.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:44.195+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:15:54.642+0000] {processor.py:154} INFO - Started process (PID=1339) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:54.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:15:54.704+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:54.703+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:54.838+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:15:55.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:55.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:15:55.503+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:15:55.502+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:15:55.749+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.122 seconds
[2022-12-17T04:16:06.193+0000] {processor.py:154} INFO - Started process (PID=1350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:06.226+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:16:06.229+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:06.228+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:06.334+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:06.893+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:06.891+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:16:07.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:07.034+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:16:07.164+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.005 seconds
[2022-12-17T04:16:17.430+0000] {processor.py:154} INFO - Started process (PID=1360) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:17.628+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:16:17.632+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:17.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:17.723+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:17.970+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:17.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:16:18.123+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:18.122+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:16:18.248+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.832 seconds
[2022-12-17T04:16:28.626+0000] {processor.py:154} INFO - Started process (PID=1368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:28.676+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:16:28.688+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:28.686+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:28.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:29.990+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:29.989+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:16:30.143+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:30.142+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:16:30.285+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.673 seconds
[2022-12-17T04:16:40.758+0000] {processor.py:154} INFO - Started process (PID=1386) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:40.786+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:16:40.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:40.791+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:41.039+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:41.808+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:41.807+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:16:42.065+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:42.064+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:16:42.277+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.561 seconds
[2022-12-17T04:16:53.095+0000] {processor.py:154} INFO - Started process (PID=1398) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:53.163+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:16:53.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:53.165+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:53.257+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:16:53.505+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:53.504+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:16:53.647+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:16:53.646+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:16:53.833+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.752 seconds
[2022-12-17T04:17:04.168+0000] {processor.py:154} INFO - Started process (PID=1408) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:04.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:17:04.192+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:04.191+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:04.303+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:04.516+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:04.514+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:17:04.666+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:04.665+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:17:04.800+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.648 seconds
[2022-12-17T04:17:15.119+0000] {processor.py:154} INFO - Started process (PID=1423) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:15.147+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:17:15.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:15.151+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:15.312+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:16.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:16.014+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:17:16.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:16.164+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:17:16.406+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.305 seconds
[2022-12-17T04:17:26.862+0000] {processor.py:154} INFO - Started process (PID=1434) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:26.865+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:17:26.870+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:26.869+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:26.979+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:27.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:27.743+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:17:27.954+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:27.953+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:17:28.358+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.513 seconds
[2022-12-17T04:17:38.999+0000] {processor.py:154} INFO - Started process (PID=1444) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:39.059+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:17:39.064+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:39.062+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:39.165+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:40.383+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:40.382+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:17:40.525+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:40.524+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:17:40.683+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.759 seconds
[2022-12-17T04:17:50.958+0000] {processor.py:154} INFO - Started process (PID=1456) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:51.003+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:17:51.007+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:51.006+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:51.095+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:17:51.431+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:51.425+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:17:51.605+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:17:51.605+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:17:51.899+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.954 seconds
[2022-12-17T04:18:02.645+0000] {processor.py:154} INFO - Started process (PID=1471) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:02.667+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:18:02.673+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:02.672+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:02.857+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:03.717+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:03.716+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:18:04.041+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:04.040+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:18:04.305+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.703 seconds
[2022-12-17T04:18:14.650+0000] {processor.py:154} INFO - Started process (PID=1481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:14.654+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:18:14.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:14.658+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:14.782+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:15.033+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:15.032+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:18:15.223+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:15.222+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:18:15.330+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.694 seconds
[2022-12-17T04:18:25.601+0000] {processor.py:154} INFO - Started process (PID=1491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:25.641+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:18:25.646+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:25.645+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:25.729+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:26.957+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:26.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:18:27.133+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:27.133+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:18:27.261+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.677 seconds
[2022-12-17T04:18:37.654+0000] {processor.py:154} INFO - Started process (PID=1511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:37.674+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:18:37.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:37.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:37.792+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:38.054+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:38.053+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:18:38.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:38.224+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:18:38.550+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.931 seconds
[2022-12-17T04:18:49.026+0000] {processor.py:154} INFO - Started process (PID=1521) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:49.029+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:18:49.033+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:49.032+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:49.120+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:18:49.552+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:49.550+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:18:49.703+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:18:49.702+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:18:49.811+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.799 seconds
[2022-12-17T04:19:00.123+0000] {processor.py:154} INFO - Started process (PID=1531) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:00.148+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:19:00.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:00.151+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:00.249+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:00.694+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:00.693+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:19:00.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:00.833+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:19:00.970+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.869 seconds
[2022-12-17T04:19:11.201+0000] {processor.py:154} INFO - Started process (PID=1541) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:11.229+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:19:11.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:11.232+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:11.330+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:11.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:11.597+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:19:11.751+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:11.750+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:19:11.866+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.681 seconds
[2022-12-17T04:19:22.207+0000] {processor.py:154} INFO - Started process (PID=1557) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:22.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:19:22.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:22.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:22.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:23.380+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:19:33.879+0000] {processor.py:154} INFO - Started process (PID=1567) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:33.883+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:19:33.887+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:33.886+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:33.988+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:34.480+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:34.479+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:19:34.636+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:34.635+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:19:34.801+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.944 seconds
[2022-12-17T04:19:45.059+0000] {processor.py:154} INFO - Started process (PID=1577) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:45.063+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:19:45.067+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:45.066+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:45.159+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:46.297+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:46.296+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:19:46.491+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:46.490+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:19:46.639+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.599 seconds
[2022-12-17T04:19:56.922+0000] {processor.py:154} INFO - Started process (PID=1594) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:56.949+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:19:56.953+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:19:56.952+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:57.073+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:19:57.709+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:20:08.342+0000] {processor.py:154} INFO - Started process (PID=1601) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:08.360+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:20:08.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:08.367+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:08.471+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:09.025+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:09.024+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:20:09.188+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:09.187+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:20:09.367+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.064 seconds
[2022-12-17T04:20:19.677+0000] {processor.py:154} INFO - Started process (PID=1614) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:19.681+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:20:19.685+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:19.684+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:19.789+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:20.039+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:20.038+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:20:20.187+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:20.186+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:20:20.388+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.725 seconds
[2022-12-17T04:20:31.200+0000] {processor.py:154} INFO - Started process (PID=1624) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:31.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:20:31.228+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:31.223+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:31.370+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:31.757+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:31.756+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:20:32.073+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:32.072+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:20:32.366+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.236 seconds
[2022-12-17T04:20:42.914+0000] {processor.py:154} INFO - Started process (PID=1642) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:42.944+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:20:42.956+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:42.947+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:43.249+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:44.381+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:44.380+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:20:44.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:44.658+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:20:44.980+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.118 seconds
[2022-12-17T04:20:56.117+0000] {processor.py:154} INFO - Started process (PID=1652) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:56.138+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:20:56.144+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:56.142+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:56.248+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:20:57.488+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:57.487+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:20:57.652+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:20:57.652+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:20:57.821+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.719 seconds
[2022-12-17T04:21:08.186+0000] {processor.py:154} INFO - Started process (PID=1662) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:08.190+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:21:08.195+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:08.194+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:08.305+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:08.556+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:08.554+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:21:08.725+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:08.724+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:21:08.876+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.716 seconds
[2022-12-17T04:21:19.227+0000] {processor.py:154} INFO - Started process (PID=1679) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:19.231+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:21:19.240+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:19.239+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:19.350+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:19.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:19.998+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:21:20.172+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:20.171+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:21:20.333+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.136 seconds
[2022-12-17T04:21:30.787+0000] {processor.py:154} INFO - Started process (PID=1687) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:30.809+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:21:30.813+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:30.812+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:30.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:31.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:31.120+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:21:31.255+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:31.254+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:21:31.392+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.619 seconds
[2022-12-17T04:21:41.662+0000] {processor.py:154} INFO - Started process (PID=1697) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:41.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:21:41.697+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:41.696+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:41.778+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:41.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:41.985+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:21:42.121+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:42.121+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:21:42.259+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.611 seconds
[2022-12-17T04:21:52.467+0000] {processor.py:154} INFO - Started process (PID=1707) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:52.529+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:21:52.534+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:52.533+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:52.631+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:21:53.028+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:53.027+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:21:53.163+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:21:53.162+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:21:53.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.850 seconds
[2022-12-17T04:22:03.913+0000] {processor.py:154} INFO - Started process (PID=1725) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:03.942+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:22:03.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:03.945+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:04.075+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:04.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:04.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:22:04.733+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:04.732+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:22:05.213+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.334 seconds
[2022-12-17T04:22:15.645+0000] {processor.py:154} INFO - Started process (PID=1735) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:15.687+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:22:15.695+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:15.690+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:15.913+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:17.460+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:17.458+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:22:17.597+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:17.596+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:22:17.716+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.086 seconds
[2022-12-17T04:22:28.010+0000] {processor.py:154} INFO - Started process (PID=1745) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:28.030+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:22:28.036+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:28.034+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:28.118+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:28.745+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:28.744+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:22:28.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:28.878+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:22:29.016+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.020 seconds
[2022-12-17T04:22:39.291+0000] {processor.py:154} INFO - Started process (PID=1755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:39.344+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:22:39.349+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:39.348+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:39.431+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:39.746+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:39.745+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:22:39.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:39.877+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:22:40.000+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.723 seconds
[2022-12-17T04:22:50.530+0000] {processor.py:154} INFO - Started process (PID=1772) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:50.565+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:22:50.578+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:22:50.577+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:50.709+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:22:51.700+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:23:02.092+0000] {processor.py:154} INFO - Started process (PID=1782) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:02.142+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:23:02.146+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:02.145+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:02.263+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:02.469+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:02.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:23:02.608+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:02.607+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:23:02.726+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.650 seconds
[2022-12-17T04:23:13.397+0000] {processor.py:154} INFO - Started process (PID=1792) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:13.402+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:23:13.407+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:13.406+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:13.527+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:13.736+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:13.736+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:23:13.867+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:13.866+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:23:13.996+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.615 seconds
[2022-12-17T04:23:24.298+0000] {processor.py:154} INFO - Started process (PID=1809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:24.343+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:23:24.355+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:24.350+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:24.455+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:24.787+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:24.786+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:23:24.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:24.968+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:23:25.083+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.800 seconds
[2022-12-17T04:23:35.494+0000] {processor.py:154} INFO - Started process (PID=1819) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:35.545+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:23:35.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:35.548+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:35.647+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:35.880+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:35.879+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:23:36.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:36.014+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:23:36.212+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.734 seconds
[2022-12-17T04:23:46.469+0000] {processor.py:154} INFO - Started process (PID=1829) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:46.482+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:23:46.490+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:46.489+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:46.623+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:47.043+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:47.042+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:23:47.172+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:47.171+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:23:47.319+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.865 seconds
[2022-12-17T04:23:57.635+0000] {processor.py:154} INFO - Started process (PID=1839) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:57.667+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:23:57.672+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:57.671+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:57.783+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:23:58.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:58.044+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:23:58.177+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:23:58.176+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:23:58.321+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.701 seconds
[2022-12-17T04:24:08.708+0000] {processor.py:154} INFO - Started process (PID=1857) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:08.720+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:24:08.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:08.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:09.105+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:10.385+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:10.384+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:24:10.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:10.545+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:24:10.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.044 seconds
[2022-12-17T04:24:21.050+0000] {processor.py:154} INFO - Started process (PID=1867) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:21.054+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:24:21.058+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:21.057+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:21.145+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:21.915+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:21.914+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:24:22.088+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:22.086+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:24:22.209+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.173 seconds
[2022-12-17T04:24:32.425+0000] {processor.py:154} INFO - Started process (PID=1877) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:32.523+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:24:32.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:32.526+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:32.653+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:33.580+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:24:43.927+0000] {processor.py:154} INFO - Started process (PID=1894) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:43.970+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:24:43.983+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:43.978+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:44.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:44.330+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:44.329+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:24:44.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:44.914+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:24:45.075+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.163 seconds
[2022-12-17T04:24:55.697+0000] {processor.py:154} INFO - Started process (PID=1905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:55.749+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:24:55.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:55.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:55.929+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:24:56.156+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:56.155+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:24:56.416+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:24:56.415+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:24:56.538+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-17T04:25:06.657+0000] {processor.py:154} INFO - Started process (PID=1915) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:06.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:25:06.715+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:06.714+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:07.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:08.597+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:08.595+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:25:08.786+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:08.785+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:25:08.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.266 seconds
[2022-12-17T04:25:19.109+0000] {processor.py:154} INFO - Started process (PID=1925) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:19.139+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:25:19.143+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:19.142+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:19.230+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:19.449+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:19.448+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:25:19.605+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:19.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:25:19.740+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.648 seconds
[2022-12-17T04:25:30.420+0000] {processor.py:154} INFO - Started process (PID=1943) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:30.451+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:25:30.456+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:30.454+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:30.590+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:30.945+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:30.944+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:25:31.207+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:31.206+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:25:31.321+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.919 seconds
[2022-12-17T04:25:41.743+0000] {processor.py:154} INFO - Started process (PID=1953) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:41.800+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:25:41.804+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:41.803+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:41.934+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:42.231+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:42.230+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:25:42.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:42.365+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:25:42.584+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.872 seconds
[2022-12-17T04:25:53.259+0000] {processor.py:154} INFO - Started process (PID=1963) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:53.268+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:25:53.273+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:53.272+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:53.505+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:25:53.836+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:53.835+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:25:53.989+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:25:53.988+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:25:54.100+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-17T04:26:04.466+0000] {processor.py:154} INFO - Started process (PID=1973) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:04.515+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:26:04.519+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:04.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:04.620+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:05.169+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:05.166+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:26:05.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:05.938+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:26:06.182+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.732 seconds
[2022-12-17T04:26:16.646+0000] {processor.py:154} INFO - Started process (PID=1991) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:16.649+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:26:16.653+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:16.652+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:16.739+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:17.902+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:26:28.062+0000] {processor.py:154} INFO - Started process (PID=2001) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:28.066+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:26:28.070+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:28.069+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:28.151+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:28.869+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:28.868+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:26:29.016+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:29.015+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:26:29.173+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.124 seconds
[2022-12-17T04:26:39.466+0000] {processor.py:154} INFO - Started process (PID=2011) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:39.507+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:26:39.512+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:39.511+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:39.602+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:26:39.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:39.804+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:26:39.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:26:39.967+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:26:40.344+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.892 seconds
[2022-12-17T04:27:23.107+0000] {processor.py:154} INFO - Started process (PID=170) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:23.133+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:27:23.143+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:23.142+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:23.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:24.085+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:24.084+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:27:24.433+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:24.432+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:27:24.569+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.506 seconds
[2022-12-17T04:27:34.973+0000] {processor.py:154} INFO - Started process (PID=179) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:35.001+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:27:35.008+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:35.007+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:35.101+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:35.325+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:35.324+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:27:35.475+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:35.474+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:27:35.606+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.646 seconds
[2022-12-17T04:27:45.869+0000] {processor.py:154} INFO - Started process (PID=189) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:45.916+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:27:45.920+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:45.919+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:46.002+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:46.221+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:46.219+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:27:46.370+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:46.369+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:27:46.886+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.030 seconds
[2022-12-17T04:27:57.170+0000] {processor.py:154} INFO - Started process (PID=199) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:57.309+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:27:57.313+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:57.312+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:57.411+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:27:57.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:57.681+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:27:57.821+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:27:57.820+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:27:57.980+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.824 seconds
[2022-12-17T04:28:08.401+0000] {processor.py:154} INFO - Started process (PID=217) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:08.454+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:28:08.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:08.457+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:08.648+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:08.901+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:08.900+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:28:09.054+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:09.053+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:28:09.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.848 seconds
[2022-12-17T04:28:19.543+0000] {processor.py:154} INFO - Started process (PID=227) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:19.558+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:28:19.566+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:19.562+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:19.669+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:19.892+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:19.891+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:28:20.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:20.018+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:28:20.136+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.606 seconds
[2022-12-17T04:28:30.984+0000] {processor.py:154} INFO - Started process (PID=237) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:31.014+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:28:31.018+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:31.017+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:31.113+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:31.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:31.309+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:28:31.437+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:31.436+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:28:31.593+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.624 seconds
[2022-12-17T04:28:42.250+0000] {processor.py:154} INFO - Started process (PID=247) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:42.254+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:28:42.258+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:42.257+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:42.349+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:42.584+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:42.583+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:28:42.721+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:42.720+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:28:42.857+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.620 seconds
[2022-12-17T04:28:53.958+0000] {processor.py:154} INFO - Started process (PID=266) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:53.966+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:28:54.000+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:53.991+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:54.222+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:28:55.719+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:55.718+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:28:55.860+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:28:55.859+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:28:56.053+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.116 seconds
[2022-12-17T04:29:06.426+0000] {processor.py:154} INFO - Started process (PID=276) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:06.454+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:29:06.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:06.457+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:06.538+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:06.747+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:06.746+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:29:06.889+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:06.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:29:07.017+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.604 seconds
[2022-12-17T04:29:17.244+0000] {processor.py:154} INFO - Started process (PID=286) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:17.269+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:29:17.273+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:17.272+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:17.353+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:17.590+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:17.588+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:29:17.730+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:17.729+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:29:17.860+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.630 seconds
[2022-12-17T04:29:28.478+0000] {processor.py:154} INFO - Started process (PID=304) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:28.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:29:28.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:28.525+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:28.671+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:28.978+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:28.977+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:29:29.221+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:29.220+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:29:29.392+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.973 seconds
[2022-12-17T04:29:39.570+0000] {processor.py:154} INFO - Started process (PID=314) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:39.597+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:29:39.601+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:39.600+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:39.686+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:40.078+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:40.075+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:29:40.276+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:40.275+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:29:40.471+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.917 seconds
[2022-12-17T04:29:50.828+0000] {processor.py:154} INFO - Started process (PID=324) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:50.877+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:29:50.882+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:29:50.881+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:50.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:29:52.218+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:30:02.415+0000] {processor.py:154} INFO - Started process (PID=334) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:02.436+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:30:02.440+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:02.439+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:02.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:02.767+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:02.766+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:30:02.987+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:02.986+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:30:03.355+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.956 seconds
[2022-12-17T04:30:14.106+0000] {processor.py:154} INFO - Started process (PID=352) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:14.147+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:30:14.192+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:14.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:14.387+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:15.327+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:15.326+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:30:15.789+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:15.788+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:30:16.038+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.982 seconds
[2022-12-17T04:30:26.317+0000] {processor.py:154} INFO - Started process (PID=362) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:26.320+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:30:26.324+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:26.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:26.407+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:27.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:27.985+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:30:28.136+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:28.135+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:30:28.269+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.966 seconds
[2022-12-17T04:30:38.669+0000] {processor.py:154} INFO - Started process (PID=372) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:38.702+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:30:38.706+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:38.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:38.802+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:40.026+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:30:50.605+0000] {processor.py:154} INFO - Started process (PID=390) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:50.612+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:30:50.620+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:50.619+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:50.896+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:30:51.814+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:51.813+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:30:51.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:30:51.993+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:30:52.142+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.556 seconds
[2022-12-17T04:31:02.471+0000] {processor.py:154} INFO - Started process (PID=400) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:02.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:31:02.563+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:02.554+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:02.789+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:03.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:03.062+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:31:03.207+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:03.206+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:31:03.368+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.941 seconds
[2022-12-17T04:31:13.778+0000] {processor.py:154} INFO - Started process (PID=410) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:13.985+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:31:14.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:13.997+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:14.187+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:14.532+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:14.530+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:31:14.704+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:14.702+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:31:14.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.175 seconds
[2022-12-17T04:31:25.261+0000] {processor.py:154} INFO - Started process (PID=420) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:25.302+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:31:25.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:25.309+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:25.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:26.194+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:26.193+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:31:26.336+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:26.336+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:31:26.489+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.263 seconds
[2022-12-17T04:31:37.076+0000] {processor.py:154} INFO - Started process (PID=437) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:37.189+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:31:37.204+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:37.199+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:37.473+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:38.075+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:38.074+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:31:38.303+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:38.302+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:31:38.458+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.475 seconds
[2022-12-17T04:31:49.318+0000] {processor.py:154} INFO - Started process (PID=447) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:49.365+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:31:49.379+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:49.374+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:49.610+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:31:50.804+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:50.802+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:31:50.951+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:31:50.950+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:31:51.060+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.769 seconds
[2022-12-17T04:32:01.415+0000] {processor.py:154} INFO - Started process (PID=457) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:01.428+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:32:01.432+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:01.431+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:01.527+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:01.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:01.769+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:32:01.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:01.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:32:02.052+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.652 seconds
[2022-12-17T04:32:12.770+0000] {processor.py:154} INFO - Started process (PID=475) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:12.833+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:32:12.846+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:12.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:13.062+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:13.312+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:13.311+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:32:13.472+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:13.471+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:32:13.638+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.882 seconds
[2022-12-17T04:32:24.105+0000] {processor.py:154} INFO - Started process (PID=485) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:24.109+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:32:24.113+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:24.112+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:24.194+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:25.000+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:32:35.523+0000] {processor.py:154} INFO - Started process (PID=495) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:35.570+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:32:35.575+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:35.574+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:35.661+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:36.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:36.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:32:36.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:36.337+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:32:36.448+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.947 seconds
[2022-12-17T04:32:46.654+0000] {processor.py:154} INFO - Started process (PID=505) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:46.658+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:32:46.662+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:46.661+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:46.750+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:46.974+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:46.973+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:32:47.131+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:47.130+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:32:47.243+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.602 seconds
[2022-12-17T04:32:57.821+0000] {processor.py:154} INFO - Started process (PID=524) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:57.849+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:32:57.853+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:57.852+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:58.201+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:32:59.154+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:59.153+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:32:59.406+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:32:59.406+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:32:59.581+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.810 seconds
[2022-12-17T04:33:09.954+0000] {processor.py:154} INFO - Started process (PID=534) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:09.958+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:33:09.962+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:09.961+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:10.055+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:10.265+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:10.264+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:33:10.392+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:10.391+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:33:10.516+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.579 seconds
[2022-12-17T04:33:20.820+0000] {processor.py:154} INFO - Started process (PID=544) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:20.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:33:20.830+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:20.829+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:21.046+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:22.343+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:33:33.446+0000] {processor.py:154} INFO - Started process (PID=561) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:33.455+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:33:33.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:33.458+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:33.586+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:34.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:34.466+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:33:34.728+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:34.727+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:33:35.085+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.679 seconds
[2022-12-17T04:33:45.796+0000] {processor.py:154} INFO - Started process (PID=571) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:45.876+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:33:45.881+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:45.879+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:45.970+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:46.186+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:46.185+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:33:46.321+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:46.320+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:33:46.457+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.676 seconds
[2022-12-17T04:33:56.715+0000] {processor.py:154} INFO - Started process (PID=581) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:56.724+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:33:56.728+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:56.727+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:56.811+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:33:57.204+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:57.203+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:33:57.330+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:33:57.329+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:33:57.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.734 seconds
[2022-12-17T04:34:07.717+0000] {processor.py:154} INFO - Started process (PID=591) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:07.738+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:34:07.743+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:07.742+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:07.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:08.680+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:34:19.064+0000] {processor.py:154} INFO - Started process (PID=609) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:19.112+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:34:19.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:19.119+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:19.318+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:19.778+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:19.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:34:19.988+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:19.987+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:34:20.175+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.125 seconds
[2022-12-17T04:34:30.400+0000] {processor.py:154} INFO - Started process (PID=619) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:30.443+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:34:30.448+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:30.447+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:30.533+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:31.844+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:31.843+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:34:32.009+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:32.008+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:34:32.136+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.750 seconds
[2022-12-17T04:34:42.491+0000] {processor.py:154} INFO - Started process (PID=629) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:42.494+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:34:42.499+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:42.498+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:42.590+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:44.051+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:44.049+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:34:44.198+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:44.197+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:34:44.317+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.839 seconds
[2022-12-17T04:34:54.683+0000] {processor.py:154} INFO - Started process (PID=645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:54.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:34:54.706+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:54.704+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:54.816+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:34:55.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:55.483+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:34:55.666+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:34:55.665+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:34:55.893+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.243 seconds
[2022-12-17T04:35:06.370+0000] {processor.py:154} INFO - Started process (PID=656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:06.374+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:35:06.378+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:06.377+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:06.481+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:06.829+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:06.829+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:35:06.974+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:06.973+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:35:07.307+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.952 seconds
[2022-12-17T04:35:17.608+0000] {processor.py:154} INFO - Started process (PID=666) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:17.611+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:35:17.615+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:17.614+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:17.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:17.911+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:17.910+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:35:18.057+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:18.056+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:35:18.181+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.587 seconds
[2022-12-17T04:35:28.514+0000] {processor.py:154} INFO - Started process (PID=676) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:28.517+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:35:28.521+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:28.520+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:28.616+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:28.817+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:28.816+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:35:28.997+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:28.996+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:35:29.107+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.606 seconds
[2022-12-17T04:35:39.826+0000] {processor.py:154} INFO - Started process (PID=694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:40.095+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:35:40.103+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:40.102+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:40.205+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:40.460+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:40.459+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:35:40.614+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:40.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:35:40.732+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.921 seconds
[2022-12-17T04:35:51.328+0000] {processor.py:154} INFO - Started process (PID=704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:51.353+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:35:51.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:51.356+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:51.439+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:35:51.677+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:51.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:35:51.818+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:35:51.817+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:35:51.931+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.616 seconds
[2022-12-17T04:36:02.122+0000] {processor.py:154} INFO - Started process (PID=714) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:02.136+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:36:02.140+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:02.139+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:02.228+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:02.784+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:02.783+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:36:02.951+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:02.950+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:36:03.068+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.960 seconds
[2022-12-17T04:36:13.350+0000] {processor.py:154} INFO - Started process (PID=724) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:13.354+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:36:13.358+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:13.357+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:13.624+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:14.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:14.003+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:36:14.157+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:14.156+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:36:14.536+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.200 seconds
[2022-12-17T04:36:25.278+0000] {processor.py:154} INFO - Started process (PID=741) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:25.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:36:25.348+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:25.343+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:25.446+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:26.941+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:26.940+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:36:27.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:27.068+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:36:27.193+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.994 seconds
[2022-12-17T04:36:37.469+0000] {processor.py:154} INFO - Started process (PID=751) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:37.515+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:36:37.519+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:37.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:37.615+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:37.825+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:37.824+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:36:37.972+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:37.971+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:36:38.083+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.628 seconds
[2022-12-17T04:36:48.353+0000] {processor.py:154} INFO - Started process (PID=761) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:48.374+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:36:48.379+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:48.378+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:48.461+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:48.810+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:48.809+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:36:48.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:48.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:36:49.078+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.738 seconds
[2022-12-17T04:36:59.457+0000] {processor.py:154} INFO - Started process (PID=778) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:59.474+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:36:59.478+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:36:59.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:36:59.608+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:00.073+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:00.072+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:37:00.342+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:00.342+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:37:00.588+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.148 seconds
[2022-12-17T04:37:10.803+0000] {processor.py:154} INFO - Started process (PID=788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:10.828+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:37:10.833+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:10.832+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:10.922+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:11.128+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:11.127+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:37:11.269+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:11.268+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:37:11.427+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.637 seconds
[2022-12-17T04:37:21.759+0000] {processor.py:154} INFO - Started process (PID=798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:21.787+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:37:21.800+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:21.790+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:21.955+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:22.163+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:22.162+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:37:22.289+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:22.288+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:37:22.430+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.712 seconds
[2022-12-17T04:37:32.718+0000] {processor.py:154} INFO - Started process (PID=808) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:32.746+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:37:32.751+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:32.750+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:32.863+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:33.250+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:33.249+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:37:33.402+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:33.401+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:37:33.561+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.856 seconds
[2022-12-17T04:37:44.297+0000] {processor.py:154} INFO - Started process (PID=826) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:44.359+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:37:44.373+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:44.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:44.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:45.037+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:45.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:37:45.307+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:45.306+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:37:45.428+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.174 seconds
[2022-12-17T04:37:55.628+0000] {processor.py:154} INFO - Started process (PID=836) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:55.651+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:37:55.655+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:55.654+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:55.739+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:37:56.048+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:56.047+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:37:56.238+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:37:56.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:37:56.484+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.869 seconds
[2022-12-17T04:38:06.858+0000] {processor.py:154} INFO - Started process (PID=846) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:06.886+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:38:06.891+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:06.890+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:06.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:07.460+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:07.459+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:38:07.608+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:07.606+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:38:07.822+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.001 seconds
[2022-12-17T04:38:18.434+0000] {processor.py:154} INFO - Started process (PID=864) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:18.456+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:38:18.460+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:18.459+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:18.680+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:19.066+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:19.065+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:38:19.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:19.453+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:38:19.996+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.600 seconds
[2022-12-17T04:38:30.647+0000] {processor.py:154} INFO - Started process (PID=874) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:30.654+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:38:30.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:30.658+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:30.754+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:31.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:31.003+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:38:31.187+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:31.186+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:38:31.399+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.767 seconds
[2022-12-17T04:38:41.517+0000] {processor.py:154} INFO - Started process (PID=884) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:41.536+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:38:41.545+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:41.544+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:41.645+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:42.138+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:42.137+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:38:42.283+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:42.278+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:38:42.457+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.953 seconds
[2022-12-17T04:38:52.902+0000] {processor.py:154} INFO - Started process (PID=894) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:52.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:38:52.964+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:52.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:53.086+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:38:53.882+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:53.881+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:38:54.066+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:38:54.066+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:38:54.220+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.351 seconds
[2022-12-17T04:39:04.746+0000] {processor.py:154} INFO - Started process (PID=912) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:04.806+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:39:04.810+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:04.809+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:04.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:06.350+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:06.349+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:39:06.747+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:06.746+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:39:06.988+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.284 seconds
[2022-12-17T04:39:17.360+0000] {processor.py:154} INFO - Started process (PID=922) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:17.363+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:39:17.367+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:17.366+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:17.471+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:18.319+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:18.318+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:39:18.568+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:18.567+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:39:18.743+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.399 seconds
[2022-12-17T04:39:29.025+0000] {processor.py:154} INFO - Started process (PID=932) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:29.065+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:39:29.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:29.068+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:29.158+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:29.692+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:29.691+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:39:29.833+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:29.832+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:39:29.966+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.956 seconds
[2022-12-17T04:39:40.244+0000] {processor.py:154} INFO - Started process (PID=942) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:40.290+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:39:40.294+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:40.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:40.378+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:41.044+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:41.043+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:39:41.180+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:41.179+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:39:41.302+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.075 seconds
[2022-12-17T04:39:51.705+0000] {processor.py:154} INFO - Started process (PID=960) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:51.742+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:39:51.746+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:51.745+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:51.887+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:39:52.506+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:52.505+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:39:52.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:39:52.872+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:39:53.088+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.398 seconds
[2022-12-17T04:40:03.459+0000] {processor.py:154} INFO - Started process (PID=970) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:03.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:40:03.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:03.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:03.552+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:04.731+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:04.730+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:40:04.868+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:04.867+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:40:05.002+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.556 seconds
[2022-12-17T04:40:15.205+0000] {processor.py:154} INFO - Started process (PID=980) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:15.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:40:15.213+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:15.212+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:15.300+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:15.538+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:15.537+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:40:15.696+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:15.691+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:40:15.827+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.636 seconds
[2022-12-17T04:40:26.485+0000] {processor.py:154} INFO - Started process (PID=998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:26.533+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:40:26.545+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:26.544+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:26.781+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:27.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:27.579+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:40:27.780+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:27.779+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:40:27.945+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.479 seconds
[2022-12-17T04:40:38.373+0000] {processor.py:154} INFO - Started process (PID=1008) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:38.377+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:40:38.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:38.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:38.470+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:38.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:38.709+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:40:38.851+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:38.850+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:40:39.022+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-17T04:40:49.348+0000] {processor.py:154} INFO - Started process (PID=1018) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:49.352+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:40:49.356+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:49.355+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:49.447+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:40:49.677+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:49.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:40:49.843+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:40:49.841+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:40:49.991+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.657 seconds
[2022-12-17T04:41:00.318+0000] {processor.py:154} INFO - Started process (PID=1028) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:00.363+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:41:00.367+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:00.366+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:00.452+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:01.326+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:01.325+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:41:01.551+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:01.550+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:41:01.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.381 seconds
[2022-12-17T04:41:12.114+0000] {processor.py:154} INFO - Started process (PID=1046) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:12.163+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:41:12.167+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:12.166+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:12.363+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:13.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:13.200+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:41:13.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:13.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:41:13.812+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.732 seconds
[2022-12-17T04:41:24.188+0000] {processor.py:154} INFO - Started process (PID=1056) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:24.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:41:24.221+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:24.219+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:24.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:25.361+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:25.361+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:41:25.521+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:25.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:41:25.718+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.560 seconds
[2022-12-17T04:41:36.213+0000] {processor.py:154} INFO - Started process (PID=1066) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:36.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:41:36.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:36.222+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:36.343+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:37.024+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:37.023+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:41:37.170+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:37.169+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:41:37.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.110 seconds
[2022-12-17T04:41:47.631+0000] {processor.py:154} INFO - Started process (PID=1083) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:47.645+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:41:47.667+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:41:47.666+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:47.910+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:41:49.549+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:41:59.993+0000] {processor.py:154} INFO - Started process (PID=1094) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:00.038+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:42:00.042+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:00.041+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:00.196+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:01.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:01.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:42:01.321+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:01.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:42:01.469+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.491 seconds
[2022-12-17T04:42:11.757+0000] {processor.py:154} INFO - Started process (PID=1104) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:11.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:42:11.764+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:11.763+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:11.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:12.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:12.151+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:42:12.321+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:12.320+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:42:12.440+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.698 seconds
[2022-12-17T04:42:22.727+0000] {processor.py:154} INFO - Started process (PID=1114) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:22.730+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:42:22.737+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:22.736+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:22.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:23.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:23.136+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:42:23.295+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:23.294+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:42:23.409+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.697 seconds
[2022-12-17T04:42:34.192+0000] {processor.py:154} INFO - Started process (PID=1131) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:34.196+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:42:34.204+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:34.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:34.309+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:34.727+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:34.726+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:42:34.909+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:34.908+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:42:35.169+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.995 seconds
[2022-12-17T04:42:45.509+0000] {processor.py:154} INFO - Started process (PID=1141) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:45.540+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:42:45.544+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:45.543+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:45.646+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:45.881+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:45.880+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:42:46.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:46.013+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:42:46.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.670 seconds
[2022-12-17T04:42:56.417+0000] {processor.py:154} INFO - Started process (PID=1151) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:56.464+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:42:56.468+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:56.467+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:56.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:42:57.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:57.224+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:42:57.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:42:57.362+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:42:57.494+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.091 seconds
[2022-12-17T04:43:07.798+0000] {processor.py:154} INFO - Started process (PID=1161) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:07.812+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:43:07.816+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:07.815+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:07.911+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:08.197+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:08.196+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:43:08.346+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:08.345+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:43:08.466+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.682 seconds
[2022-12-17T04:43:18.872+0000] {processor.py:154} INFO - Started process (PID=1179) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:18.892+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:43:18.896+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:18.895+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:18.986+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:19.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:19.224+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:43:19.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:19.362+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:43:19.492+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.638 seconds
[2022-12-17T04:43:30.120+0000] {processor.py:154} INFO - Started process (PID=1189) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:30.161+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:43:30.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:30.165+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:30.295+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:31.519+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:43:41.947+0000] {processor.py:154} INFO - Started process (PID=1199) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:41.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:43:42.000+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:41.999+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:42.112+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:43.005+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:43.004+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:43:43.132+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:43.131+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:43:43.282+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.350 seconds
[2022-12-17T04:43:53.587+0000] {processor.py:154} INFO - Started process (PID=1218) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:53.624+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:43:53.632+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:53.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:53.782+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:43:54.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:54.146+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:43:54.345+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:43:54.344+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:43:54.499+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.950 seconds
[2022-12-17T04:44:04.909+0000] {processor.py:154} INFO - Started process (PID=1228) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:04.933+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:44:04.936+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:04.935+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:05.022+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:05.254+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:05.253+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:44:05.380+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:05.379+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:44:05.501+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.610 seconds
[2022-12-17T04:44:15.816+0000] {processor.py:154} INFO - Started process (PID=1238) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:15.843+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:44:15.848+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:15.847+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:15.939+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:16.144+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:16.142+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:44:16.270+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:16.269+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:44:16.422+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.620 seconds
[2022-12-17T04:44:26.708+0000] {processor.py:154} INFO - Started process (PID=1248) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:26.755+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:44:26.762+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:26.761+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:26.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:28.311+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:44:38.773+0000] {processor.py:154} INFO - Started process (PID=1267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:39.146+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:44:39.154+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:39.153+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:39.261+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:40.150+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:40.149+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:44:40.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:40.383+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:44:40.639+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.902 seconds
[2022-12-17T04:44:51.691+0000] {processor.py:154} INFO - Started process (PID=1277) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:51.746+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:44:51.750+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:51.749+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:51.873+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:44:52.442+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:52.441+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:44:52.588+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:44:52.587+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:44:52.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.056 seconds
[2022-12-17T04:45:02.948+0000] {processor.py:154} INFO - Started process (PID=1287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:02.971+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:45:02.975+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:02.974+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:03.063+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:03.755+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:03.754+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:45:03.902+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:03.901+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:45:04.016+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.083 seconds
[2022-12-17T04:45:14.529+0000] {processor.py:154} INFO - Started process (PID=1305) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:14.565+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:45:14.573+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:14.572+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:14.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:16.334+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:16.333+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:45:16.514+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:16.513+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:45:16.685+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.200 seconds
[2022-12-17T04:45:27.066+0000] {processor.py:154} INFO - Started process (PID=1315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:27.094+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:45:27.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:27.100+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:27.200+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:28.024+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:45:38.256+0000] {processor.py:154} INFO - Started process (PID=1325) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:38.287+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:45:38.294+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:38.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:38.386+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:39.134+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:39.133+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:45:39.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:39.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:45:39.446+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.218 seconds
[2022-12-17T04:45:49.770+0000] {processor.py:154} INFO - Started process (PID=1335) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:49.799+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:45:49.803+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:49.802+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:49.897+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:45:50.100+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:50.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:45:50.267+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:45:50.266+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:45:50.493+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.747 seconds
[2022-12-17T04:46:01.374+0000] {processor.py:154} INFO - Started process (PID=1352) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:01.457+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:46:01.470+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:01.469+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:01.824+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:02.156+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:02.155+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:46:02.370+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:02.369+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:46:02.566+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.365 seconds
[2022-12-17T04:46:12.808+0000] {processor.py:154} INFO - Started process (PID=1362) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:12.835+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:46:12.846+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:12.838+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:13.049+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:13.381+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:13.380+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:46:13.535+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:13.534+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:46:13.958+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.174 seconds
[2022-12-17T04:46:24.250+0000] {processor.py:154} INFO - Started process (PID=1372) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:24.290+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:46:24.300+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:24.299+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:24.459+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:24.905+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:46:35.260+0000] {processor.py:154} INFO - Started process (PID=1389) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:35.308+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:46:35.320+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:35.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:35.429+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:35.806+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:35.805+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:46:36.407+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:36.406+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:46:36.538+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.296 seconds
[2022-12-17T04:46:46.973+0000] {processor.py:154} INFO - Started process (PID=1399) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:47.002+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:46:47.006+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:47.005+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:47.119+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:47.443+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:47.442+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:46:47.706+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:47.705+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:46:47.861+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.903 seconds
[2022-12-17T04:46:58.135+0000] {processor.py:154} INFO - Started process (PID=1409) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:58.223+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:46:58.228+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:46:58.226+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:58.309+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:46:59.197+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:47:09.551+0000] {processor.py:154} INFO - Started process (PID=1419) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:09.602+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:47:09.607+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:09.606+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:09.691+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:10.172+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:10.171+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:47:10.382+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:10.378+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:47:10.674+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.136 seconds
[2022-12-17T04:47:21.113+0000] {processor.py:154} INFO - Started process (PID=1437) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:21.181+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:47:21.196+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:21.187+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:21.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:22.706+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:22.705+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:47:23.011+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:23.010+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:47:23.218+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.122 seconds
[2022-12-17T04:47:33.504+0000] {processor.py:154} INFO - Started process (PID=1447) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:33.524+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:47:33.529+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:33.528+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:33.618+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:33.823+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:33.822+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:47:34.129+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:34.128+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:47:34.425+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.935 seconds
[2022-12-17T04:47:44.713+0000] {processor.py:154} INFO - Started process (PID=1457) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:44.738+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:47:44.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:44.743+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:44.831+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:45.046+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:45.045+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:47:45.177+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:45.176+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:47:45.312+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.615 seconds
[2022-12-17T04:47:55.654+0000] {processor.py:154} INFO - Started process (PID=1473) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:55.679+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:47:55.687+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:55.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:55.786+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:47:56.046+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:56.045+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:47:56.212+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:47:56.211+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:47:56.363+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.724 seconds
[2022-12-17T04:48:06.783+0000] {processor.py:154} INFO - Started process (PID=1484) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:06.786+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:48:06.790+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:06.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:06.891+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:07.115+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:07.114+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:48:07.341+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:07.341+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:48:07.453+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.684 seconds
[2022-12-17T04:48:17.739+0000] {processor.py:154} INFO - Started process (PID=1494) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:17.782+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:48:17.787+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:17.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:17.880+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:18.083+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:18.082+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:48:18.218+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:18.217+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:48:18.363+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.641 seconds
[2022-12-17T04:48:28.642+0000] {processor.py:154} INFO - Started process (PID=1504) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:28.664+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:48:28.668+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:28.667+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:28.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:28.987+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:28.986+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:48:29.167+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:29.166+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:48:29.306+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.679 seconds
[2022-12-17T04:48:39.618+0000] {processor.py:154} INFO - Started process (PID=1523) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:39.626+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:48:39.637+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:39.635+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:39.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:40.733+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:40.732+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:48:41.136+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:41.128+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:48:41.314+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.737 seconds
[2022-12-17T04:48:51.861+0000] {processor.py:154} INFO - Started process (PID=1533) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:51.910+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:48:51.917+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:51.916+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:52.013+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:48:52.292+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:52.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:48:52.431+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:48:52.430+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:48:52.570+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.725 seconds
[2022-12-17T04:49:03.448+0000] {processor.py:154} INFO - Started process (PID=1543) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:03.501+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:49:03.539+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:03.508+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:03.680+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:04.300+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:04.299+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:49:04.435+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:04.434+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:49:04.621+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.190 seconds
[2022-12-17T04:49:14.856+0000] {processor.py:154} INFO - Started process (PID=1553) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:14.903+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:49:14.907+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:14.906+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:14.994+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:15.670+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:49:26.329+0000] {processor.py:154} INFO - Started process (PID=1570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:26.726+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:49:26.742+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:26.740+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:26.901+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:27.228+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:27.227+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:49:27.669+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:27.668+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:49:27.996+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.710 seconds
[2022-12-17T04:49:38.686+0000] {processor.py:154} INFO - Started process (PID=1580) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:38.736+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:49:38.741+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:38.740+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:38.821+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:39.050+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:39.049+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:49:39.196+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:39.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:49:39.303+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-17T04:49:50.296+0000] {processor.py:154} INFO - Started process (PID=1590) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:50.348+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:49:50.353+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:49:50.352+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:50.439+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:49:51.448+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:50:01.684+0000] {processor.py:154} INFO - Started process (PID=1608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:01.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:50:01.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:01.712+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:01.813+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:02.072+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:02.071+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:50:02.219+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:02.218+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:50:02.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.696 seconds
[2022-12-17T04:50:13.078+0000] {processor.py:154} INFO - Started process (PID=1618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:13.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:50:13.112+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:13.111+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:13.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:13.730+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:13.730+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:50:13.895+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:13.894+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:50:14.070+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.006 seconds
[2022-12-17T04:50:24.374+0000] {processor.py:154} INFO - Started process (PID=1628) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:24.388+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:50:24.393+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:24.392+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:24.473+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:24.698+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:24.698+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:50:24.832+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:24.832+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:50:25.345+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.987 seconds
[2022-12-17T04:50:35.605+0000] {processor.py:154} INFO - Started process (PID=1638) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:35.635+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:50:35.640+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:35.639+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:35.720+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:36.040+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:36.038+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:50:36.197+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:36.196+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:50:36.326+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.736 seconds
[2022-12-17T04:50:47.257+0000] {processor.py:154} INFO - Started process (PID=1656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:47.292+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:50:47.311+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:47.295+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:47.494+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:47.780+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:47.779+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:50:47.987+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:47.986+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:50:48.130+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.944 seconds
[2022-12-17T04:50:58.324+0000] {processor.py:154} INFO - Started process (PID=1666) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:58.328+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:50:58.332+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:58.331+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:58.440+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:50:58.910+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:58.909+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:50:59.041+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:50:59.040+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:50:59.202+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.892 seconds
[2022-12-17T04:51:09.715+0000] {processor.py:154} INFO - Started process (PID=1676) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:09.764+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:51:09.769+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:09.768+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:09.882+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:10.330+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:10.329+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:51:10.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:10.466+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:51:10.604+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.904 seconds
[2022-12-17T04:51:20.927+0000] {processor.py:154} INFO - Started process (PID=1693) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:20.959+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:51:20.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:20.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:21.181+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:21.961+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:21.960+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:51:22.237+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:22.236+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:51:22.368+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.478 seconds
[2022-12-17T04:51:32.885+0000] {processor.py:154} INFO - Started process (PID=1704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:33.198+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:51:33.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:33.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:33.290+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:33.498+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:33.497+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:51:33.647+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:33.646+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:51:34.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.172 seconds
[2022-12-17T04:51:44.362+0000] {processor.py:154} INFO - Started process (PID=1714) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:44.385+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:51:44.402+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:44.401+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:44.683+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:45.489+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:45.489+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:51:45.674+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:45.673+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:51:45.783+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.440 seconds
[2022-12-17T04:51:55.916+0000] {processor.py:154} INFO - Started process (PID=1724) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:55.919+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:51:55.923+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:55.922+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:56.004+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:51:56.223+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:56.222+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:51:56.490+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:51:56.489+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:51:56.678+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.776 seconds
[2022-12-17T04:52:06.906+0000] {processor.py:154} INFO - Started process (PID=1742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:06.951+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:52:06.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:06.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:07.086+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:08.394+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:52:19.401+0000] {processor.py:154} INFO - Started process (PID=1752) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:19.427+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:52:19.435+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:19.434+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:19.538+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:19.933+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:19.932+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:52:20.136+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:20.134+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:52:20.303+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.915 seconds
[2022-12-17T04:52:30.865+0000] {processor.py:154} INFO - Started process (PID=1762) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:30.917+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:52:30.921+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:30.920+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:31.013+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:31.514+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:31.513+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:52:31.684+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:31.683+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:52:31.812+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.961 seconds
[2022-12-17T04:52:42.109+0000] {processor.py:154} INFO - Started process (PID=1772) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:42.175+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:52:42.187+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:42.178+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:42.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:43.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:43.018+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:52:43.301+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:43.300+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:52:43.553+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.463 seconds
[2022-12-17T04:52:53.904+0000] {processor.py:154} INFO - Started process (PID=1790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:53.954+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:52:53.958+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:53.957+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:54.046+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:52:54.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:54.921+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:52:55.178+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:52:55.177+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:52:55.317+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.426 seconds
[2022-12-17T04:53:05.681+0000] {processor.py:154} INFO - Started process (PID=1800) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:05.710+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:53:05.723+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:05.713+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:05.978+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:06.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:06.792+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:53:06.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:06.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:53:07.075+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.414 seconds
[2022-12-17T04:53:17.384+0000] {processor.py:154} INFO - Started process (PID=1810) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:17.403+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:53:17.416+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:17.415+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:17.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:17.836+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:17.834+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:53:17.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:17.972+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:53:18.088+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.758 seconds
[2022-12-17T04:53:28.414+0000] {processor.py:154} INFO - Started process (PID=1827) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:28.426+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:53:28.430+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:28.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:28.554+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:28.819+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:28.818+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:53:29.170+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:29.169+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:53:29.505+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.106 seconds
[2022-12-17T04:53:39.739+0000] {processor.py:154} INFO - Started process (PID=1837) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:39.795+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:53:39.800+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:39.799+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:39.927+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:40.718+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:40.717+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:53:41.184+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:41.183+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:53:41.305+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.582 seconds
[2022-12-17T04:53:51.623+0000] {processor.py:154} INFO - Started process (PID=1847) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:51.673+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:53:51.678+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:53:51.677+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:51.764+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:53:52.706+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:54:03.024+0000] {processor.py:154} INFO - Started process (PID=1857) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:03.052+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:54:03.057+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:03.056+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:03.142+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:03.342+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:03.341+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:54:03.473+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:03.472+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:54:03.621+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.611 seconds
[2022-12-17T04:54:13.972+0000] {processor.py:154} INFO - Started process (PID=1875) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:14.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:54:14.035+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:14.034+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:14.202+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:15.832+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:15.831+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:54:16.170+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:16.169+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:54:16.588+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.641 seconds
[2022-12-17T04:54:26.765+0000] {processor.py:154} INFO - Started process (PID=1885) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:26.795+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:54:26.800+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:26.799+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:26.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:27.096+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:27.095+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:54:27.246+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:27.245+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:54:27.393+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.644 seconds
[2022-12-17T04:54:37.714+0000] {processor.py:154} INFO - Started process (PID=1895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:37.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:54:37.741+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:37.735+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:37.842+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:38.143+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:38.142+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:54:38.282+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:38.281+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:54:38.410+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.716 seconds
[2022-12-17T04:54:49.111+0000] {processor.py:154} INFO - Started process (PID=1913) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:49.159+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:54:49.168+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:49.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:49.260+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:54:50.927+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:50.926+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:54:51.205+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:54:51.204+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:54:51.583+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.487 seconds
[2022-12-17T04:55:02.198+0000] {processor.py:154} INFO - Started process (PID=1923) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:02.229+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:55:02.234+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:02.233+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:02.315+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:02.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:02.541+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:55:02.955+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:02.954+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:55:03.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.969 seconds
[2022-12-17T04:55:13.529+0000] {processor.py:154} INFO - Started process (PID=1933) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:13.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:55:13.558+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:13.557+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:13.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:15.037+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:15.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:55:15.179+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:15.178+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:55:15.287+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.774 seconds
[2022-12-17T04:55:25.550+0000] {processor.py:154} INFO - Started process (PID=1943) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:25.564+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:55:25.573+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:25.572+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:25.658+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:26.392+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:26.391+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:55:26.578+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:26.574+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:55:26.733+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.196 seconds
[2022-12-17T04:55:37.033+0000] {processor.py:154} INFO - Started process (PID=1961) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:37.054+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:55:37.059+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:37.058+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:37.141+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:37.393+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:37.392+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:55:37.520+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:37.519+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:55:37.658+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.640 seconds
[2022-12-17T04:55:48.280+0000] {processor.py:154} INFO - Started process (PID=1971) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:48.301+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:55:48.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:48.304+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:48.396+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:49.241+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:55:59.516+0000] {processor.py:154} INFO - Started process (PID=1981) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:59.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:55:59.550+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:59.549+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:59.706+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:55:59.936+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:55:59.935+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:56:00.074+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:00.073+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:56:00.183+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.694 seconds
[2022-12-17T04:56:10.501+0000] {processor.py:154} INFO - Started process (PID=1999) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:10.532+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:56:10.540+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:10.539+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:10.951+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:11.664+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:11.663+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:56:11.814+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:11.813+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:56:11.992+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.538 seconds
[2022-12-17T04:56:22.964+0000] {processor.py:154} INFO - Started process (PID=2009) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:22.995+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:56:23.000+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:22.998+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:23.083+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:23.292+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:23.291+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:56:23.446+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:23.445+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:56:23.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.607 seconds
[2022-12-17T04:56:33.890+0000] {processor.py:154} INFO - Started process (PID=2019) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:33.894+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:56:33.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:33.898+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:34.047+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:35.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:35.221+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:56:35.381+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:35.380+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:56:35.562+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.687 seconds
[2022-12-17T04:56:45.918+0000] {processor.py:154} INFO - Started process (PID=2029) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:45.921+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:56:45.926+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:45.925+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:46.012+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:46.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:46.945+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:56:47.364+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:47.362+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:56:47.672+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.768 seconds
[2022-12-17T04:56:58.122+0000] {processor.py:154} INFO - Started process (PID=2050) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:58.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:56:58.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:58.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:58.326+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:56:58.600+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:58.599+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:56:58.821+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:56:58.820+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:56:58.983+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.877 seconds
[2022-12-17T04:57:09.241+0000] {processor.py:154} INFO - Started process (PID=2060) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:57:09.267+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:57:09.276+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:57:09.275+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:57:09.383+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:57:10.136+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:57:10.135+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:57:10.273+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:57:10.272+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:57:10.426+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.198 seconds
[2022-12-17T04:57:20.765+0000] {processor.py:154} INFO - Started process (PID=2070) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:57:20.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:57:20.785+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:57:20.784+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:57:20.898+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:57:21.317+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:57:21.316+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:57:21.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:57:21.674+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:57:21.848+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.097 seconds
[2022-12-17T04:58:06.958+0000] {processor.py:154} INFO - Started process (PID=170) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:06.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:58:07.001+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:07.000+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:07.230+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:08.550+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T04:58:19.254+0000] {processor.py:154} INFO - Started process (PID=182) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:19.310+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:58:19.314+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:19.313+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:19.448+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:19.780+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:19.779+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:58:19.931+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:19.930+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:58:20.063+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.824 seconds
[2022-12-17T04:58:30.349+0000] {processor.py:154} INFO - Started process (PID=192) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:30.353+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:58:30.361+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:30.360+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:30.459+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:30.959+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:30.958+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:58:31.127+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:31.126+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:58:31.244+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.910 seconds
[2022-12-17T04:58:41.609+0000] {processor.py:154} INFO - Started process (PID=202) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:41.717+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:58:41.729+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:41.728+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:41.886+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:42.656+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:42.654+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:58:42.898+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:42.898+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:58:43.060+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.470 seconds
[2022-12-17T04:58:53.529+0000] {processor.py:154} INFO - Started process (PID=220) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:53.544+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:58:53.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:53.548+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:53.791+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:58:55.244+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:55.243+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:58:55.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:58:55.453+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:58:55.748+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.245 seconds
[2022-12-17T04:59:05.969+0000] {processor.py:154} INFO - Started process (PID=232) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:05.999+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:59:06.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:06.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:06.126+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:06.374+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:06.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:59:06.516+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:06.515+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:59:06.737+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.783 seconds
[2022-12-17T04:59:17.050+0000] {processor.py:154} INFO - Started process (PID=242) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:17.080+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:59:17.096+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:17.083+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:17.245+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:17.497+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:17.497+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:59:17.639+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:17.638+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:59:17.795+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.769 seconds
[2022-12-17T04:59:28.181+0000] {processor.py:154} INFO - Started process (PID=257) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:28.185+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:59:28.189+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:28.188+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:28.304+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:29.085+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:29.084+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:59:29.266+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:29.265+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:59:29.493+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.331 seconds
[2022-12-17T04:59:40.001+0000] {processor.py:154} INFO - Started process (PID=268) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:40.032+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:59:40.047+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:40.042+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:40.211+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:41.237+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:41.236+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:59:41.389+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:41.388+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:59:41.496+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.512 seconds
[2022-12-17T04:59:51.825+0000] {processor.py:154} INFO - Started process (PID=278) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:51.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T04:59:51.880+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:51.879+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:51.968+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T04:59:52.904+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:52.903+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T04:59:53.051+0000] {logging_mixin.py:137} INFO - [2022-12-17T04:59:53.050+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T04:59:53.206+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.400 seconds
[2022-12-17T05:00:03.440+0000] {processor.py:154} INFO - Started process (PID=288) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:03.491+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:00:03.499+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:03.494+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:03.709+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:03.982+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:03.981+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:00:04.169+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:04.168+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:00:04.289+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.865 seconds
[2022-12-17T05:00:14.698+0000] {processor.py:154} INFO - Started process (PID=306) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:14.711+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:00:14.715+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:14.714+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:14.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:15.863+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:15.862+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:00:16.181+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:16.178+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:00:16.450+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.795 seconds
[2022-12-17T05:00:26.786+0000] {processor.py:154} INFO - Started process (PID=316) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:26.833+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:00:26.840+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:26.838+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:26.930+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:27.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:27.224+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:00:27.472+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:27.471+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:00:27.622+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.851 seconds
[2022-12-17T05:00:37.945+0000] {processor.py:154} INFO - Started process (PID=326) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:37.957+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:00:37.966+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:37.962+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:38.071+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:38.926+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:00:49.371+0000] {processor.py:154} INFO - Started process (PID=342) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:49.416+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:00:49.420+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:49.419+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:49.654+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:00:51.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:51.026+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:00:51.307+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:00:51.306+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:00:51.524+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.191 seconds
[2022-12-17T05:01:02.388+0000] {processor.py:154} INFO - Started process (PID=355) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:02.392+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:01:02.396+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:02.395+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:02.484+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:02.732+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:02.731+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:01:02.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:02.877+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:01:02.991+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.616 seconds
[2022-12-17T05:01:13.290+0000] {processor.py:154} INFO - Started process (PID=363) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:13.315+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:01:13.319+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:13.318+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:13.408+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:13.738+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:13.737+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:01:13.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:13.967+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:01:14.103+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.826 seconds
[2022-12-17T05:01:24.443+0000] {processor.py:154} INFO - Started process (PID=373) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:24.518+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:01:24.523+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:24.522+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:24.618+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:24.850+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:24.848+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:01:24.987+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:24.986+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:01:25.179+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.750 seconds
[2022-12-17T05:01:35.502+0000] {processor.py:154} INFO - Started process (PID=387) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:35.530+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:01:35.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:35.541+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:35.671+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:36.611+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:36.609+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:01:36.812+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:36.810+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:01:37.108+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.627 seconds
[2022-12-17T05:01:47.299+0000] {processor.py:154} INFO - Started process (PID=397) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:47.311+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:01:47.316+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:47.315+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:47.398+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:47.613+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:47.612+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:01:47.740+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:47.739+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:01:47.895+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.610 seconds
[2022-12-17T05:01:58.492+0000] {processor.py:154} INFO - Started process (PID=407) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:58.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:01:58.500+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:58.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:58.588+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:01:59.540+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:59.539+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:01:59.688+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:01:59.687+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:02:00.089+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.610 seconds
[2022-12-17T05:02:10.470+0000] {processor.py:154} INFO - Started process (PID=424) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:10.473+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:02:10.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:10.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:10.616+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:10.930+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:10.928+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:02:11.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:11.165+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:02:11.305+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.855 seconds
[2022-12-17T05:02:21.710+0000] {processor.py:154} INFO - Started process (PID=435) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:21.761+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:02:21.765+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:21.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:21.864+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:22.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:22.119+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:02:22.257+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:22.256+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:02:22.410+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.713 seconds
[2022-12-17T05:02:33.164+0000] {processor.py:154} INFO - Started process (PID=445) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:33.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:02:33.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:33.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:33.307+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:33.507+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:33.506+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:02:33.639+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:33.638+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:02:33.778+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.627 seconds
[2022-12-17T05:02:44.431+0000] {processor.py:154} INFO - Started process (PID=455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:44.476+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:02:44.480+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:44.478+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:44.584+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:44.836+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:44.834+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:02:44.992+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:44.991+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:02:45.145+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.731 seconds
[2022-12-17T05:02:55.567+0000] {processor.py:154} INFO - Started process (PID=472) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:55.591+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:02:55.602+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:55.601+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:55.712+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:02:56.379+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:56.378+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:02:56.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:02:56.548+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:02:56.766+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.214 seconds
[2022-12-17T05:03:07.082+0000] {processor.py:154} INFO - Started process (PID=482) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:07.086+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:03:07.090+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:07.089+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:07.173+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:07.374+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:07.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:03:07.521+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:07.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:03:07.926+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.857 seconds
[2022-12-17T05:03:18.289+0000] {processor.py:154} INFO - Started process (PID=492) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:18.339+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:03:18.344+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:18.343+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:18.436+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:19.370+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:03:29.755+0000] {processor.py:154} INFO - Started process (PID=508) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:29.774+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:03:29.782+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:29.781+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:29.932+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:30.437+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:03:41.090+0000] {processor.py:154} INFO - Started process (PID=519) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:41.109+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:03:41.113+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:41.112+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:41.260+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:41.674+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:41.673+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:03:41.810+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:41.809+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:03:41.960+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.885 seconds
[2022-12-17T05:03:52.659+0000] {processor.py:154} INFO - Started process (PID=529) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:52.663+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:03:52.667+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:52.666+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:52.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:03:52.990+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:52.989+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:03:53.119+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:03:53.118+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:03:53.267+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.620 seconds
[2022-12-17T05:04:03.520+0000] {processor.py:154} INFO - Started process (PID=539) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:03.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:04:03.556+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:03.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:03.644+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:03.858+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:03.857+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:04:04.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:04.000+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:04:04.154+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.648 seconds
[2022-12-17T05:04:14.359+0000] {processor.py:154} INFO - Started process (PID=557) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:14.419+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:04:14.427+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:14.426+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:14.743+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:15.212+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:15.211+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:04:15.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:15.400+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:04:15.906+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.562 seconds
[2022-12-17T05:04:26.266+0000] {processor.py:154} INFO - Started process (PID=567) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:26.297+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:04:26.301+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:26.300+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:26.391+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:26.671+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:26.670+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:04:26.864+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:26.861+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:04:27.026+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.774 seconds
[2022-12-17T05:04:37.282+0000] {processor.py:154} INFO - Started process (PID=577) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:37.333+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:04:37.337+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:37.337+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:37.430+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:37.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:37.733+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:04:37.869+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:37.868+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:04:37.998+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.730 seconds
[2022-12-17T05:04:48.372+0000] {processor.py:154} INFO - Started process (PID=587) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:48.404+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:04:48.414+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:48.413+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:48.569+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:48.814+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:48.813+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:04:48.952+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:48.951+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:04:49.098+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.763 seconds
[2022-12-17T05:04:59.644+0000] {processor.py:154} INFO - Started process (PID=605) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:04:59.697+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:04:59.708+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:04:59.707+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:00.272+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:00.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:00.873+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:05:01.012+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:01.011+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:05:01.129+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.528 seconds
[2022-12-17T05:05:11.373+0000] {processor.py:154} INFO - Started process (PID=615) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:11.395+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:05:11.399+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:11.398+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:11.564+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:11.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:11.998+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:05:12.258+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:12.258+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:05:12.424+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.066 seconds
[2022-12-17T05:05:22.742+0000] {processor.py:154} INFO - Started process (PID=625) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:22.786+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:05:22.792+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:22.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:22.889+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:23.951+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:23.950+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:05:24.107+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:24.106+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:05:24.230+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.502 seconds
[2022-12-17T05:05:34.676+0000] {processor.py:154} INFO - Started process (PID=643) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:34.697+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:05:34.713+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:34.707+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:34.816+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:35.139+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:35.138+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:05:35.411+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:35.410+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:05:36.081+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.421 seconds
[2022-12-17T05:05:47.229+0000] {processor.py:154} INFO - Started process (PID=653) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:47.283+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:05:47.292+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:47.286+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:47.421+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:48.151+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:48.150+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:05:48.292+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:48.291+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:05:48.420+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.231 seconds
[2022-12-17T05:05:58.726+0000] {processor.py:154} INFO - Started process (PID=663) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:58.731+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:05:58.736+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:58.734+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:58.861+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:05:59.076+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:59.075+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:05:59.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:05:59.217+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:05:59.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.890 seconds
[2022-12-17T05:06:09.906+0000] {processor.py:154} INFO - Started process (PID=673) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:10.014+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:06:10.018+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:10.017+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:10.113+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:11.741+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:11.740+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:06:12.016+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:12.015+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:06:12.146+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.254 seconds
[2022-12-17T05:06:22.806+0000] {processor.py:154} INFO - Started process (PID=692) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:22.841+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:06:22.858+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:22.851+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:23.117+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:23.765+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:23.765+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:06:23.910+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:23.909+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:06:24.055+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.266 seconds
[2022-12-17T05:06:34.388+0000] {processor.py:154} INFO - Started process (PID=702) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:34.409+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:06:34.413+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:34.412+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:34.522+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:34.733+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:34.732+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:06:34.894+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:34.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:06:35.056+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.684 seconds
[2022-12-17T05:06:45.311+0000] {processor.py:154} INFO - Started process (PID=712) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:45.357+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:06:45.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:45.365+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:45.551+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:46.751+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:46.750+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:06:46.893+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:46.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:06:47.192+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.895 seconds
[2022-12-17T05:06:58.346+0000] {processor.py:154} INFO - Started process (PID=730) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:58.386+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:06:58.393+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:58.392+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:58.691+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:06:59.106+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:59.105+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:06:59.320+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:06:59.319+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:06:59.472+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.322 seconds
[2022-12-17T05:07:09.847+0000] {processor.py:154} INFO - Started process (PID=740) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:09.877+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:07:09.882+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:09.881+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:09.967+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:10.354+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:10.353+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:07:10.481+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:10.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:07:10.592+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.762 seconds
[2022-12-17T05:07:20.881+0000] {processor.py:154} INFO - Started process (PID=750) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:20.902+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:07:20.907+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:20.906+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:21.058+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:22.458+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:07:32.900+0000] {processor.py:154} INFO - Started process (PID=767) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:32.935+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:07:32.943+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:32.942+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:33.121+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:34.501+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:07:45.262+0000] {processor.py:154} INFO - Started process (PID=778) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:45.277+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:07:45.281+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:45.280+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:45.377+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:45.591+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:45.590+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:07:45.727+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:45.726+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:07:45.900+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.651 seconds
[2022-12-17T05:07:56.561+0000] {processor.py:154} INFO - Started process (PID=788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:56.581+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:07:56.585+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:56.584+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:56.681+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:07:56.930+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:56.929+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:07:57.059+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:07:57.058+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:07:57.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.619 seconds
[2022-12-17T05:08:07.476+0000] {processor.py:154} INFO - Started process (PID=798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:07.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:08:07.510+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:07.509+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:07.633+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:07.876+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:07.875+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:08:08.018+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:08.017+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:08:08.133+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.684 seconds
[2022-12-17T05:08:18.850+0000] {processor.py:154} INFO - Started process (PID=817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:18.921+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:08:18.934+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:18.933+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:19.323+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:19.951+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:19.950+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:08:20.153+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:20.152+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:08:20.316+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.510 seconds
[2022-12-17T05:08:30.801+0000] {processor.py:154} INFO - Started process (PID=827) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:30.830+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:08:30.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:30.833+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:30.996+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:32.640+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:08:42.848+0000] {processor.py:154} INFO - Started process (PID=837) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:42.876+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:08:42.883+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:42.882+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:43.012+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:44.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:44.802+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:08:44.992+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:44.992+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:08:45.105+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.293 seconds
[2022-12-17T05:08:55.425+0000] {processor.py:154} INFO - Started process (PID=855) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:55.456+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:08:55.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:55.465+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:55.606+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:08:55.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:55.873+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:08:56.024+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:08:56.023+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:08:56.227+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.820 seconds
[2022-12-17T05:09:06.708+0000] {processor.py:154} INFO - Started process (PID=866) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:06.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:09:06.716+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:06.715+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:06.810+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:07.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:07.463+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:09:07.608+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:07.607+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:09:07.723+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.028 seconds
[2022-12-17T05:09:18.113+0000] {processor.py:154} INFO - Started process (PID=876) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:18.121+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:09:18.126+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:18.125+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:18.235+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:18.446+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:18.445+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:09:18.588+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:18.587+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:09:18.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.635 seconds
[2022-12-17T05:09:29.385+0000] {processor.py:154} INFO - Started process (PID=886) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:29.405+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:09:29.413+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:29.412+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:29.497+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:29.729+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:29.728+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:09:29.870+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:29.870+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:09:29.987+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.616 seconds
[2022-12-17T05:09:40.910+0000] {processor.py:154} INFO - Started process (PID=904) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:40.936+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:09:40.944+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:40.938+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:41.090+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:42.869+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:09:53.377+0000] {processor.py:154} INFO - Started process (PID=914) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:53.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:09:53.428+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:53.427+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:53.535+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:09:54.274+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:54.273+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:09:54.413+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:09:54.412+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:09:54.549+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.184 seconds
[2022-12-17T05:10:04.730+0000] {processor.py:154} INFO - Started process (PID=924) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:04.762+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:10:04.766+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:04.765+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:04.890+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:05.104+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:05.103+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:10:05.240+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:05.239+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:10:05.356+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.639 seconds
[2022-12-17T05:10:15.694+0000] {processor.py:154} INFO - Started process (PID=941) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:15.784+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:10:15.797+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:15.796+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:15.981+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:16.251+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:16.250+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:10:16.404+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:16.404+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:10:16.641+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.008 seconds
[2022-12-17T05:10:27.204+0000] {processor.py:154} INFO - Started process (PID=952) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:27.254+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:10:27.271+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:27.270+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:27.471+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:27.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:27.904+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:10:28.046+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:28.045+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:10:28.158+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.986 seconds
[2022-12-17T05:10:38.495+0000] {processor.py:154} INFO - Started process (PID=962) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:38.546+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:10:38.550+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:38.549+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:38.698+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:39.349+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:39.348+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:10:39.486+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:39.485+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:10:39.609+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.128 seconds
[2022-12-17T05:10:49.719+0000] {processor.py:154} INFO - Started process (PID=972) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:49.759+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:10:49.764+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:49.763+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:49.850+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:10:50.109+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:50.108+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:10:50.529+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:10:50.528+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:10:50.717+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.011 seconds
[2022-12-17T05:11:01.240+0000] {processor.py:154} INFO - Started process (PID=990) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:01.294+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:11:01.298+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:01.297+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:01.435+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:02.594+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:11:12.977+0000] {processor.py:154} INFO - Started process (PID=1000) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:13.006+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:11:13.009+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:13.009+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:13.097+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:13.350+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:13.349+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:11:13.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:13.526+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:11:13.691+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.728 seconds
[2022-12-17T05:11:24.483+0000] {processor.py:154} INFO - Started process (PID=1010) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:24.500+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:11:24.504+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:24.503+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:24.777+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:25.469+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:25.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:11:25.604+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:25.603+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:11:25.764+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.302 seconds
[2022-12-17T05:11:36.061+0000] {processor.py:154} INFO - Started process (PID=1020) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:36.068+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:11:36.072+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:36.071+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:36.161+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:36.383+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:36.382+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:11:36.514+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:36.513+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:11:36.805+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.760 seconds
[2022-12-17T05:11:47.288+0000] {processor.py:154} INFO - Started process (PID=1038) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:47.344+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:11:47.348+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:47.347+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:47.435+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:47.698+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:47.697+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:11:47.891+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:47.890+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:11:48.173+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.922 seconds
[2022-12-17T05:11:58.521+0000] {processor.py:154} INFO - Started process (PID=1048) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:58.524+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:11:58.528+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:58.527+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:58.624+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:11:58.871+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:58.870+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:11:59.001+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:11:59.000+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:11:59.150+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.643 seconds
[2022-12-17T05:12:09.335+0000] {processor.py:154} INFO - Started process (PID=1058) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:09.383+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:12:09.386+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:09.385+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:09.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:09.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:09.844+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:12:09.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:09.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:12:10.204+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.888 seconds
[2022-12-17T05:12:20.499+0000] {processor.py:154} INFO - Started process (PID=1075) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:20.553+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:12:20.562+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:20.561+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:20.733+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:21.214+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:21.213+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:12:21.451+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:21.450+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:12:21.636+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.155 seconds
[2022-12-17T05:12:32.025+0000] {processor.py:154} INFO - Started process (PID=1085) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:32.076+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:12:32.080+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:32.080+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:32.171+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:32.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:32.770+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:12:32.921+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:32.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:12:33.210+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.199 seconds
[2022-12-17T05:12:43.474+0000] {processor.py:154} INFO - Started process (PID=1095) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:43.478+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:12:43.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:43.483+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:43.585+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:44.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:44.037+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:12:44.196+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:44.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:12:44.350+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.891 seconds
[2022-12-17T05:12:54.672+0000] {processor.py:154} INFO - Started process (PID=1105) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:54.690+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:12:54.694+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:54.693+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:54.781+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:12:55.565+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:55.564+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:12:55.759+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:12:55.758+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:12:55.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.226 seconds
[2022-12-17T05:13:06.497+0000] {processor.py:154} INFO - Started process (PID=1123) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:06.534+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:13:06.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:06.541+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:06.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:07.437+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:07.436+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:13:07.718+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:07.718+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:13:07.913+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.465 seconds
[2022-12-17T05:13:18.171+0000] {processor.py:154} INFO - Started process (PID=1133) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:18.181+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:13:18.185+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:18.184+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:18.312+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:18.540+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:18.539+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:13:18.706+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:18.705+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:13:18.852+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.704 seconds
[2022-12-17T05:13:29.157+0000] {processor.py:154} INFO - Started process (PID=1143) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:29.198+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:13:29.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:29.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:29.291+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:29.512+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:29.511+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:13:29.716+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:29.715+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:13:29.898+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.756 seconds
[2022-12-17T05:13:40.047+0000] {processor.py:154} INFO - Started process (PID=1160) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:40.091+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:13:40.095+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:40.094+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:40.315+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:40.781+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:40.780+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:13:41.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:41.061+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:13:41.198+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.182 seconds
[2022-12-17T05:13:51.690+0000] {processor.py:154} INFO - Started process (PID=1171) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:51.694+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:13:51.698+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:51.697+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:51.785+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:13:52.023+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:52.022+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:13:52.218+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:13:52.218+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:13:52.386+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.710 seconds
[2022-12-17T05:14:02.692+0000] {processor.py:154} INFO - Started process (PID=1181) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:02.718+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:14:02.724+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:02.723+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:02.811+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:03.377+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:14:13.731+0000] {processor.py:154} INFO - Started process (PID=1189) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:13.779+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:14:13.787+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:13.786+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:13.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:14.472+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:14.471+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:14:14.628+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:14.627+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:14:14.756+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.038 seconds
[2022-12-17T05:14:25.168+0000] {processor.py:154} INFO - Started process (PID=1208) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:25.225+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:14:25.237+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:25.236+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:25.423+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:26.041+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:26.040+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:14:26.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:26.588+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:14:26.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.569 seconds
[2022-12-17T05:14:37.187+0000] {processor.py:154} INFO - Started process (PID=1218) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:37.201+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:14:37.206+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:37.205+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:37.312+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:37.579+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:37.578+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:14:37.718+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:37.718+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:14:37.869+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.703 seconds
[2022-12-17T05:14:48.149+0000] {processor.py:154} INFO - Started process (PID=1228) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:48.173+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:14:48.177+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:48.176+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:48.302+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:48.576+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:48.575+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:14:48.775+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:48.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:14:49.114+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.983 seconds
[2022-12-17T05:14:59.442+0000] {processor.py:154} INFO - Started process (PID=1238) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:59.447+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:14:59.451+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:14:59.450+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:14:59.544+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:01.286+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:15:11.760+0000] {processor.py:154} INFO - Started process (PID=1255) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:11.764+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:15:11.769+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:11.768+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:11.936+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:13.231+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:13.230+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:15:13.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:13.362+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:15:13.519+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.774 seconds
[2022-12-17T05:15:23.871+0000] {processor.py:154} INFO - Started process (PID=1267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:23.875+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:15:23.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:23.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:23.968+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:24.299+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:24.297+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:15:24.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:24.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:15:24.628+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.774 seconds
[2022-12-17T05:15:34.981+0000] {processor.py:154} INFO - Started process (PID=1277) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:34.985+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:15:34.989+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:34.988+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:35.172+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:36.092+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:36.091+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:15:36.238+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:36.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:15:36.416+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.448 seconds
[2022-12-17T05:15:46.903+0000] {processor.py:154} INFO - Started process (PID=1295) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:46.950+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:15:46.961+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:46.960+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:47.251+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:47.535+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:47.533+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:15:47.777+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:47.777+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:15:48.021+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.145 seconds
[2022-12-17T05:15:58.426+0000] {processor.py:154} INFO - Started process (PID=1303) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:58.478+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:15:58.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:15:58.483+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:15:58.605+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:00.066+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:16:10.426+0000] {processor.py:154} INFO - Started process (PID=1315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:10.430+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:16:10.434+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:10.433+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:10.537+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:11.672+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:16:22.069+0000] {processor.py:154} INFO - Started process (PID=1332) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:22.077+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:16:22.090+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:22.089+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:22.314+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:23.086+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:23.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:16:23.568+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:23.559+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:16:23.793+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.738 seconds
[2022-12-17T05:16:34.188+0000] {processor.py:154} INFO - Started process (PID=1343) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:34.208+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:16:34.217+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:34.217+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:34.314+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:34.577+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:34.576+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:16:34.733+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:34.733+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:16:34.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.717 seconds
[2022-12-17T05:16:45.193+0000] {processor.py:154} INFO - Started process (PID=1353) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:45.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:16:45.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:45.200+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:45.340+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:45.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:45.760+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:16:45.910+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:45.910+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:16:46.054+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-17T05:16:56.370+0000] {processor.py:154} INFO - Started process (PID=1363) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:56.414+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:16:56.418+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:16:56.417+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:56.512+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:16:57.312+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:17:07.989+0000] {processor.py:154} INFO - Started process (PID=1381) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:07.992+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:17:08.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:08.003+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:08.151+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:08.569+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:08.568+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:17:08.823+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:08.822+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:17:09.141+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.165 seconds
[2022-12-17T05:17:19.360+0000] {processor.py:154} INFO - Started process (PID=1391) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:19.364+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:17:19.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:19.367+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:19.455+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:20.250+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:20.249+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:17:20.457+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:20.456+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:17:20.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.236 seconds
[2022-12-17T05:17:30.937+0000] {processor.py:154} INFO - Started process (PID=1401) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:30.952+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:17:30.957+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:30.956+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:31.047+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:31.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:31.304+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:17:31.445+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:31.445+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:17:31.576+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.657 seconds
[2022-12-17T05:17:41.805+0000] {processor.py:154} INFO - Started process (PID=1409) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:41.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:17:41.828+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:41.827+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:41.918+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:42.469+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:42.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:17:42.656+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:42.654+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:17:42.943+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.153 seconds
[2022-12-17T05:17:53.233+0000] {processor.py:154} INFO - Started process (PID=1427) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:53.281+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:17:53.285+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:53.284+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:53.407+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:17:53.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:53.709+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:17:53.909+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:17:53.906+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:17:54.115+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.900 seconds
[2022-12-17T05:18:04.301+0000] {processor.py:154} INFO - Started process (PID=1437) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:04.312+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:18:04.318+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:04.314+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:04.428+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:04.929+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:04.928+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:18:05.093+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:05.092+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:18:05.242+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.954 seconds
[2022-12-17T05:18:15.423+0000] {processor.py:154} INFO - Started process (PID=1447) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:15.426+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:18:15.431+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:15.430+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:15.546+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:16.249+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:16.248+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:18:16.507+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:16.506+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:18:16.657+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.248 seconds
[2022-12-17T05:18:27.106+0000] {processor.py:154} INFO - Started process (PID=1465) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:27.154+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:18:27.162+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:27.161+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:27.433+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:27.971+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:27.970+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:18:28.257+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:28.256+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:18:28.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.634 seconds
[2022-12-17T05:18:39.537+0000] {processor.py:154} INFO - Started process (PID=1475) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:39.567+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:18:39.575+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:39.573+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:39.710+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:41.181+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:18:51.493+0000] {processor.py:154} INFO - Started process (PID=1485) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:51.497+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:18:51.504+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:51.501+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:51.630+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:18:51.985+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:51.985+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:18:52.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:18:52.146+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:18:52.480+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.001 seconds
[2022-12-17T05:19:02.926+0000] {processor.py:154} INFO - Started process (PID=1497) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:02.930+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:19:02.934+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:02.933+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:03.060+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:03.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:03.972+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:19:04.185+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:04.184+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:19:04.416+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.504 seconds
[2022-12-17T05:19:14.794+0000] {processor.py:154} INFO - Started process (PID=1516) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:14.798+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:19:14.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:14.801+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:14.897+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:15.365+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:15.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:19:15.506+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:15.505+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:19:15.749+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.971 seconds
[2022-12-17T05:19:26.007+0000] {processor.py:154} INFO - Started process (PID=1526) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:26.063+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:19:26.066+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:26.065+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:26.162+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:26.429+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:26.428+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:19:26.614+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:26.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:19:26.793+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.810 seconds
[2022-12-17T05:19:37.212+0000] {processor.py:154} INFO - Started process (PID=1536) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:37.266+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:19:37.270+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:37.269+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:37.385+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:37.672+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:37.671+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:19:37.818+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:37.817+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:19:38.106+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.908 seconds
[2022-12-17T05:19:48.795+0000] {processor.py:154} INFO - Started process (PID=1553) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:48.845+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:19:48.852+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:19:48.851+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:49.006+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:19:50.221+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:20:01.291+0000] {processor.py:154} INFO - Started process (PID=1563) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:01.309+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:20:01.318+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:01.317+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:01.417+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:01.680+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:01.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:20:01.821+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:01.820+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:20:01.966+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.690 seconds
[2022-12-17T05:20:12.273+0000] {processor.py:154} INFO - Started process (PID=1573) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:12.277+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:20:12.281+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:12.280+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:12.367+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:12.654+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:12.653+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:20:12.887+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:12.886+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:20:13.002+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.741 seconds
[2022-12-17T05:20:23.471+0000] {processor.py:154} INFO - Started process (PID=1583) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:23.516+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:20:23.521+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:23.520+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:23.634+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:23.888+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:23.887+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:20:24.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:24.029+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:20:24.613+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.156 seconds
[2022-12-17T05:20:35.113+0000] {processor.py:154} INFO - Started process (PID=1601) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:35.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:20:35.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:35.119+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:35.207+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:35.709+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:20:46.501+0000] {processor.py:154} INFO - Started process (PID=1611) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:46.552+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:20:46.556+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:46.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:46.682+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:46.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:46.937+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:20:47.080+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:47.079+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:20:47.224+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.738 seconds
[2022-12-17T05:20:57.890+0000] {processor.py:154} INFO - Started process (PID=1621) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:57.921+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:20:57.926+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:57.925+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:58.021+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:20:58.416+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:58.415+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:20:58.566+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:20:58.565+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:20:58.786+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.909 seconds
[2022-12-17T05:21:09.194+0000] {processor.py:154} INFO - Started process (PID=1639) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:09.222+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:21:09.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:09.225+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:09.430+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:10.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:10.365+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:21:10.625+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:10.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:21:10.789+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.634 seconds
[2022-12-17T05:21:21.175+0000] {processor.py:154} INFO - Started process (PID=1649) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:21.178+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:21:21.182+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:21.181+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:21.305+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:21.564+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:21.563+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:21:21.721+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:21.720+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:21:22.123+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.965 seconds
[2022-12-17T05:21:32.496+0000] {processor.py:154} INFO - Started process (PID=1659) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:32.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:21:32.551+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:32.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:32.651+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:34.281+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:21:44.628+0000] {processor.py:154} INFO - Started process (PID=1669) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:44.680+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:21:44.684+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:44.683+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:44.776+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:45.040+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:45.038+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:21:45.278+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:45.278+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:21:45.441+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.829 seconds
[2022-12-17T05:21:55.872+0000] {processor.py:154} INFO - Started process (PID=1686) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:55.876+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:21:55.880+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:21:55.879+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:55.970+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:21:56.981+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:22:07.349+0000] {processor.py:154} INFO - Started process (PID=1696) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:07.352+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:22:07.356+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:07.355+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:07.449+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:08.274+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:22:18.630+0000] {processor.py:154} INFO - Started process (PID=1704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:18.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:22:18.643+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:18.642+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:18.749+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:19.125+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:19.124+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:22:19.265+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:19.264+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:22:19.397+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.781 seconds
[2022-12-17T05:22:30.017+0000] {processor.py:154} INFO - Started process (PID=1722) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:30.021+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:22:30.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:30.037+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:30.244+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:31.349+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:22:41.715+0000] {processor.py:154} INFO - Started process (PID=1732) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:41.719+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:22:41.726+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:41.725+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:41.852+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:42.214+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:42.213+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:22:42.441+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:42.439+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:22:42.616+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.916 seconds
[2022-12-17T05:22:53.042+0000] {processor.py:154} INFO - Started process (PID=1742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:53.056+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:22:53.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:53.068+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:53.224+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:22:54.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:54.466+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:22:54.633+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:22:54.632+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:22:54.790+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.788 seconds
[2022-12-17T05:23:04.939+0000] {processor.py:154} INFO - Started process (PID=1754) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:04.943+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:23:04.947+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:04.946+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:05.046+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:05.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:05.769+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:23:05.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:05.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:23:06.088+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.164 seconds
[2022-12-17T05:23:16.532+0000] {processor.py:154} INFO - Started process (PID=1770) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:16.760+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:23:16.768+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:16.767+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:17.037+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:17.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:17.367+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:23:17.553+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:17.552+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:23:17.745+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.246 seconds
[2022-12-17T05:23:28.126+0000] {processor.py:154} INFO - Started process (PID=1779) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:28.162+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:23:28.168+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:28.166+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:28.265+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:29.322+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:23:39.634+0000] {processor.py:154} INFO - Started process (PID=1790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:39.725+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:23:39.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:39.733+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:39.848+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:40.602+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:40.601+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:23:40.764+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:40.763+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:23:40.911+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.292 seconds
[2022-12-17T05:23:51.435+0000] {processor.py:154} INFO - Started process (PID=1807) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:51.439+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:23:51.443+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:51.442+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:51.648+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:23:51.979+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:51.978+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:23:52.237+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:23:52.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:23:52.900+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.518 seconds
[2022-12-17T05:24:03.369+0000] {processor.py:154} INFO - Started process (PID=1817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:03.400+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:24:03.405+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:03.404+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:03.512+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:04.833+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:04.832+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:24:05.001+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:05.000+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:24:05.138+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.783 seconds
[2022-12-17T05:24:15.433+0000] {processor.py:154} INFO - Started process (PID=1829) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:15.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:24:15.441+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:15.440+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:15.527+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:15.754+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:15.753+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:24:15.895+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:15.894+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:24:16.113+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.693 seconds
[2022-12-17T05:24:26.341+0000] {processor.py:154} INFO - Started process (PID=1837) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:26.345+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:24:26.349+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:26.348+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:26.438+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:27.061+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:27.060+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:24:27.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:27.222+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:24:27.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.108 seconds
[2022-12-17T05:24:37.904+0000] {processor.py:154} INFO - Started process (PID=1855) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:37.920+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:24:37.924+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:37.923+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:38.043+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:38.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:38.710+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:24:39.070+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:39.069+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:24:39.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.441 seconds
[2022-12-17T05:24:49.583+0000] {processor.py:154} INFO - Started process (PID=1864) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:49.587+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:24:49.597+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:49.594+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:49.716+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:24:50.179+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:50.178+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:24:50.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:24:50.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:24:50.453+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.911 seconds
[2022-12-17T05:25:00.817+0000] {processor.py:154} INFO - Started process (PID=1875) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:00.863+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:25:00.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:00.866+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:01.020+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:02.516+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:02.515+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:25:02.758+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:02.757+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:25:02.992+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.195 seconds
[2022-12-17T05:25:13.188+0000] {processor.py:154} INFO - Started process (PID=1893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:13.193+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:25:13.198+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:13.197+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:13.354+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:13.778+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:13.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:25:13.928+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:13.927+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:25:14.151+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.987 seconds
[2022-12-17T05:25:24.554+0000] {processor.py:154} INFO - Started process (PID=1900) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:24.558+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:25:24.566+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:24.563+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:24.659+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:24.894+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:24.893+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:25:25.058+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:25.057+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:25:25.173+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.636 seconds
[2022-12-17T05:25:35.956+0000] {processor.py:154} INFO - Started process (PID=1910) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:36.000+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:25:36.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:36.003+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:36.091+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:36.300+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:36.299+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:25:36.433+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:36.432+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:25:36.573+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.632 seconds
[2022-12-17T05:25:47.408+0000] {processor.py:154} INFO - Started process (PID=1920) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:47.428+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:25:47.433+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:47.432+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:47.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:48.460+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:48.459+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:25:48.605+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:48.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:25:48.743+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.348 seconds
[2022-12-17T05:25:59.089+0000] {processor.py:154} INFO - Started process (PID=1937) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:59.161+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:25:59.169+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:59.168+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:59.282+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:25:59.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:59.664+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:25:59.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:25:59.966+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:26:00.265+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.207 seconds
[2022-12-17T05:26:10.621+0000] {processor.py:154} INFO - Started process (PID=1947) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:10.641+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:26:10.646+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:10.645+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:10.729+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:10.942+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:10.941+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:26:11.103+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:11.102+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:26:11.248+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.641 seconds
[2022-12-17T05:26:21.528+0000] {processor.py:154} INFO - Started process (PID=1957) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:21.557+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:26:21.577+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:21.567+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:21.686+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:21.908+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:21.907+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:26:22.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:22.044+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:26:22.167+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.654 seconds
[2022-12-17T05:26:32.584+0000] {processor.py:154} INFO - Started process (PID=1974) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:32.594+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:26:32.605+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:32.604+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:32.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:33.494+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:33.493+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:26:33.748+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:33.747+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:26:34.022+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.474 seconds
[2022-12-17T05:26:44.408+0000] {processor.py:154} INFO - Started process (PID=1985) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:44.411+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:26:44.418+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:44.417+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:44.534+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:46.032+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:46.031+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:26:46.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:46.165+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:26:46.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.909 seconds
[2022-12-17T05:26:57.140+0000] {processor.py:154} INFO - Started process (PID=1995) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:57.144+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:26:57.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:57.147+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:57.236+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:26:57.887+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:57.886+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:26:58.037+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:26:58.036+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:26:58.144+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.018 seconds
[2022-12-17T05:27:08.486+0000] {processor.py:154} INFO - Started process (PID=2005) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:08.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:27:08.518+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:08.517+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:08.613+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:08.821+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:08.820+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:27:08.961+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:08.961+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:27:09.124+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.654 seconds
[2022-12-17T05:27:19.726+0000] {processor.py:154} INFO - Started process (PID=2024) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:19.921+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:27:19.925+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:19.924+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:20.289+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:21.875+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:21.874+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:27:22.180+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:22.178+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:27:22.321+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.668 seconds
[2022-12-17T05:27:32.661+0000] {processor.py:154} INFO - Started process (PID=2034) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:32.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:27:32.717+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:32.716+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:32.807+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:34.101+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:27:45.023+0000] {processor.py:154} INFO - Started process (PID=2044) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:45.043+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:27:45.047+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:45.046+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:45.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:45.446+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:45.445+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:27:45.578+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:45.577+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:27:45.723+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.748 seconds
[2022-12-17T05:27:56.448+0000] {processor.py:154} INFO - Started process (PID=2061) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:56.469+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:27:56.480+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:27:56.479+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:56.581+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:27:57.357+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:28:07.849+0000] {processor.py:154} INFO - Started process (PID=2071) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:07.872+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:28:07.877+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:07.875+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:07.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:08.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:08.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:28:08.296+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:08.295+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:28:08.441+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.613 seconds
[2022-12-17T05:28:19.293+0000] {processor.py:154} INFO - Started process (PID=2081) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:19.341+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:28:19.346+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:19.344+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:19.433+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:20.241+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:28:30.613+0000] {processor.py:154} INFO - Started process (PID=2091) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:30.654+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:28:30.658+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:30.657+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:30.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:32.215+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:32.214+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:28:32.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:32.356+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:28:32.830+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.231 seconds
[2022-12-17T05:28:43.372+0000] {processor.py:154} INFO - Started process (PID=2109) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:43.397+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:28:43.413+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:43.412+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:43.511+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:44.332+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:44.328+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:28:44.547+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:44.546+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:28:44.679+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.325 seconds
[2022-12-17T05:28:55.014+0000] {processor.py:154} INFO - Started process (PID=2119) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:55.067+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:28:55.071+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:55.070+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:55.174+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:28:55.406+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:55.405+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:28:55.540+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:28:55.539+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:28:55.692+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.692 seconds
[2022-12-17T05:29:06.441+0000] {processor.py:154} INFO - Started process (PID=2129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:06.445+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:29:06.451+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:06.450+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:06.531+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:06.747+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:06.746+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:29:06.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:06.898+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:29:07.052+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.626 seconds
[2022-12-17T05:29:17.414+0000] {processor.py:154} INFO - Started process (PID=2147) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:17.418+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:29:17.423+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:17.422+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:17.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:18.419+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:18.418+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:29:18.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:18.597+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:29:18.731+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.333 seconds
[2022-12-17T05:29:29.210+0000] {processor.py:154} INFO - Started process (PID=2157) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:29.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:29:29.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:29.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:29.321+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:29.530+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:29.527+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:29:29.672+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:29.671+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:29:29.799+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.604 seconds
[2022-12-17T05:29:40.147+0000] {processor.py:154} INFO - Started process (PID=2167) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:40.192+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:29:40.196+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:40.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:40.278+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:41.539+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:29:51.917+0000] {processor.py:154} INFO - Started process (PID=2177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:51.939+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:29:51.944+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:51.943+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:52.033+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:29:52.273+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:52.272+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:29:52.424+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:29:52.423+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:29:52.650+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.748 seconds
[2022-12-17T05:30:03.314+0000] {processor.py:154} INFO - Started process (PID=2195) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:03.330+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:30:03.333+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:03.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:03.666+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:05.470+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:05.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:30:05.688+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:05.687+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:30:05.929+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.659 seconds
[2022-12-17T05:30:16.642+0000] {processor.py:154} INFO - Started process (PID=2205) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:16.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:30:16.697+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:16.695+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:16.805+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:17.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:17.463+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:30:17.604+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:17.603+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:30:17.744+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.116 seconds
[2022-12-17T05:30:28.100+0000] {processor.py:154} INFO - Started process (PID=2215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:28.127+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:30:28.135+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:28.133+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:28.215+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:28.427+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:28.426+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:30:28.597+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:28.596+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:30:28.711+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.627 seconds
[2022-12-17T05:30:38.995+0000] {processor.py:154} INFO - Started process (PID=2233) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:39.019+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:30:39.031+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:39.026+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:39.133+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:39.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:39.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:30:39.575+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:39.574+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:30:39.744+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.765 seconds
[2022-12-17T05:30:49.924+0000] {processor.py:154} INFO - Started process (PID=2243) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:49.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:30:49.939+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:49.938+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:50.024+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:30:50.967+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:50.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:30:51.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:30:51.100+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:30:51.248+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.338 seconds
[2022-12-17T05:31:01.532+0000] {processor.py:154} INFO - Started process (PID=2253) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:01.580+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:31:01.584+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:01.583+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:01.668+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:02.024+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:02.023+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:31:02.151+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:02.150+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:31:02.288+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.772 seconds
[2022-12-17T05:31:13.191+0000] {processor.py:154} INFO - Started process (PID=2263) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:13.226+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:31:13.230+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:13.229+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:13.338+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:13.817+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:13.816+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:31:13.950+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:13.949+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:31:14.090+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.912 seconds
[2022-12-17T05:31:24.554+0000] {processor.py:154} INFO - Started process (PID=2282) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:24.589+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:31:24.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:24.597+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:24.771+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:25.561+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:25.560+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:31:25.740+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:25.739+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:31:25.903+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.364 seconds
[2022-12-17T05:31:36.355+0000] {processor.py:154} INFO - Started process (PID=2292) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:36.368+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:31:36.371+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:36.370+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:36.480+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:36.705+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:36.704+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:31:36.862+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:36.861+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:31:37.076+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.751 seconds
[2022-12-17T05:31:47.413+0000] {processor.py:154} INFO - Started process (PID=2302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:47.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:31:47.449+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:47.448+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:47.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:48.597+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:31:59.123+0000] {processor.py:154} INFO - Started process (PID=2319) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:59.130+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:31:59.142+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:59.133+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:59.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:31:59.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:31:59.792+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:32:00.032+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:00.025+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:32:00.161+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.084 seconds
[2022-12-17T05:32:10.545+0000] {processor.py:154} INFO - Started process (PID=2330) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:10.576+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:32:10.584+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:10.583+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:10.678+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:11.256+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:11.255+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:32:11.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:11.587+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:32:11.730+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.199 seconds
[2022-12-17T05:32:22.122+0000] {processor.py:154} INFO - Started process (PID=2340) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:22.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:32:22.215+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:22.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:22.303+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:22.769+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:22.767+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:32:22.915+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:22.914+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:32:23.033+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.925 seconds
[2022-12-17T05:32:33.434+0000] {processor.py:154} INFO - Started process (PID=2350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:33.458+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:32:33.469+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:33.468+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:33.614+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:34.445+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:32:44.905+0000] {processor.py:154} INFO - Started process (PID=2367) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:44.952+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:32:44.960+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:44.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:45.198+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:46.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:46.068+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:32:46.430+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:46.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:32:46.645+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.789 seconds
[2022-12-17T05:32:57.141+0000] {processor.py:154} INFO - Started process (PID=2377) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:57.160+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:32:57.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:57.164+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:57.261+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:32:58.156+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:58.155+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:32:58.288+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:32:58.287+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:32:58.451+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.326 seconds
[2022-12-17T05:33:08.860+0000] {processor.py:154} INFO - Started process (PID=2387) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:08.927+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:33:08.935+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:08.934+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:09.052+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:09.730+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:33:20.467+0000] {processor.py:154} INFO - Started process (PID=2402) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:20.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:33:20.500+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:20.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:20.636+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:21.501+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:33:32.168+0000] {processor.py:154} INFO - Started process (PID=2414) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:32.172+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:33:32.179+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:32.175+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:32.265+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:32.646+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:32.645+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:33:32.774+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:32.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:33:32.898+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.758 seconds
[2022-12-17T05:33:43.153+0000] {processor.py:154} INFO - Started process (PID=2424) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:43.200+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:33:43.204+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:43.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:43.293+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:43.582+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:43.581+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:33:43.726+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:43.725+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:33:43.832+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.693 seconds
[2022-12-17T05:33:54.121+0000] {processor.py:154} INFO - Started process (PID=2434) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:54.148+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:33:54.153+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:54.152+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:54.238+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:33:54.966+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:54.965+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:33:55.113+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:33:55.112+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:33:55.220+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.112 seconds
[2022-12-17T05:34:05.483+0000] {processor.py:154} INFO - Started process (PID=2452) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:05.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:34:05.510+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:05.508+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:05.644+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:06.535+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:06.534+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:34:06.779+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:06.778+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:34:06.932+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.488 seconds
[2022-12-17T05:34:17.280+0000] {processor.py:154} INFO - Started process (PID=2462) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:17.306+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:34:17.312+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:17.310+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:17.399+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:17.613+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:17.612+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:34:17.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:17.760+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:34:17.900+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.634 seconds
[2022-12-17T05:34:28.594+0000] {processor.py:154} INFO - Started process (PID=2472) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:28.620+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:34:28.627+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:28.626+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:28.725+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:29.719+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:29.718+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:34:29.889+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:29.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:34:30.003+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-17T05:34:40.357+0000] {processor.py:154} INFO - Started process (PID=2482) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:40.376+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:34:40.397+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:40.396+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:40.600+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:42.076+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:34:52.370+0000] {processor.py:154} INFO - Started process (PID=2500) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:52.440+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:34:52.462+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:34:52.443+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:52.678+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:34:53.668+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:35:03.842+0000] {processor.py:154} INFO - Started process (PID=2510) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:03.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:35:03.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:03.873+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:03.959+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:04.161+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:04.160+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:35:04.294+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:04.294+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:35:04.432+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.607 seconds
[2022-12-17T05:35:14.736+0000] {processor.py:154} INFO - Started process (PID=2520) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:14.761+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:35:14.766+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:14.765+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:14.882+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:15.318+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:15.317+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:35:15.456+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:15.455+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:35:15.611+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.889 seconds
[2022-12-17T05:35:25.992+0000] {processor.py:154} INFO - Started process (PID=2538) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:26.030+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:35:26.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:26.033+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:26.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:26.789+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:26.788+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:35:27.001+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:27.000+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:35:27.169+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.212 seconds
[2022-12-17T05:35:37.735+0000] {processor.py:154} INFO - Started process (PID=2548) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:37.786+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:35:37.790+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:37.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:37.967+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:38.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:38.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:35:38.340+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:38.339+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:35:38.461+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.789 seconds
[2022-12-17T05:35:48.797+0000] {processor.py:154} INFO - Started process (PID=2558) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:48.834+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:35:48.838+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:48.837+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:48.978+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:49.244+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:49.243+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:35:49.379+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:49.379+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:35:49.502+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.747 seconds
[2022-12-17T05:35:59.789+0000] {processor.py:154} INFO - Started process (PID=2568) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:59.838+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:35:59.847+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:35:59.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:35:59.936+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:00.157+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:00.156+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:36:00.339+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:00.338+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:36:00.822+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.046 seconds
[2022-12-17T05:36:11.490+0000] {processor.py:154} INFO - Started process (PID=2586) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:11.493+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:36:11.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:11.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:11.818+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:12.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:12.801+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:36:13.332+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:13.314+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:36:13.638+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.217 seconds
[2022-12-17T05:36:23.924+0000] {processor.py:154} INFO - Started process (PID=2596) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:23.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:36:23.953+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:23.952+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:24.036+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:24.400+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:24.399+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:36:24.681+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:24.680+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:36:24.925+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.015 seconds
[2022-12-17T05:36:35.264+0000] {processor.py:154} INFO - Started process (PID=2606) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:35.306+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:36:35.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:35.309+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:35.416+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:36.902+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:36.901+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:36:37.028+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:37.027+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:36:37.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.917 seconds
[2022-12-17T05:36:47.627+0000] {processor.py:154} INFO - Started process (PID=2623) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:47.649+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:36:47.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:47.664+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:47.877+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:48.178+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:48.177+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:36:48.581+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:48.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:36:48.784+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.230 seconds
[2022-12-17T05:36:59.317+0000] {processor.py:154} INFO - Started process (PID=2634) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:59.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:36:59.390+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:36:59.389+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:36:59.619+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:00.321+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:37:10.826+0000] {processor.py:154} INFO - Started process (PID=2644) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:10.940+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:37:10.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:10.943+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:11.126+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:11.441+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:11.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:37:11.656+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:11.655+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:37:11.773+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.009 seconds
[2022-12-17T05:37:21.967+0000] {processor.py:154} INFO - Started process (PID=2654) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:22.010+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:37:22.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:22.013+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:22.106+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:22.324+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:22.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:37:22.528+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:22.527+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:37:22.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.788 seconds
[2022-12-17T05:37:33.488+0000] {processor.py:154} INFO - Started process (PID=2672) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:33.511+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:37:33.519+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:33.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:33.751+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:34.272+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:34.271+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:37:34.758+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:34.757+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:37:34.960+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.498 seconds
[2022-12-17T05:37:45.362+0000] {processor.py:154} INFO - Started process (PID=2682) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:45.411+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:37:45.416+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:45.415+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:45.500+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:46.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:46.464+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:37:46.604+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:46.603+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:37:46.809+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.459 seconds
[2022-12-17T05:37:57.099+0000] {processor.py:154} INFO - Started process (PID=2692) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:57.146+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:37:57.168+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:57.166+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:57.254+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:37:58.086+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:58.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:37:58.278+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:37:58.277+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:37:58.396+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.310 seconds
[2022-12-17T05:38:08.759+0000] {processor.py:154} INFO - Started process (PID=2709) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:08.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:38:08.807+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:08.806+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:09.035+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:09.811+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:09.810+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:38:10.111+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:10.110+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:38:10.405+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.681 seconds
[2022-12-17T05:38:20.784+0000] {processor.py:154} INFO - Started process (PID=2720) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:20.846+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:38:20.850+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:20.849+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:20.938+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:21.309+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:21.308+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:38:21.633+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:21.632+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:38:21.871+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.099 seconds
[2022-12-17T05:38:32.022+0000] {processor.py:154} INFO - Started process (PID=2730) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:32.071+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:38:32.076+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:32.075+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:32.176+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:32.397+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:32.396+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:38:32.550+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:32.550+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:38:32.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.876 seconds
[2022-12-17T05:38:43.142+0000] {processor.py:154} INFO - Started process (PID=2740) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:43.196+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:38:43.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:43.200+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:43.285+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:43.490+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:43.489+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:38:43.633+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:43.632+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:38:43.778+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.649 seconds
[2022-12-17T05:38:54.099+0000] {processor.py:154} INFO - Started process (PID=2758) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:54.148+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:38:54.156+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:54.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:54.256+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:38:54.590+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:54.589+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:38:54.750+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:38:54.749+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:38:54.921+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.838 seconds
[2022-12-17T05:39:05.310+0000] {processor.py:154} INFO - Started process (PID=2768) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:05.358+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:39:05.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:05.362+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:05.446+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:06.118+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:06.117+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:39:06.255+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:06.254+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:39:06.398+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.103 seconds
[2022-12-17T05:39:16.682+0000] {processor.py:154} INFO - Started process (PID=2778) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:16.709+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:39:16.714+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:16.713+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:16.797+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:17.930+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:17.929+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:39:18.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:18.068+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:39:18.222+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.555 seconds
[2022-12-17T05:39:28.911+0000] {processor.py:154} INFO - Started process (PID=2788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:28.952+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:39:28.956+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:28.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:29.109+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:29.353+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:29.352+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:39:29.566+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:29.565+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:39:29.924+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.030 seconds
[2022-12-17T05:39:40.466+0000] {processor.py:154} INFO - Started process (PID=2806) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:40.488+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:39:40.493+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:40.491+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:40.633+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:40.855+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:40.854+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:39:40.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:40.985+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:39:41.124+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.675 seconds
[2022-12-17T05:39:51.400+0000] {processor.py:154} INFO - Started process (PID=2816) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:51.447+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:39:51.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:51.450+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:51.540+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:39:51.778+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:51.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:39:51.945+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:39:51.944+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:39:52.057+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.671 seconds
[2022-12-17T05:40:02.313+0000] {processor.py:154} INFO - Started process (PID=2826) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:02.340+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:40:02.344+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:02.343+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:02.428+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:02.741+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:02.740+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:40:02.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:02.879+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:40:02.990+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.691 seconds
[2022-12-17T05:40:13.452+0000] {processor.py:154} INFO - Started process (PID=2844) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:13.497+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:40:13.512+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:13.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:13.687+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:14.674+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:40:24.986+0000] {processor.py:154} INFO - Started process (PID=2854) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:25.033+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:40:25.039+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:25.038+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:25.121+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:26.669+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:40:37.216+0000] {processor.py:154} INFO - Started process (PID=2864) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:37.265+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:40:37.270+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:37.269+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:37.354+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:37.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:37.588+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:40:37.726+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:37.725+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:40:37.904+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.702 seconds
[2022-12-17T05:40:48.198+0000] {processor.py:154} INFO - Started process (PID=2874) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:48.246+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:40:48.252+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:40:48.250+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:48.344+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:40:49.831+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:41:00.286+0000] {processor.py:154} INFO - Started process (PID=2892) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:00.313+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:41:00.324+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:00.323+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:00.425+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:00.704+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:00.704+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:41:00.897+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:00.896+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:41:01.080+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.809 seconds
[2022-12-17T05:41:11.402+0000] {processor.py:154} INFO - Started process (PID=2902) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:11.449+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:41:11.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:11.453+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:11.538+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:11.894+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:11.893+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:41:12.043+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:12.041+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:41:12.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.777 seconds
[2022-12-17T05:41:22.418+0000] {processor.py:154} INFO - Started process (PID=2912) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:22.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:41:22.468+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:22.467+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:22.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:22.786+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:22.785+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:41:22.921+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:22.920+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:41:23.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.765 seconds
[2022-12-17T05:41:33.688+0000] {processor.py:154} INFO - Started process (PID=2929) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:33.732+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:41:33.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:33.743+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:33.944+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:34.915+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:41:45.086+0000] {processor.py:154} INFO - Started process (PID=2940) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:45.135+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:41:45.139+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:45.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:45.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:46.086+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:46.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:41:46.340+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:46.339+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:41:46.628+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.557 seconds
[2022-12-17T05:41:57.058+0000] {processor.py:154} INFO - Started process (PID=2950) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:57.079+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:41:57.083+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:57.082+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:57.303+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:41:57.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:57.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:41:57.683+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:41:57.682+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:41:57.863+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.835 seconds
[2022-12-17T05:42:08.165+0000] {processor.py:154} INFO - Started process (PID=2960) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:08.193+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:42:08.198+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:08.197+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:08.282+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:08.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:08.545+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:42:08.688+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:08.687+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:42:08.821+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.673 seconds
[2022-12-17T05:42:19.272+0000] {processor.py:154} INFO - Started process (PID=2978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:19.366+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:42:19.370+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:19.369+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:19.479+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:20.064+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:20.063+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:42:20.227+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:20.226+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:42:20.389+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.140 seconds
[2022-12-17T05:42:30.741+0000] {processor.py:154} INFO - Started process (PID=2988) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:30.765+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:42:30.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:30.769+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:30.861+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:31.067+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:31.066+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:42:31.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:31.200+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:42:31.337+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.610 seconds
[2022-12-17T05:42:42.001+0000] {processor.py:154} INFO - Started process (PID=2998) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:42.034+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:42:42.039+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:42.038+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:42.232+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:42.607+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:42.606+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:42:42.733+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:42.732+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:42:42.870+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.889 seconds
[2022-12-17T05:42:53.394+0000] {processor.py:154} INFO - Started process (PID=3015) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:53.425+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:42:53.429+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:53.428+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:53.532+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:42:53.888+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:53.887+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:42:54.047+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:42:54.046+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:42:54.168+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.789 seconds
[2022-12-17T05:43:04.788+0000] {processor.py:154} INFO - Started process (PID=3026) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:04.795+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:43:04.803+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:04.802+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:05.021+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:05.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:05.548+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:43:05.759+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:05.758+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:43:05.923+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.186 seconds
[2022-12-17T05:43:16.257+0000] {processor.py:154} INFO - Started process (PID=3036) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:16.284+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:43:16.288+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:16.287+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:16.384+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:16.657+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:16.656+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:43:17.006+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:17.006+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:43:17.203+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.960 seconds
[2022-12-17T05:43:27.478+0000] {processor.py:154} INFO - Started process (PID=3046) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:27.531+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:43:27.536+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:27.535+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:27.628+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:28.573+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:28.572+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:43:28.798+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:28.797+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:43:28.958+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.494 seconds
[2022-12-17T05:43:39.283+0000] {processor.py:154} INFO - Started process (PID=3064) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:39.309+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:43:39.318+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:39.317+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:39.425+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:40.933+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:40.931+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:43:41.091+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:41.090+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:43:41.234+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.967 seconds
[2022-12-17T05:43:51.533+0000] {processor.py:154} INFO - Started process (PID=3074) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:51.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:43:51.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:51.545+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:51.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:43:52.355+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:52.354+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:43:52.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:43:52.483+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:43:52.620+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.101 seconds
[2022-12-17T05:44:02.915+0000] {processor.py:154} INFO - Started process (PID=3084) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:02.943+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:44:02.950+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:02.949+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:03.163+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:04.657+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:44:15.181+0000] {processor.py:154} INFO - Started process (PID=3100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:15.236+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:44:15.259+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:15.239+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:15.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:16.239+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:16.238+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:44:16.392+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:16.391+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:44:16.602+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.440 seconds
[2022-12-17T05:44:27.130+0000] {processor.py:154} INFO - Started process (PID=3111) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:27.157+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:44:27.161+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:27.160+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:27.257+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:27.916+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:27.916+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:44:28.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:28.045+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:44:28.224+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.107 seconds
[2022-12-17T05:44:38.545+0000] {processor.py:154} INFO - Started process (PID=3121) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:38.577+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:44:38.582+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:38.581+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:38.679+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:40.299+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:44:51.344+0000] {processor.py:154} INFO - Started process (PID=3131) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:51.366+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:44:51.371+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:51.370+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:51.454+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:44:51.951+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:51.950+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:44:52.089+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:44:52.088+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:44:52.228+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.896 seconds
[2022-12-17T05:45:02.390+0000] {processor.py:154} INFO - Started process (PID=3149) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:02.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:45:02.441+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:02.440+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:02.608+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:03.021+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:03.020+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:45:03.198+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:03.197+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:45:03.336+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.962 seconds
[2022-12-17T05:45:13.678+0000] {processor.py:154} INFO - Started process (PID=3159) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:13.705+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:45:13.709+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:13.708+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:13.797+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:14.261+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:14.260+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:45:14.407+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:14.406+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:45:14.541+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.876 seconds
[2022-12-17T05:45:24.697+0000] {processor.py:154} INFO - Started process (PID=3169) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:24.742+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:45:24.747+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:24.746+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:24.829+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:25.499+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:25.498+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:45:25.640+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:25.639+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:45:25.810+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.127 seconds
[2022-12-17T05:45:36.161+0000] {processor.py:154} INFO - Started process (PID=3186) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:36.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:45:36.200+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:36.196+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:36.304+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:36.895+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:36.894+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:45:37.063+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:37.062+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:45:37.240+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.095 seconds
[2022-12-17T05:45:47.385+0000] {processor.py:154} INFO - Started process (PID=3197) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:47.413+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:45:47.417+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:47.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:47.509+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:47.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:47.760+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:45:47.907+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:47.906+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:45:48.280+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.910 seconds
[2022-12-17T05:45:58.543+0000] {processor.py:154} INFO - Started process (PID=3207) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:58.572+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:45:58.577+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:58.576+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:58.665+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:45:58.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:58.905+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:45:59.029+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:45:59.028+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:45:59.132+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.603 seconds
[2022-12-17T05:46:09.273+0000] {processor.py:154} INFO - Started process (PID=3217) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:09.343+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:46:09.374+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:09.373+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:09.654+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:10.849+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:10.848+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:46:10.984+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:10.983+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:46:11.137+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.890 seconds
[2022-12-17T05:46:21.593+0000] {processor.py:154} INFO - Started process (PID=3235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:21.652+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:46:21.663+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:21.659+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:21.764+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:23.306+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:23.304+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:46:23.468+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:23.467+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:46:23.635+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.100 seconds
[2022-12-17T05:46:33.847+0000] {processor.py:154} INFO - Started process (PID=3245) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:33.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:46:33.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:33.877+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:33.968+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:35.273+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:46:45.441+0000] {processor.py:154} INFO - Started process (PID=3255) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:45.453+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:46:45.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:45.463+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:45.626+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:45.911+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:45.910+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:46:46.053+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:46.052+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:46:46.188+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.764 seconds
[2022-12-17T05:46:56.609+0000] {processor.py:154} INFO - Started process (PID=3270) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:56.633+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:46:56.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:56.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:56.754+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:46:57.565+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:57.564+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:46:57.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:46:57.734+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:46:57.886+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.309 seconds
[2022-12-17T05:47:08.168+0000] {processor.py:154} INFO - Started process (PID=3282) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:08.190+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:47:08.194+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:08.193+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:08.306+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:08.612+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:08.610+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:47:08.817+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:08.816+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:47:08.935+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.805 seconds
[2022-12-17T05:47:19.132+0000] {processor.py:154} INFO - Started process (PID=3292) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:19.183+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:47:19.188+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:19.187+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:19.270+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:19.492+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:19.491+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:47:19.865+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:19.864+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:47:20.053+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.934 seconds
[2022-12-17T05:47:30.339+0000] {processor.py:154} INFO - Started process (PID=3302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:30.386+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:47:30.391+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:30.389+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:30.475+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:31.346+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:31.345+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:47:31.529+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:31.528+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:47:31.720+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.394 seconds
[2022-12-17T05:47:42.178+0000] {processor.py:154} INFO - Started process (PID=3320) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:42.208+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:47:42.220+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:42.219+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:42.334+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:42.630+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:42.629+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:47:42.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:42.801+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:47:42.994+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.843 seconds
[2022-12-17T05:47:53.227+0000] {processor.py:154} INFO - Started process (PID=3330) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:53.256+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:47:53.261+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:53.260+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:53.344+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:47:53.877+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:53.876+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:47:54.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:47:54.013+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:47:54.181+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.967 seconds
[2022-12-17T05:48:04.393+0000] {processor.py:154} INFO - Started process (PID=3340) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:04.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:48:04.441+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:04.440+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:04.542+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:05.169+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:05.168+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:48:05.377+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:05.376+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:48:05.559+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.181 seconds
[2022-12-17T05:48:15.905+0000] {processor.py:154} INFO - Started process (PID=3350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:15.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:48:15.964+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:15.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:16.054+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:16.318+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:16.317+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:48:16.494+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:16.493+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:48:16.642+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.751 seconds
[2022-12-17T05:48:27.198+0000] {processor.py:154} INFO - Started process (PID=3368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:27.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:48:27.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:27.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:27.334+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:27.742+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:27.741+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:48:27.970+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:27.969+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:48:28.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.010 seconds
[2022-12-17T05:48:38.510+0000] {processor.py:154} INFO - Started process (PID=3378) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:38.554+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:48:38.561+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:38.559+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:38.652+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:38.862+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:38.862+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:48:39.016+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:39.015+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:48:39.171+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.676 seconds
[2022-12-17T05:48:49.455+0000] {processor.py:154} INFO - Started process (PID=3388) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:49.480+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:48:49.486+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:49.485+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:49.587+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:48:49.801+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:49.800+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:48:49.964+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:48:49.964+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:48:50.072+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-17T05:49:00.830+0000] {processor.py:154} INFO - Started process (PID=3406) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:00.849+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:49:00.861+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:00.860+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:01.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:01.844+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:01.842+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:49:02.131+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:02.130+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:49:02.284+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.498 seconds
[2022-12-17T05:49:13.134+0000] {processor.py:154} INFO - Started process (PID=3416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:13.181+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:49:13.185+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:13.184+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:13.298+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:13.689+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:13.688+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:49:13.913+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:13.912+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:49:14.051+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.930 seconds
[2022-12-17T05:49:24.527+0000] {processor.py:154} INFO - Started process (PID=3426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:24.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:49:24.552+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:24.552+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:24.652+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:24.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:24.904+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:49:25.042+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:25.041+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:49:25.155+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.645 seconds
[2022-12-17T05:49:35.450+0000] {processor.py:154} INFO - Started process (PID=3436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:35.497+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:49:35.502+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:35.501+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:35.593+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:37.194+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:49:47.594+0000] {processor.py:154} INFO - Started process (PID=3455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:47.674+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:49:47.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:47.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:47.803+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:48.314+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:48.313+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:49:48.614+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:48.614+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:49:48.869+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.308 seconds
[2022-12-17T05:49:59.194+0000] {processor.py:154} INFO - Started process (PID=3465) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:59.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:49:59.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:49:59.223+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:49:59.308+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:00.355+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:00.354+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:50:00.480+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:00.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:50:00.622+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.441 seconds
[2022-12-17T05:50:11.394+0000] {processor.py:154} INFO - Started process (PID=3475) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:11.441+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:50:11.449+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:11.448+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:11.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:11.775+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:11.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:50:11.912+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:11.911+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:50:12.022+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.643 seconds
[2022-12-17T05:50:22.387+0000] {processor.py:154} INFO - Started process (PID=3491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:22.413+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:50:22.416+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:22.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:22.527+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:22.792+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:22.791+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:50:22.966+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:22.965+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:50:23.108+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.747 seconds
[2022-12-17T05:50:33.887+0000] {processor.py:154} INFO - Started process (PID=3502) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:33.935+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:50:33.940+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:33.939+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:34.022+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:34.248+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:34.247+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:50:34.376+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:34.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:50:34.485+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.614 seconds
[2022-12-17T05:50:45.130+0000] {processor.py:154} INFO - Started process (PID=3512) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:45.216+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:50:45.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:45.225+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:45.316+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:45.519+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:45.518+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:50:45.654+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:45.653+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:50:45.913+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.801 seconds
[2022-12-17T05:50:56.165+0000] {processor.py:154} INFO - Started process (PID=3522) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:56.230+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:50:56.236+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:56.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:56.326+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:50:56.538+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:56.537+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:50:56.711+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:50:56.710+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:50:56.834+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.683 seconds
[2022-12-17T05:51:07.618+0000] {processor.py:154} INFO - Started process (PID=3541) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:07.643+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:51:07.647+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:07.646+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:07.817+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:09.063+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:09.061+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:51:09.217+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:09.217+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:51:09.378+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.813 seconds
[2022-12-17T05:51:19.790+0000] {processor.py:154} INFO - Started process (PID=3551) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:19.853+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:51:19.856+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:19.855+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:19.950+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:21.648+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:51:31.926+0000] {processor.py:154} INFO - Started process (PID=3561) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:31.964+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:51:31.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:31.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:32.099+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:32.299+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:32.298+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:51:32.440+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:32.440+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:51:32.634+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.733 seconds
[2022-12-17T05:51:42.862+0000] {processor.py:154} INFO - Started process (PID=3576) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:42.901+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:51:42.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:42.904+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:43.006+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:43.249+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:43.248+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:51:43.410+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:43.409+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:51:43.603+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.774 seconds
[2022-12-17T05:51:54.102+0000] {processor.py:154} INFO - Started process (PID=3588) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:54.126+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:51:54.130+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:54.129+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:54.212+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:51:54.623+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:54.622+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:51:54.756+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:51:54.755+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:51:54.892+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.803 seconds
[2022-12-17T05:52:05.170+0000] {processor.py:154} INFO - Started process (PID=3598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:05.185+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:52:05.189+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:05.188+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:05.301+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:05.534+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:05.533+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:52:05.694+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:05.693+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:52:05.799+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.645 seconds
[2022-12-17T05:52:15.948+0000] {processor.py:154} INFO - Started process (PID=3608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:16.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:52:16.022+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:16.018+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:16.113+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:16.864+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:16.863+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:52:17.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:17.033+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:52:17.212+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.279 seconds
[2022-12-17T05:52:27.362+0000] {processor.py:154} INFO - Started process (PID=3626) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:27.427+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:52:27.436+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:27.430+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:27.537+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:27.823+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:27.822+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:52:28.106+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:28.105+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:52:28.220+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-17T05:52:38.554+0000] {processor.py:154} INFO - Started process (PID=3636) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:38.582+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:52:38.592+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:38.591+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:38.679+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:38.894+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:38.893+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:52:39.020+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:39.019+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:52:39.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.624 seconds
[2022-12-17T05:52:49.505+0000] {processor.py:154} INFO - Started process (PID=3646) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:49.532+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:52:49.541+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:49.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:49.743+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:52:50.079+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:50.078+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:52:50.217+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:52:50.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:52:50.358+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.883 seconds
[2022-12-17T05:53:00.791+0000] {processor.py:154} INFO - Started process (PID=3656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:00.810+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:53:00.815+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:00.814+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:00.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:01.377+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:01.376+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:53:01.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:01.597+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:53:01.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.969 seconds
[2022-12-17T05:53:12.281+0000] {processor.py:154} INFO - Started process (PID=3674) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:12.334+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:53:12.354+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:12.353+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:12.605+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:13.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:13.365+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:53:13.713+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:13.713+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:53:13.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.662 seconds
[2022-12-17T05:53:24.110+0000] {processor.py:154} INFO - Started process (PID=3684) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:24.135+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:53:24.140+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:24.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:24.225+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:24.905+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:53:35.286+0000] {processor.py:154} INFO - Started process (PID=3694) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:35.338+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:53:35.343+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:35.342+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:35.437+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:36.388+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:36.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:53:36.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:36.545+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:53:36.706+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.435 seconds
[2022-12-17T05:53:47.124+0000] {processor.py:154} INFO - Started process (PID=3711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:47.138+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:53:47.141+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:47.140+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:47.257+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:47.676+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:47.675+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:53:48.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:48.309+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:53:48.664+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.565 seconds
[2022-12-17T05:53:59.214+0000] {processor.py:154} INFO - Started process (PID=3722) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:59.260+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:53:59.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:59.263+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:59.361+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:53:59.638+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:59.637+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:53:59.939+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:53:59.938+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:54:00.173+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.992 seconds
[2022-12-17T05:54:10.360+0000] {processor.py:154} INFO - Started process (PID=3732) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:10.407+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:54:10.412+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:10.411+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:10.496+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:10.913+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:10.912+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:54:11.097+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:11.096+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:54:11.645+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.299 seconds
[2022-12-17T05:54:22.273+0000] {processor.py:154} INFO - Started process (PID=3742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:22.307+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:54:22.314+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:22.310+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:22.415+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:23.295+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:23.294+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:54:23.679+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:23.678+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:54:23.885+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.669 seconds
[2022-12-17T05:54:34.183+0000] {processor.py:154} INFO - Started process (PID=3760) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:34.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:54:34.220+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:34.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:34.368+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:34.977+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:34.976+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:54:35.551+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:35.550+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:54:35.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.586 seconds
[2022-12-17T05:54:46.133+0000] {processor.py:154} INFO - Started process (PID=3770) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:46.185+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:54:46.189+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:46.188+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:46.273+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:47.840+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:47.839+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:54:47.985+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:47.984+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:54:48.118+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.000 seconds
[2022-12-17T05:54:58.628+0000] {processor.py:154} INFO - Started process (PID=3780) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:58.661+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:54:58.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:58.664+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:58.759+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:54:58.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:58.972+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:54:59.109+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:54:59.108+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:54:59.254+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.669 seconds
[2022-12-17T05:55:09.598+0000] {processor.py:154} INFO - Started process (PID=3798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:09.653+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:55:09.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:09.665+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:09.799+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:10.626+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:10.621+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:55:10.889+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:10.888+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:55:11.027+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.462 seconds
[2022-12-17T05:55:21.171+0000] {processor.py:154} INFO - Started process (PID=3808) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:21.221+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:55:21.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:21.224+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:21.329+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:21.680+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:21.679+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:55:21.836+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:21.834+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:55:21.958+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.804 seconds
[2022-12-17T05:55:32.295+0000] {processor.py:154} INFO - Started process (PID=3818) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:32.379+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:55:32.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:32.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:32.466+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:34.092+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:55:44.750+0000] {processor.py:154} INFO - Started process (PID=3828) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:44.810+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:55:44.817+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:44.816+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:44.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:45.249+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:45.241+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:55:45.388+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:45.387+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:55:45.505+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.839 seconds
[2022-12-17T05:55:55.798+0000] {processor.py:154} INFO - Started process (PID=3846) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:55.841+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:55:55.857+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:55.856+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:56.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:55:56.768+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:56.767+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:55:57.092+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:55:57.091+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:55:57.218+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.436 seconds
[2022-12-17T05:56:07.564+0000] {processor.py:154} INFO - Started process (PID=3856) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:07.569+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:56:07.578+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:07.577+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:07.669+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:07.888+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:07.886+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:56:08.091+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:08.090+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:56:08.349+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.803 seconds
[2022-12-17T05:56:18.958+0000] {processor.py:154} INFO - Started process (PID=3866) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:18.962+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:56:18.966+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:18.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:19.049+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:20.448+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:56:30.813+0000] {processor.py:154} INFO - Started process (PID=3882) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:30.817+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:56:30.821+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:30.820+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:30.934+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:31.433+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:31.432+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:56:31.863+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:31.862+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:56:31.995+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.196 seconds
[2022-12-17T05:56:42.223+0000] {processor.py:154} INFO - Started process (PID=3893) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:42.275+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:56:42.279+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:42.278+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:42.367+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:42.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:42.579+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:56:42.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:42.709+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:56:42.871+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.668 seconds
[2022-12-17T05:56:53.461+0000] {processor.py:154} INFO - Started process (PID=3903) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:53.465+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:56:53.469+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:53.468+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:53.563+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:56:53.790+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:53.789+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:56:54.022+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:56:54.021+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:56:54.135+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.688 seconds
[2022-12-17T05:57:04.470+0000] {processor.py:154} INFO - Started process (PID=3913) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:04.517+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:57:04.526+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:04.525+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:04.640+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:05.186+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:05.185+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:57:05.312+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:05.311+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:57:05.441+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.986 seconds
[2022-12-17T05:57:15.607+0000] {processor.py:154} INFO - Started process (PID=3932) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:15.846+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:57:15.856+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:15.854+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:15.964+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:16.260+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:16.259+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:57:16.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:16.425+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:57:16.650+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.060 seconds
[2022-12-17T05:57:27.057+0000] {processor.py:154} INFO - Started process (PID=3942) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:27.078+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:57:27.083+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:27.082+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:27.166+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:27.644+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:27.643+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:57:27.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:27.769+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:57:27.918+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.874 seconds
[2022-12-17T05:57:38.594+0000] {processor.py:154} INFO - Started process (PID=3952) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:38.646+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:57:38.651+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:38.650+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:38.735+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:39.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:39.733+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:57:39.869+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:39.868+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:57:39.999+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.419 seconds
[2022-12-17T05:57:50.312+0000] {processor.py:154} INFO - Started process (PID=3962) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:50.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:57:50.320+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:50.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:50.402+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:57:50.614+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:50.613+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:57:50.760+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:57:50.759+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:57:50.915+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.618 seconds
[2022-12-17T05:58:01.336+0000] {processor.py:154} INFO - Started process (PID=3980) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:01.395+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:58:01.406+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:01.401+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:01.507+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:02.188+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:58:09.623+0000] {processor.py:154} INFO - Started process (PID=4003) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:09.644+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:58:09.652+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:09.651+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:09.813+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:10.787+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:10.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:58:11.149+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:11.148+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:58:11.321+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.745 seconds
[2022-12-17T05:58:21.556+0000] {processor.py:154} INFO - Started process (PID=4013) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:21.575+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:58:21.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:21.579+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:21.678+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:22.262+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:22.261+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:58:22.479+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:22.477+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:58:22.633+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.121 seconds
[2022-12-17T05:58:32.944+0000] {processor.py:154} INFO - Started process (PID=4020) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:32.966+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:58:32.971+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:32.970+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:33.186+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:33.512+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:33.494+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:58:34.123+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:34.122+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:58:34.358+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.427 seconds
[2022-12-17T05:58:44.793+0000] {processor.py:154} INFO - Started process (PID=4038) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:44.813+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:58:44.816+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:44.815+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:44.918+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:45.638+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:45.636+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:58:46.018+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:46.017+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:58:46.206+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.427 seconds
[2022-12-17T05:58:56.485+0000] {processor.py:154} INFO - Started process (PID=4048) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:56.512+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:58:56.517+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:56.516+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:56.609+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:58:57.282+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:57.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:58:57.599+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:58:57.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:58:57.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.256 seconds
[2022-12-17T05:59:07.996+0000] {processor.py:154} INFO - Started process (PID=4058) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:08.025+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:59:08.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:08.029+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:08.123+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:08.523+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:59:19.271+0000] {processor.py:154} INFO - Started process (PID=4075) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:19.349+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:59:19.353+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:19.352+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:19.473+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:20.182+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:20.181+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:59:20.326+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:20.325+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:59:20.473+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.219 seconds
[2022-12-17T05:59:30.917+0000] {processor.py:154} INFO - Started process (PID=4085) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:30.983+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:59:30.987+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:30.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:31.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:31.216+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:31.214+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:59:31.325+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:31.324+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:59:31.526+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.629 seconds
[2022-12-17T05:59:41.895+0000] {processor.py:154} INFO - Started process (PID=4095) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:41.954+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:59:41.960+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:41.959+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:42.050+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:43.295+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T05:59:53.633+0000] {processor.py:154} INFO - Started process (PID=4105) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:53.637+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T05:59:53.642+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:53.641+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:53.734+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T05:59:53.875+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:53.874+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T05:59:53.984+0000] {logging_mixin.py:137} INFO - [2022-12-17T05:59:53.983+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T05:59:54.113+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.494 seconds
[2022-12-17T06:00:04.485+0000] {processor.py:154} INFO - Started process (PID=4124) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:04.490+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:00:04.495+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:04.493+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:04.609+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:05.813+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:05.812+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:00:05.934+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:05.934+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:00:06.072+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.604 seconds
[2022-12-17T06:00:16.711+0000] {processor.py:154} INFO - Started process (PID=4134) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:16.714+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:00:16.718+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:16.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:16.801+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:17.248+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:17.247+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:00:17.359+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:17.358+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:00:17.459+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.762 seconds
[2022-12-17T06:00:27.704+0000] {processor.py:154} INFO - Started process (PID=4144) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:27.707+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:00:27.711+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:27.710+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:27.794+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:28.082+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:28.081+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:00:28.194+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:28.193+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:00:28.302+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.612 seconds
[2022-12-17T06:00:38.624+0000] {processor.py:154} INFO - Started process (PID=4163) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:38.628+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:00:38.632+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:38.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:38.732+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:38.918+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:38.917+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:00:39.053+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:39.052+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:00:39.235+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.626 seconds
[2022-12-17T06:00:49.814+0000] {processor.py:154} INFO - Started process (PID=4173) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:49.860+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:00:49.864+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:00:49.863+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:49.952+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:00:50.945+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:01:01.280+0000] {processor.py:154} INFO - Started process (PID=4183) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:01.284+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:01:01.290+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:01.289+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:01.370+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:01.871+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:01:12.214+0000] {processor.py:154} INFO - Started process (PID=4193) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:12.262+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:01:12.267+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:12.266+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:12.351+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:13.025+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:13.024+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:01:13.142+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:13.141+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:01:13.263+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.062 seconds
[2022-12-17T06:01:23.464+0000] {processor.py:154} INFO - Started process (PID=4210) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:23.476+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:01:23.487+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:23.483+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:23.632+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:24.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:24.014+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:01:24.246+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:24.245+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:01:24.425+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.976 seconds
[2022-12-17T06:01:34.885+0000] {processor.py:154} INFO - Started process (PID=4220) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:34.889+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:01:34.893+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:34.892+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:34.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:35.111+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:35.110+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:01:35.256+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:35.255+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:01:35.368+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.500 seconds
[2022-12-17T06:01:45.737+0000] {processor.py:154} INFO - Started process (PID=4230) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:45.742+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:01:45.749+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:45.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:45.846+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:46.021+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:46.020+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:01:46.179+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:46.175+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:01:46.350+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.627 seconds
[2022-12-17T06:01:57.250+0000] {processor.py:154} INFO - Started process (PID=4246) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:57.260+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:01:57.269+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:57.268+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:57.389+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:01:57.557+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:57.556+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:01:57.701+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:01:57.700+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:01:57.876+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.665 seconds
[2022-12-17T06:02:08.294+0000] {processor.py:154} INFO - Started process (PID=4258) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:08.298+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:02:08.302+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:08.301+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:08.384+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:08.517+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:08.516+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:02:08.645+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:08.645+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:02:08.778+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.501 seconds
[2022-12-17T06:02:19.100+0000] {processor.py:154} INFO - Started process (PID=4268) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:19.146+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:02:19.151+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:19.150+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:19.237+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:19.377+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:19.376+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:02:19.518+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:19.517+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:02:19.628+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.542 seconds
[2022-12-17T06:02:29.901+0000] {processor.py:154} INFO - Started process (PID=4278) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:29.950+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:02:29.956+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:29.954+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:30.107+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:30.911+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:02:41.154+0000] {processor.py:154} INFO - Started process (PID=4295) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:41.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:02:41.207+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:41.206+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:41.446+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:42.163+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:42.158+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:02:42.814+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:42.813+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:02:43.061+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.921 seconds
[2022-12-17T06:02:53.456+0000] {processor.py:154} INFO - Started process (PID=4305) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:53.485+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:02:53.492+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:53.491+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:53.737+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:02:54.316+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:54.315+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:02:54.430+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:02:54.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:02:54.540+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.103 seconds
[2022-12-17T06:03:04.816+0000] {processor.py:154} INFO - Started process (PID=4315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:04.866+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:03:04.871+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:04.869+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:05.003+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:05.604+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:05.603+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:03:05.723+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:05.723+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:03:05.832+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.031 seconds
[2022-12-17T06:03:16.222+0000] {processor.py:154} INFO - Started process (PID=4325) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:16.245+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:03:16.249+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:16.248+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:16.336+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:17.115+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:17.114+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:03:17.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:17.232+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:03:17.369+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.160 seconds
[2022-12-17T06:03:27.791+0000] {processor.py:154} INFO - Started process (PID=4344) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:27.827+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:03:27.856+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:27.849+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:27.958+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:28.104+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:28.103+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:03:28.503+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:28.471+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:03:28.854+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.077 seconds
[2022-12-17T06:03:39.173+0000] {processor.py:154} INFO - Started process (PID=4354) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:39.215+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:03:39.223+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:39.222+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:39.304+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:39.438+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:39.437+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:03:39.596+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:39.595+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:03:39.720+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.561 seconds
[2022-12-17T06:03:50.048+0000] {processor.py:154} INFO - Started process (PID=4364) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:50.076+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:03:50.080+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:50.079+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:50.171+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:03:50.814+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:50.813+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:03:50.931+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:03:50.930+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:03:51.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.172 seconds
[2022-12-17T06:04:01.724+0000] {processor.py:154} INFO - Started process (PID=4382) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:01.775+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:04:01.783+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:01.782+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:01.893+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:02.767+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:02.765+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:04:03.050+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:03.049+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:04:03.281+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.587 seconds
[2022-12-17T06:04:13.694+0000] {processor.py:154} INFO - Started process (PID=4392) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:13.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:04:13.702+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:13.701+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:13.802+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:14.967+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:14.966+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:04:15.129+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:15.128+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:04:15.272+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.594 seconds
[2022-12-17T06:04:25.768+0000] {processor.py:154} INFO - Started process (PID=4402) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:25.789+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:04:25.796+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:25.795+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:25.967+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:26.568+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:26.567+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:04:26.690+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:26.690+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:04:26.856+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.109 seconds
[2022-12-17T06:04:37.305+0000] {processor.py:154} INFO - Started process (PID=4412) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:37.335+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:04:37.343+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:37.342+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:37.468+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:37.632+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:37.631+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:04:37.757+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:37.756+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:04:37.918+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.629 seconds
[2022-12-17T06:04:48.462+0000] {processor.py:154} INFO - Started process (PID=4430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:48.476+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:04:48.490+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:48.489+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:48.654+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:04:49.422+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:49.421+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:04:49.673+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:04:49.672+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:04:49.936+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.523 seconds
[2022-12-17T06:05:00.364+0000] {processor.py:154} INFO - Started process (PID=4440) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:00.419+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:05:00.424+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:00.423+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:00.687+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:01.197+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:01.196+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:05:01.376+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:01.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:05:01.520+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.172 seconds
[2022-12-17T06:05:11.817+0000] {processor.py:154} INFO - Started process (PID=4450) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:11.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:05:11.846+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:11.845+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:11.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:12.228+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:12.227+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:05:12.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:12.481+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:05:12.716+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.913 seconds
[2022-12-17T06:05:23.064+0000] {processor.py:154} INFO - Started process (PID=4466) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:23.068+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:05:23.072+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:23.071+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:23.223+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:24.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:24.123+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:05:24.369+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:24.369+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:05:24.486+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.438 seconds
[2022-12-17T06:05:34.896+0000] {processor.py:154} INFO - Started process (PID=4477) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:34.920+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:05:34.924+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:34.923+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:35.015+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:35.153+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:35.152+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:05:35.263+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:35.262+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:05:35.401+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.521 seconds
[2022-12-17T06:05:45.637+0000] {processor.py:154} INFO - Started process (PID=4487) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:45.669+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:05:45.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:45.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:45.799+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:45.954+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:45.954+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:05:46.086+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:46.085+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:05:46.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.616 seconds
[2022-12-17T06:05:56.502+0000] {processor.py:154} INFO - Started process (PID=4497) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:56.551+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:05:56.556+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:56.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:56.646+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:05:56.776+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:56.775+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:05:56.896+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:05:56.895+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:05:57.029+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.540 seconds
[2022-12-17T06:06:07.403+0000] {processor.py:154} INFO - Started process (PID=4515) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:07.428+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:06:07.440+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:07.439+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:07.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:07.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:07.760+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:06:07.962+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:07.962+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:06:08.143+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.758 seconds
[2022-12-17T06:06:18.549+0000] {processor.py:154} INFO - Started process (PID=4525) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:18.591+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:06:18.594+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:18.594+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:18.687+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:19.966+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:19.965+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:06:20.108+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:20.107+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:06:20.235+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.700 seconds
[2022-12-17T06:06:30.516+0000] {processor.py:154} INFO - Started process (PID=4535) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:30.520+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:06:30.524+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:30.523+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:30.642+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:30.788+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:30.787+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:06:30.907+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:30.906+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:06:31.037+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.536 seconds
[2022-12-17T06:06:41.361+0000] {processor.py:154} INFO - Started process (PID=4545) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:41.373+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:06:41.378+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:41.377+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:41.459+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:41.597+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:41.596+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:06:41.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:41.710+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:06:41.860+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.516 seconds
[2022-12-17T06:06:52.390+0000] {processor.py:154} INFO - Started process (PID=4564) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:52.443+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:06:52.447+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:52.446+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:52.661+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:06:52.830+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:52.829+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:06:52.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:06:52.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:06:53.128+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.753 seconds
[2022-12-17T06:07:03.353+0000] {processor.py:154} INFO - Started process (PID=4574) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:03.380+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:07:03.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:03.383+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:03.478+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:03.646+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:03.645+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:07:03.763+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:03.762+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:07:03.888+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.549 seconds
[2022-12-17T06:07:14.171+0000] {processor.py:154} INFO - Started process (PID=4584) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:14.174+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:07:14.179+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:14.178+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:14.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:14.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:14.400+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:07:14.523+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:14.522+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:07:14.673+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.518 seconds
[2022-12-17T06:07:25.182+0000] {processor.py:154} INFO - Started process (PID=4594) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:25.207+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:07:25.221+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:25.210+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:25.618+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:25.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:25.833+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:07:25.985+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:25.984+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:07:26.135+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.021 seconds
[2022-12-17T06:07:36.530+0000] {processor.py:154} INFO - Started process (PID=4612) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:36.576+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:07:36.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:36.579+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:36.697+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:37.074+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:37.073+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:07:37.455+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:37.454+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:07:37.629+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.113 seconds
[2022-12-17T06:07:48.050+0000] {processor.py:154} INFO - Started process (PID=4622) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:48.054+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:07:48.058+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:48.057+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:48.160+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:48.746+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:07:59.074+0000] {processor.py:154} INFO - Started process (PID=4632) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:59.077+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:07:59.081+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:59.080+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:59.175+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:07:59.790+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:59.789+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:07:59.921+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:07:59.920+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:08:00.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.179 seconds
[2022-12-17T06:08:10.671+0000] {processor.py:154} INFO - Started process (PID=4649) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:10.684+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:08:10.691+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:10.686+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:10.840+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:11.227+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:11.226+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:08:11.502+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:11.501+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:08:11.841+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.187 seconds
[2022-12-17T06:08:22.612+0000] {processor.py:154} INFO - Started process (PID=4659) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:22.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:08:22.632+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:22.631+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:22.722+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:22.896+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:22.895+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:08:23.247+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:23.246+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:08:23.465+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.873 seconds
[2022-12-17T06:08:33.976+0000] {processor.py:154} INFO - Started process (PID=4669) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:33.993+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:08:34.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:34.013+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:34.112+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:34.282+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:34.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:08:34.410+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:34.409+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:08:34.539+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.578 seconds
[2022-12-17T06:08:44.820+0000] {processor.py:154} INFO - Started process (PID=4679) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:44.867+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:08:44.872+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:44.870+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:45.065+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:45.223+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:45.222+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:08:45.337+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:45.336+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:08:45.469+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.663 seconds
[2022-12-17T06:08:55.813+0000] {processor.py:154} INFO - Started process (PID=4696) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:55.859+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:08:55.862+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:55.861+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:56.078+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:08:56.336+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:56.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:08:56.540+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:08:56.539+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:08:56.761+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.997 seconds
[2022-12-17T06:09:07.082+0000] {processor.py:154} INFO - Started process (PID=4706) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:07.086+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:09:07.090+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:07.089+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:07.179+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:08.335+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:08.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:09:08.604+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:08.603+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:09:08.756+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.688 seconds
[2022-12-17T06:09:19.230+0000] {processor.py:154} INFO - Started process (PID=4716) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:19.416+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:09:19.420+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:19.419+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:19.524+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:19.957+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:19.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:09:20.094+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:20.093+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:09:20.259+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.043 seconds
[2022-12-17T06:09:30.672+0000] {processor.py:154} INFO - Started process (PID=4733) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:30.681+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:09:30.689+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:30.688+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:30.824+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:31.421+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:31.420+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:09:31.625+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:31.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:09:31.921+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.290 seconds
[2022-12-17T06:09:42.368+0000] {processor.py:154} INFO - Started process (PID=4743) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:42.397+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:09:42.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:42.400+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:42.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:42.706+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:42.705+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:09:42.891+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:42.890+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:09:42.998+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.645 seconds
[2022-12-17T06:09:53.360+0000] {processor.py:154} INFO - Started process (PID=4753) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:53.389+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:09:53.393+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:53.392+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:53.484+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:09:53.803+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:53.802+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:09:53.936+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:09:53.935+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:09:54.077+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.738 seconds
[2022-12-17T06:10:04.369+0000] {processor.py:154} INFO - Started process (PID=4763) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:04.381+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:10:04.388+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:04.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:04.502+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:04.674+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:04.673+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:10:04.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:04.793+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:10:04.942+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.592 seconds
[2022-12-17T06:10:15.447+0000] {processor.py:154} INFO - Started process (PID=4780) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:15.452+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:10:15.465+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:15.464+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:15.672+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:17.132+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:17.131+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:10:17.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:17.425+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:10:17.641+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.224 seconds
[2022-12-17T06:10:27.993+0000] {processor.py:154} INFO - Started process (PID=4790) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:28.019+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:10:28.023+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:28.022+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:28.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:28.906+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:10:39.247+0000] {processor.py:154} INFO - Started process (PID=4800) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:39.273+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:10:39.283+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:39.282+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:39.374+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:39.619+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:39.618+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:10:39.778+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:39.777+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:10:39.911+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.679 seconds
[2022-12-17T06:10:50.438+0000] {processor.py:154} INFO - Started process (PID=4817) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:50.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:10:50.487+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:50.486+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:50.796+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:10:51.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:51.136+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:10:51.339+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:10:51.338+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:10:51.627+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.235 seconds
[2022-12-17T06:11:02.041+0000] {processor.py:154} INFO - Started process (PID=4828) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:02.059+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:11:02.065+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:02.063+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:02.167+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:03.445+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:11:13.765+0000] {processor.py:154} INFO - Started process (PID=4838) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:13.790+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:11:13.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:13.793+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:13.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:14.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:14.061+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:11:14.182+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:14.181+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:11:14.294+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.543 seconds
[2022-12-17T06:11:24.720+0000] {processor.py:154} INFO - Started process (PID=4848) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:24.749+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:11:24.752+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:24.752+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:24.853+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:25.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:25.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:11:25.478+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:25.477+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:11:25.670+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.989 seconds
[2022-12-17T06:11:35.909+0000] {processor.py:154} INFO - Started process (PID=4866) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:35.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:11:36.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:35.999+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:36.164+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:36.403+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:36.402+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:11:36.590+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:36.589+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:11:36.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.821 seconds
[2022-12-17T06:11:47.022+0000] {processor.py:154} INFO - Started process (PID=4876) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:47.044+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:11:47.048+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:47.047+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:47.134+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:47.746+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:47.745+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:11:47.940+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:47.939+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:11:48.046+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.039 seconds
[2022-12-17T06:11:58.361+0000] {processor.py:154} INFO - Started process (PID=4886) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:58.364+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:11:58.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:58.367+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:58.457+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:11:58.633+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:58.632+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:11:58.758+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:11:58.757+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:11:58.940+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.593 seconds
[2022-12-17T06:12:09.381+0000] {processor.py:154} INFO - Started process (PID=4902) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:09.409+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:12:09.417+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:09.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:09.679+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:10.286+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:10.285+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:12:10.410+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:10.409+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:12:10.545+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.215 seconds
[2022-12-17T06:12:21.130+0000] {processor.py:154} INFO - Started process (PID=4914) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:21.150+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:12:21.154+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:21.153+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:21.396+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:21.941+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:21.940+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:12:22.059+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:22.058+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:12:22.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.095 seconds
[2022-12-17T06:12:32.450+0000] {processor.py:154} INFO - Started process (PID=4924) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:32.475+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:12:32.480+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:32.478+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:32.575+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:32.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:32.792+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:12:32.909+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:32.908+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:12:33.024+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.588 seconds
[2022-12-17T06:12:43.385+0000] {processor.py:154} INFO - Started process (PID=4934) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:43.415+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:12:43.423+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:43.422+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:43.726+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:43.882+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:43.881+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:12:43.991+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:43.990+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:12:44.103+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.733 seconds
[2022-12-17T06:12:55.050+0000] {processor.py:154} INFO - Started process (PID=4952) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:55.096+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:12:55.122+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:55.099+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:55.324+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:12:55.739+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:55.738+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:12:55.926+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:12:55.925+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:12:56.090+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.057 seconds
[2022-12-17T06:13:06.482+0000] {processor.py:154} INFO - Started process (PID=4962) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:06.508+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:13:06.512+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:06.511+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:06.606+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:06.801+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:06.799+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:13:07.054+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:07.053+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:13:07.190+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.721 seconds
[2022-12-17T06:13:17.350+0000] {processor.py:154} INFO - Started process (PID=4972) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:17.364+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:13:17.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:17.367+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:17.466+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:17.699+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:17.698+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:13:17.828+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:17.827+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:13:18.012+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.676 seconds
[2022-12-17T06:13:28.678+0000] {processor.py:154} INFO - Started process (PID=4982) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:28.700+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:13:28.705+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:28.704+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:28.789+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:29.509+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:29.508+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:13:29.654+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:29.653+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:13:29.767+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.103 seconds
[2022-12-17T06:13:40.073+0000] {processor.py:154} INFO - Started process (PID=5000) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:40.161+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:13:40.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:40.165+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:40.385+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:40.596+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:40.595+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:13:40.819+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:40.818+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:13:40.956+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.942 seconds
[2022-12-17T06:13:51.133+0000] {processor.py:154} INFO - Started process (PID=5010) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:51.160+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:13:51.164+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:51.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:51.256+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:13:51.443+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:51.442+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:13:51.571+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:13:51.570+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:13:51.753+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.633 seconds
[2022-12-17T06:14:02.080+0000] {processor.py:154} INFO - Started process (PID=5020) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:02.134+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:14:02.138+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:02.137+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:02.244+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:02.399+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:02.398+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:14:02.520+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:02.519+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:14:02.667+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.613 seconds
[2022-12-17T06:14:13.022+0000] {processor.py:154} INFO - Started process (PID=5037) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:13.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:14:13.046+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:13.039+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:13.259+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:13.475+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:13.474+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:14:13.657+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:13.656+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:14:13.866+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.860 seconds
[2022-12-17T06:14:24.104+0000] {processor.py:154} INFO - Started process (PID=5048) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:24.115+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:14:24.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:24.119+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:24.210+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:24.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:24.400+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:14:24.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:24.545+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:14:24.676+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.590 seconds
[2022-12-17T06:14:34.941+0000] {processor.py:154} INFO - Started process (PID=5058) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:34.998+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:14:35.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:35.001+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:35.104+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:36.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:36.658+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:14:36.798+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:36.797+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:14:37.237+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.310 seconds
[2022-12-17T06:14:47.485+0000] {processor.py:154} INFO - Started process (PID=5068) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:47.489+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:14:47.493+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:47.492+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:47.587+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:48.021+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:48.020+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:14:48.151+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:48.150+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:14:48.263+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.795 seconds
[2022-12-17T06:14:58.687+0000] {processor.py:154} INFO - Started process (PID=5086) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:58.709+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:14:58.714+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:58.713+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:58.918+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:14:59.134+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:59.133+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:14:59.312+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:14:59.312+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:14:59.517+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.853 seconds
[2022-12-17T06:15:10.080+0000] {processor.py:154} INFO - Started process (PID=5096) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:10.111+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:15:10.116+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:10.115+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:10.203+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:11.223+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:15:21.621+0000] {processor.py:154} INFO - Started process (PID=5106) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:21.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:15:21.672+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:21.671+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:21.763+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:22.194+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:22.192+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:15:22.313+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:22.312+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:15:22.458+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.853 seconds
[2022-12-17T06:15:32.746+0000] {processor.py:154} INFO - Started process (PID=5123) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:32.782+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:15:32.786+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:32.785+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:32.984+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:33.752+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:15:44.354+0000] {processor.py:154} INFO - Started process (PID=5134) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:44.383+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:15:44.394+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:44.386+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:44.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:44.692+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:44.691+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:15:44.817+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:44.816+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:15:45.137+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.799 seconds
[2022-12-17T06:15:55.494+0000] {processor.py:154} INFO - Started process (PID=5144) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:55.520+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:15:55.530+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:15:55.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:55.677+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:15:56.379+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:16:06.783+0000] {processor.py:154} INFO - Started process (PID=5154) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:06.811+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:16:06.815+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:06.814+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:07.027+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:07.495+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:07.494+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:16:07.626+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:07.625+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:16:07.758+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.003 seconds
[2022-12-17T06:16:18.279+0000] {processor.py:154} INFO - Started process (PID=5172) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:18.301+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:16:18.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:18.321+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:18.657+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:19.511+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:19.509+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:16:19.828+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:19.827+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:16:20.025+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.809 seconds
[2022-12-17T06:16:30.390+0000] {processor.py:154} INFO - Started process (PID=5182) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:30.424+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:16:30.428+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:30.427+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:30.540+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:31.407+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:16:41.765+0000] {processor.py:154} INFO - Started process (PID=5192) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:41.792+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:16:41.796+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:41.795+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:41.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:43.174+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:16:53.651+0000] {processor.py:154} INFO - Started process (PID=5208) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:53.673+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:16:53.681+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:16:53.680+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:53.943+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:16:54.913+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:17:05.271+0000] {processor.py:154} INFO - Started process (PID=5219) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:05.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:17:05.320+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:05.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:05.467+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:05.611+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:05.610+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:17:05.727+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:05.726+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:17:05.880+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.637 seconds
[2022-12-17T06:17:16.450+0000] {processor.py:154} INFO - Started process (PID=5229) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:16.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:17:16.471+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:16.470+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:16.585+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:17.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:17.904+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:17:18.024+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:18.023+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:17:18.141+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.711 seconds
[2022-12-17T06:17:28.396+0000] {processor.py:154} INFO - Started process (PID=5239) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:28.419+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:17:28.424+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:28.422+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:28.506+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:28.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:28.921+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:17:29.051+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:29.050+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:17:29.166+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.784 seconds
[2022-12-17T06:17:39.714+0000] {processor.py:154} INFO - Started process (PID=5257) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:39.742+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:17:39.764+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:39.749+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:39.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:40.144+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:40.143+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:17:40.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:40.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:17:40.527+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.864 seconds
[2022-12-17T06:17:51.033+0000] {processor.py:154} INFO - Started process (PID=5267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:51.060+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:17:51.065+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:51.064+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:51.159+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:17:51.702+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:51.701+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:17:51.813+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:17:51.812+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:17:51.950+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.941 seconds
[2022-12-17T06:18:02.301+0000] {processor.py:154} INFO - Started process (PID=5277) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:02.337+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:18:02.342+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:02.341+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:02.435+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:02.914+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:02.913+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:18:03.025+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:03.025+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:18:03.142+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.878 seconds
[2022-12-17T06:18:13.598+0000] {processor.py:154} INFO - Started process (PID=5287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:13.617+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:18:13.621+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:13.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:13.741+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:13.943+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:13.942+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:18:14.093+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:14.092+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:18:14.296+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.717 seconds
[2022-12-17T06:18:24.421+0000] {processor.py:154} INFO - Started process (PID=5305) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:24.531+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:18:24.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:24.535+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:24.763+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:25.283+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:25.282+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:18:25.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:25.522+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:18:25.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.339 seconds
[2022-12-17T06:18:36.048+0000] {processor.py:154} INFO - Started process (PID=5315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:36.055+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:18:36.060+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:36.059+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:36.149+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:36.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:36.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:18:36.534+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:36.533+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:18:36.885+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.862 seconds
[2022-12-17T06:18:47.227+0000] {processor.py:154} INFO - Started process (PID=5325) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:47.231+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:18:47.235+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:47.234+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:47.326+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:47.469+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:47.468+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:18:47.621+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:47.620+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:18:47.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.528 seconds
[2022-12-17T06:18:58.159+0000] {processor.py:154} INFO - Started process (PID=5343) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:58.206+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:18:58.210+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:58.209+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:58.347+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:18:58.737+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:58.735+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:18:59.144+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:18:59.134+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:18:59.802+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.659 seconds
[2022-12-17T06:19:10.252+0000] {processor.py:154} INFO - Started process (PID=5353) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:10.305+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:19:10.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:10.309+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:10.421+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:10.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:10.588+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:19:10.707+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:10.706+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:19:10.819+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.587 seconds
[2022-12-17T06:19:21.010+0000] {processor.py:154} INFO - Started process (PID=5363) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:21.032+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:19:21.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:21.037+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:21.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:21.324+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:21.323+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:19:21.522+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:21.521+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:19:21.668+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.673 seconds
[2022-12-17T06:19:31.961+0000] {processor.py:154} INFO - Started process (PID=5373) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:31.965+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:19:31.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:31.968+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:32.056+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:32.251+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:32.250+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:19:32.421+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:32.420+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:19:32.552+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.606 seconds
[2022-12-17T06:19:43.021+0000] {processor.py:154} INFO - Started process (PID=5391) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:43.049+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:19:43.053+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:43.052+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:43.284+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:43.544+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:43.542+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:19:43.763+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:43.762+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:19:43.985+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.979 seconds
[2022-12-17T06:19:54.426+0000] {processor.py:154} INFO - Started process (PID=5401) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:54.430+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:19:54.434+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:54.433+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:54.521+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:19:54.684+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:54.683+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:19:54.798+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:19:54.797+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:19:54.967+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.555 seconds
[2022-12-17T06:20:05.633+0000] {processor.py:154} INFO - Started process (PID=5411) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:05.661+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:20:05.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:05.664+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:05.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:06.348+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:20:16.711+0000] {processor.py:154} INFO - Started process (PID=5427) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:16.715+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:20:16.727+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:16.722+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:16.907+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:17.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:17.967+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:20:18.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:18.222+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:20:18.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.740 seconds
[2022-12-17T06:20:28.887+0000] {processor.py:154} INFO - Started process (PID=5440) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:28.932+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:20:28.936+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:28.935+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:29.025+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:29.669+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:29.668+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:20:29.831+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:29.830+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:20:29.976+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.104 seconds
[2022-12-17T06:20:40.282+0000] {processor.py:154} INFO - Started process (PID=5450) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:40.332+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:20:40.336+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:40.335+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:40.470+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:40.630+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:40.629+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:20:40.774+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:40.773+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:20:40.943+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.685 seconds
[2022-12-17T06:20:51.079+0000] {processor.py:154} INFO - Started process (PID=5460) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:51.082+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:20:51.086+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:51.085+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:51.175+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:20:51.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:51.453+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:20:51.633+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:20:51.633+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:20:51.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.669 seconds
[2022-12-17T06:21:02.421+0000] {processor.py:154} INFO - Started process (PID=5478) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:02.456+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:21:02.487+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:02.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:02.646+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:03.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:03.465+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:21:03.667+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:03.666+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:21:03.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.533 seconds
[2022-12-17T06:21:14.154+0000] {processor.py:154} INFO - Started process (PID=5488) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:14.206+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:21:14.210+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:14.209+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:14.303+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:14.480+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:14.479+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:21:14.614+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:14.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:21:14.748+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.609 seconds
[2022-12-17T06:21:25.130+0000] {processor.py:154} INFO - Started process (PID=5498) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:25.177+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:21:25.180+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:25.180+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:25.286+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:25.446+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:25.445+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:21:25.567+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:25.566+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:21:25.717+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.601 seconds
[2022-12-17T06:21:36.192+0000] {processor.py:154} INFO - Started process (PID=5511) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:36.226+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:21:36.234+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:36.233+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:36.540+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:36.890+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:36.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:21:37.167+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:37.166+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:21:37.313+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.172 seconds
[2022-12-17T06:21:47.716+0000] {processor.py:154} INFO - Started process (PID=5527) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:47.748+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:21:47.752+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:47.751+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:47.859+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:48.010+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:48.009+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:21:48.123+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:48.122+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:21:48.257+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.555 seconds
[2022-12-17T06:21:58.898+0000] {processor.py:154} INFO - Started process (PID=5537) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:58.921+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:21:58.925+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:58.924+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:59.015+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:21:59.335+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:59.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:21:59.591+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:21:59.586+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:21:59.836+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.955 seconds
[2022-12-17T06:22:10.349+0000] {processor.py:154} INFO - Started process (PID=5547) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:10.375+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:22:10.383+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:10.382+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:10.670+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:10.995+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:10.994+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:22:11.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:11.120+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:22:11.260+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.986 seconds
[2022-12-17T06:22:21.631+0000] {processor.py:154} INFO - Started process (PID=5565) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:21.676+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:22:21.692+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:21.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:22.004+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:22.280+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:22.279+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:22:22.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:22.478+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:22:22.809+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.256 seconds
[2022-12-17T06:22:33.348+0000] {processor.py:154} INFO - Started process (PID=5575) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:33.351+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:22:33.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:33.356+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:33.551+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:34.129+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:22:44.610+0000] {processor.py:154} INFO - Started process (PID=5585) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:44.614+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:22:44.618+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:44.617+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:44.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:45.593+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:22:55.873+0000] {processor.py:154} INFO - Started process (PID=5595) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:56.016+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:22:56.020+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:56.019+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:56.109+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:22:56.287+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:56.286+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:22:56.570+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:22:56.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:22:57.064+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.222 seconds
[2022-12-17T06:23:07.265+0000] {processor.py:154} INFO - Started process (PID=5614) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:07.288+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:23:07.292+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:07.291+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:07.395+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:07.558+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:07.557+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:23:07.720+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:07.720+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:23:07.886+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.635 seconds
[2022-12-17T06:23:18.170+0000] {processor.py:154} INFO - Started process (PID=5624) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:18.222+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:23:18.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:18.225+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:18.359+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:19.737+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:23:30.144+0000] {processor.py:154} INFO - Started process (PID=5634) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:30.148+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:23:30.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:30.151+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:30.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:30.395+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:30.394+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:23:30.513+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:30.512+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:23:30.780+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.651 seconds
[2022-12-17T06:23:41.279+0000] {processor.py:154} INFO - Started process (PID=5653) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:41.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:23:41.307+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:41.306+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:41.442+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:41.667+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:41.661+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:23:41.949+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:41.948+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:23:42.125+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.876 seconds
[2022-12-17T06:23:52.561+0000] {processor.py:154} INFO - Started process (PID=5663) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:52.605+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:23:52.611+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:52.609+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:52.721+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:23:52.886+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:52.885+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:23:53.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:23:53.001+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:23:53.400+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.865 seconds
[2022-12-17T06:24:03.603+0000] {processor.py:154} INFO - Started process (PID=5673) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:03.632+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:24:03.637+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:03.636+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:03.724+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:03.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:03.877+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:24:03.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:03.993+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:24:04.132+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.545 seconds
[2022-12-17T06:24:14.439+0000] {processor.py:154} INFO - Started process (PID=5683) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:14.443+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:24:14.450+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:14.449+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:14.536+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:15.335+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:24:25.879+0000] {processor.py:154} INFO - Started process (PID=5701) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:25.937+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:24:25.942+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:25.941+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:26.222+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:26.958+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:26.957+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:24:27.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:27.165+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:24:27.385+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.530 seconds
[2022-12-17T06:24:37.790+0000] {processor.py:154} INFO - Started process (PID=5711) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:37.801+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:24:37.806+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:37.804+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:37.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:38.562+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:38.561+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:24:38.733+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:38.732+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:24:38.883+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.107 seconds
[2022-12-17T06:24:49.036+0000] {processor.py:154} INFO - Started process (PID=5721) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:49.040+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:24:49.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:49.044+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:49.137+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:24:49.286+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:49.285+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:24:49.398+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:24:49.397+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:24:49.553+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.535 seconds
[2022-12-17T06:25:00.053+0000] {processor.py:154} INFO - Started process (PID=5736) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:00.083+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:25:00.088+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:00.087+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:00.266+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:01.551+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:25:12.128+0000] {processor.py:154} INFO - Started process (PID=5749) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:12.154+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:25:12.158+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:12.157+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:12.247+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:12.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:12.482+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:25:12.614+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:12.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:25:12.730+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.616 seconds
[2022-12-17T06:25:23.094+0000] {processor.py:154} INFO - Started process (PID=5759) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:23.122+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:25:23.130+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:23.128+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:23.232+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:23.378+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:23.377+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:25:23.493+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:23.492+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:25:23.608+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.556 seconds
[2022-12-17T06:25:33.878+0000] {processor.py:154} INFO - Started process (PID=5769) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:33.904+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:25:33.913+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:33.912+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:34.117+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:34.720+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:34.719+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:25:34.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:34.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:25:34.957+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.097 seconds
[2022-12-17T06:25:45.324+0000] {processor.py:154} INFO - Started process (PID=5788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:45.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:25:45.385+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:45.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:45.527+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:45.949+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:45.948+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:25:46.276+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:46.275+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:25:46.753+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.460 seconds
[2022-12-17T06:25:57.116+0000] {processor.py:154} INFO - Started process (PID=5798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:57.140+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:25:57.151+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:57.142+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:57.239+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:25:57.913+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:57.912+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:25:58.230+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:25:58.229+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:25:58.521+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.420 seconds
[2022-12-17T06:26:09.488+0000] {processor.py:154} INFO - Started process (PID=5808) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:09.507+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:26:09.510+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:09.509+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:09.640+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:10.214+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:10.213+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:26:10.330+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:10.329+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:26:10.500+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.048 seconds
[2022-12-17T06:26:21.063+0000] {processor.py:154} INFO - Started process (PID=5825) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:21.130+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:26:21.162+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:21.161+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:21.449+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:23.425+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:23.424+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:26:23.683+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:23.682+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:26:24.122+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.106 seconds
[2022-12-17T06:26:34.521+0000] {processor.py:154} INFO - Started process (PID=5836) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:34.602+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:26:34.606+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:34.605+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:34.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:35.950+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:26:46.776+0000] {processor.py:154} INFO - Started process (PID=5846) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:46.805+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:26:46.812+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:46.812+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:47.064+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:47.358+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:47.357+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:26:47.533+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:47.532+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:26:47.653+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.891 seconds
[2022-12-17T06:26:57.925+0000] {processor.py:154} INFO - Started process (PID=5856) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:57.949+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:26:57.953+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:57.952+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:58.054+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:26:58.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:58.242+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:26:58.389+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:26:58.388+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:26:58.492+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.585 seconds
[2022-12-17T06:27:08.734+0000] {processor.py:154} INFO - Started process (PID=5874) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:08.775+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:27:08.792+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:08.778+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:08.936+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:10.385+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:27:20.595+0000] {processor.py:154} INFO - Started process (PID=5884) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:20.609+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:27:20.613+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:20.612+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:20.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:20.937+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:20.936+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:27:21.058+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:21.058+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:27:21.298+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.720 seconds
[2022-12-17T06:27:32.153+0000] {processor.py:154} INFO - Started process (PID=5894) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:32.166+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:27:32.176+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:32.175+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:32.273+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:33.421+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:27:43.773+0000] {processor.py:154} INFO - Started process (PID=5910) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:43.826+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:27:43.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:43.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:43.960+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:44.760+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:44.759+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:27:44.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:44.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:27:45.038+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.280 seconds
[2022-12-17T06:27:55.432+0000] {processor.py:154} INFO - Started process (PID=5921) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:55.480+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:27:55.485+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:55.484+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:55.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:27:56.149+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:56.148+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:27:56.257+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:27:56.256+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:27:56.423+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.005 seconds
[2022-12-17T06:28:06.744+0000] {processor.py:154} INFO - Started process (PID=5931) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:06.771+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:28:06.776+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:06.775+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:06.874+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:07.447+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:07.446+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:28:07.556+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:07.555+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:28:07.702+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.971 seconds
[2022-12-17T06:28:17.929+0000] {processor.py:154} INFO - Started process (PID=5941) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:17.951+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:28:17.955+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:17.954+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:18.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:18.639+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:18.638+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:28:18.760+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:18.759+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:28:18.953+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.041 seconds
[2022-12-17T06:28:29.537+0000] {processor.py:154} INFO - Started process (PID=5958) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:29.564+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:28:29.569+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:29.567+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:29.730+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:29.940+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:29.939+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:28:30.061+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:30.060+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:28:30.260+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.771 seconds
[2022-12-17T06:28:40.607+0000] {processor.py:154} INFO - Started process (PID=5968) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:40.633+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:28:40.639+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:40.638+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:40.725+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:40.871+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:40.870+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:28:41.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:41.014+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:28:41.125+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.532 seconds
[2022-12-17T06:28:51.410+0000] {processor.py:154} INFO - Started process (PID=5978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:51.460+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:28:51.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:51.464+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:51.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:28:52.364+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:52.362+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:28:52.481+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:28:52.480+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:28:52.595+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.202 seconds
[2022-12-17T06:29:02.874+0000] {processor.py:154} INFO - Started process (PID=5988) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:02.906+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:29:02.912+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:02.910+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:03.104+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:03.953+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:03.952+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:29:04.139+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:04.138+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:29:04.252+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.392 seconds
[2022-12-17T06:29:14.421+0000] {processor.py:154} INFO - Started process (PID=6006) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:14.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:29:14.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:14.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:14.562+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:14.741+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:14.740+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:29:14.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:14.873+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:29:15.057+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.650 seconds
[2022-12-17T06:29:25.517+0000] {processor.py:154} INFO - Started process (PID=6016) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:25.566+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:29:25.575+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:25.574+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:25.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:26.369+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:26.369+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:29:26.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:26.659+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:29:26.862+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.358 seconds
[2022-12-17T06:29:37.382+0000] {processor.py:154} INFO - Started process (PID=6026) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:37.433+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:29:37.439+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:37.438+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:37.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:37.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:37.659+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:29:37.774+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:37.773+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:29:37.902+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.564 seconds
[2022-12-17T06:29:48.387+0000] {processor.py:154} INFO - Started process (PID=6043) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:48.414+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:29:48.418+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:48.417+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:48.657+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:49.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:49.263+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:29:49.403+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:49.402+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:29:49.614+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.258 seconds
[2022-12-17T06:29:59.913+0000] {processor.py:154} INFO - Started process (PID=6053) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:29:59.927+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:29:59.932+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:29:59.931+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:00.022+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:00.475+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:00.474+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:30:00.603+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:00.602+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:30:00.714+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.814 seconds
[2022-12-17T06:30:11.199+0000] {processor.py:154} INFO - Started process (PID=6063) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:11.203+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:30:11.207+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:11.206+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:11.297+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:11.429+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:11.428+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:30:11.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:11.545+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:30:11.686+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.502 seconds
[2022-12-17T06:30:22.011+0000] {processor.py:154} INFO - Started process (PID=6073) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:22.015+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:30:22.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:22.018+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:22.106+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:22.242+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:22.241+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:30:22.376+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:22.375+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:30:22.546+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.548 seconds
[2022-12-17T06:30:33.032+0000] {processor.py:154} INFO - Started process (PID=6090) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:33.040+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:30:33.056+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:33.043+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:33.206+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:33.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:33.597+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:30:33.737+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:33.736+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:30:33.960+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.975 seconds
[2022-12-17T06:30:44.208+0000] {processor.py:154} INFO - Started process (PID=6100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:44.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:30:44.216+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:44.215+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:44.302+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:44.433+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:44.432+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:30:44.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:44.548+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:30:44.687+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.495 seconds
[2022-12-17T06:30:54.964+0000] {processor.py:154} INFO - Started process (PID=6110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:55.028+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:30:55.036+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:55.035+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:55.131+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:30:55.278+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:55.277+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:30:55.389+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:30:55.388+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:30:55.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.594 seconds
[2022-12-17T06:31:06.493+0000] {processor.py:154} INFO - Started process (PID=6121) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:06.574+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:31:06.602+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:06.601+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:06.725+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:07.346+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:07.345+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:31:07.564+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:07.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:31:07.793+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.340 seconds
[2022-12-17T06:31:18.156+0000] {processor.py:154} INFO - Started process (PID=6138) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:18.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:31:18.214+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:18.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:18.305+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:18.687+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:31:29.030+0000] {processor.py:154} INFO - Started process (PID=6148) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:29.057+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:31:29.061+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:29.060+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:29.163+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:29.523+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:29.522+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:31:29.637+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:29.637+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:31:29.750+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.734 seconds
[2022-12-17T06:31:40.192+0000] {processor.py:154} INFO - Started process (PID=6158) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:40.227+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:31:40.231+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:40.230+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:40.464+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:40.720+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:40.719+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:31:40.866+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:40.866+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:31:40.986+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.845 seconds
[2022-12-17T06:31:51.191+0000] {processor.py:154} INFO - Started process (PID=6176) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:51.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:31:51.212+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:51.200+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:51.599+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:31:52.130+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:52.129+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:31:52.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:31:52.442+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:31:52.773+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.609 seconds
[2022-12-17T06:32:03.180+0000] {processor.py:154} INFO - Started process (PID=6186) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:03.252+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:32:03.256+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:03.255+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:03.346+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:03.493+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:03.492+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:32:03.735+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:03.734+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:32:04.041+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-17T06:32:14.642+0000] {processor.py:154} INFO - Started process (PID=6196) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:14.688+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:32:14.693+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:14.692+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:14.786+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:14.941+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:14.940+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:32:15.057+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:15.056+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:32:15.210+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.583 seconds
[2022-12-17T06:32:25.668+0000] {processor.py:154} INFO - Started process (PID=6206) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:25.713+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:32:25.717+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:25.716+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:25.899+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:26.627+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:32:37.282+0000] {processor.py:154} INFO - Started process (PID=6224) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:37.292+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:32:37.300+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:37.299+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:37.434+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:37.605+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:37.604+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:32:37.758+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:37.757+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:32:38.003+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.732 seconds
[2022-12-17T06:32:48.707+0000] {processor.py:154} INFO - Started process (PID=6234) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:48.827+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:32:48.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:48.833+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:48.943+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:49.139+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:49.130+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:32:49.285+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:49.284+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:32:49.397+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.721 seconds
[2022-12-17T06:32:59.754+0000] {processor.py:154} INFO - Started process (PID=6244) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:59.786+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:32:59.791+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:32:59.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:32:59.891+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:00.856+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:33:11.430+0000] {processor.py:154} INFO - Started process (PID=6262) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:11.498+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:33:11.513+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:11.501+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:11.731+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:12.099+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:12.098+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:33:12.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:12.242+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:33:12.604+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.194 seconds
[2022-12-17T06:33:23.114+0000] {processor.py:154} INFO - Started process (PID=6272) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:23.118+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:33:23.126+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:23.125+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:23.213+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:23.375+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:23.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:33:23.495+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:23.494+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:33:23.628+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.528 seconds
[2022-12-17T06:33:33.747+0000] {processor.py:154} INFO - Started process (PID=6282) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:33.751+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:33:33.755+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:33.754+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:33.849+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:33.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:33.993+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:33:34.114+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:34.113+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:33:34.271+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.538 seconds
[2022-12-17T06:33:44.587+0000] {processor.py:154} INFO - Started process (PID=6292) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:44.630+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:33:44.634+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:44.633+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:44.723+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:44.880+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:44.878+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:33:45.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:45.013+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:33:45.200+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-17T06:33:55.678+0000] {processor.py:154} INFO - Started process (PID=6311) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:55.746+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:33:55.751+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:55.750+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:55.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:33:56.725+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:56.724+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:33:56.954+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:33:56.953+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:33:57.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.554 seconds
[2022-12-17T06:34:07.505+0000] {processor.py:154} INFO - Started process (PID=6321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:07.534+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:34:07.538+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:07.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:07.682+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:07.851+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:07.850+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:34:07.982+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:07.981+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:34:08.099+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.610 seconds
[2022-12-17T06:34:18.506+0000] {processor.py:154} INFO - Started process (PID=6331) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:18.516+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:34:18.525+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:18.519+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:18.780+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:19.349+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:34:29.648+0000] {processor.py:154} INFO - Started process (PID=6348) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:29.658+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:34:29.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:29.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:29.827+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:31.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:31.465+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:34:31.763+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:31.762+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:34:31.961+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.335 seconds
[2022-12-17T06:34:42.554+0000] {processor.py:154} INFO - Started process (PID=6359) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:42.570+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:34:42.579+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:42.578+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:42.679+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:43.279+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:43.278+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:34:43.404+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:43.399+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:34:43.608+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.070 seconds
[2022-12-17T06:34:53.922+0000] {processor.py:154} INFO - Started process (PID=6369) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:53.925+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:34:53.934+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:53.933+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:54.029+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:34:54.271+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:54.270+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:34:54.392+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:34:54.391+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:34:54.497+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.600 seconds
[2022-12-17T06:35:04.826+0000] {processor.py:154} INFO - Started process (PID=6379) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:04.834+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:35:04.842+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:04.841+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:04.973+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:05.829+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:35:16.101+0000] {processor.py:154} INFO - Started process (PID=6397) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:16.129+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:35:16.133+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:16.132+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:16.329+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:16.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:16.658+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:35:17.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:17.136+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:35:17.520+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.435 seconds
[2022-12-17T06:35:28.104+0000] {processor.py:154} INFO - Started process (PID=6407) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:28.258+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:35:28.276+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:28.275+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:28.702+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:29.048+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:29.046+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:35:29.260+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:29.259+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:35:29.384+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.320 seconds
[2022-12-17T06:35:40.367+0000] {processor.py:154} INFO - Started process (PID=6417) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:40.399+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:35:40.408+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:40.402+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:40.500+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:40.728+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:40.727+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:35:40.866+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:40.865+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:35:40.972+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-17T06:35:51.397+0000] {processor.py:154} INFO - Started process (PID=6427) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:51.425+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:35:51.429+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:35:51.428+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:51.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:35:53.062+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:36:03.673+0000] {processor.py:154} INFO - Started process (PID=6445) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:03.852+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:36:03.856+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:03.855+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:03.962+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:04.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:04.119+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:36:04.236+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:04.234+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:36:04.420+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.763 seconds
[2022-12-17T06:36:15.298+0000] {processor.py:154} INFO - Started process (PID=6455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:15.320+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:36:15.332+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:15.327+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:15.534+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:15.860+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:15.859+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:36:15.995+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:15.994+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:36:16.095+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.811 seconds
[2022-12-17T06:36:26.361+0000] {processor.py:154} INFO - Started process (PID=6465) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:26.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:36:26.432+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:26.431+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:26.624+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:26.862+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:26.862+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:36:27.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:27.003+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:36:27.197+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.850 seconds
[2022-12-17T06:36:37.658+0000] {processor.py:154} INFO - Started process (PID=6483) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:37.715+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:36:37.724+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:37.718+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:37.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:38.389+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:38.388+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:36:38.782+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:38.781+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:36:39.253+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.624 seconds
[2022-12-17T06:36:49.447+0000] {processor.py:154} INFO - Started process (PID=6493) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:49.493+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:36:49.498+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:49.497+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:49.590+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:36:49.931+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:49.930+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:36:50.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:36:50.061+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:36:50.326+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.894 seconds
[2022-12-17T06:37:00.749+0000] {processor.py:154} INFO - Started process (PID=6503) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:00.762+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:37:00.766+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:00.765+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:00.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:01.012+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:01.011+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:37:01.140+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:01.139+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:37:01.270+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.537 seconds
[2022-12-17T06:37:11.570+0000] {processor.py:154} INFO - Started process (PID=6513) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:11.621+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:37:11.628+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:11.624+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:11.723+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:12.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:12.001+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:37:12.128+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:12.127+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:37:12.258+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.704 seconds
[2022-12-17T06:37:22.526+0000] {processor.py:154} INFO - Started process (PID=6531) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:22.536+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:37:22.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:22.541+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:22.697+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:23.669+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:23.668+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:37:23.825+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:23.824+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:37:23.964+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.453 seconds
[2022-12-17T06:37:34.249+0000] {processor.py:154} INFO - Started process (PID=6541) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:34.260+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:37:34.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:34.263+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:34.351+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:34.800+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:37:45.170+0000] {processor.py:154} INFO - Started process (PID=6551) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:45.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:37:45.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:45.200+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:45.290+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:45.785+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:45.784+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:37:45.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:45.904+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:37:46.043+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.887 seconds
[2022-12-17T06:37:56.461+0000] {processor.py:154} INFO - Started process (PID=6568) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:56.488+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:37:56.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:56.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:56.662+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:37:57.457+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:57.456+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:37:57.893+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:37:57.892+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:37:58.072+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.651 seconds
[2022-12-17T06:38:08.462+0000] {processor.py:154} INFO - Started process (PID=6578) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:08.488+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:38:08.502+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:08.491+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:08.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:08.891+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:08.888+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:38:09.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:09.018+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:38:09.149+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.760 seconds
[2022-12-17T06:38:19.434+0000] {processor.py:154} INFO - Started process (PID=6588) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:19.439+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:38:19.447+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:19.445+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:19.559+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:19.721+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:19.720+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:38:19.839+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:19.838+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:38:19.989+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.579 seconds
[2022-12-17T06:38:30.654+0000] {processor.py:154} INFO - Started process (PID=6598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:30.716+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:38:30.721+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:30.720+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:30.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:30.987+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:30.986+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:38:31.128+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:31.127+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:38:31.238+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.597 seconds
[2022-12-17T06:38:41.784+0000] {processor.py:154} INFO - Started process (PID=6617) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:41.793+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:38:41.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:41.804+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:41.951+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:42.142+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:42.141+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:38:42.358+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:42.357+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:38:42.517+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.765 seconds
[2022-12-17T06:38:52.894+0000] {processor.py:154} INFO - Started process (PID=6627) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:52.922+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:38:52.926+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:52.925+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:53.006+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:38:53.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:53.898+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:38:54.011+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:38:54.010+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:38:54.143+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.261 seconds
[2022-12-17T06:39:04.440+0000] {processor.py:154} INFO - Started process (PID=6637) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:04.656+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:39:04.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:04.659+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:04.747+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:05.343+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:05.342+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:39:05.455+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:05.454+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:39:05.595+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.171 seconds
[2022-12-17T06:39:15.954+0000] {processor.py:154} INFO - Started process (PID=6654) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:16.041+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:39:16.049+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:16.048+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:16.153+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:16.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:16.304+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:39:16.434+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:16.434+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:39:16.591+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.655 seconds
[2022-12-17T06:39:26.941+0000] {processor.py:154} INFO - Started process (PID=6665) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:26.945+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:39:26.949+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:26.948+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:27.036+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:27.182+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:27.178+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:39:27.323+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:27.322+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:39:27.432+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.505 seconds
[2022-12-17T06:39:38.345+0000] {processor.py:154} INFO - Started process (PID=6675) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:38.364+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:39:38.369+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:38.368+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:38.453+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:39.065+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:39.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:39:39.175+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:39.174+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:39:39.329+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.998 seconds
[2022-12-17T06:39:49.538+0000] {processor.py:154} INFO - Started process (PID=6685) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:49.561+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:39:49.566+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:39:49.564+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:49.664+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:39:50.522+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:40:01.041+0000] {processor.py:154} INFO - Started process (PID=6703) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:01.072+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:40:01.080+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:01.079+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:01.321+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:01.663+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:01.662+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:40:01.883+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:01.882+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:40:02.129+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.105 seconds
[2022-12-17T06:40:12.634+0000] {processor.py:154} INFO - Started process (PID=6713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:12.658+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:40:12.662+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:12.661+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:12.885+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:13.115+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:13.114+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:40:13.231+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:13.231+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:40:13.347+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.749 seconds
[2022-12-17T06:40:23.835+0000] {processor.py:154} INFO - Started process (PID=6723) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:23.883+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:40:23.890+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:23.886+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:24.013+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:24.711+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:24.710+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:40:24.827+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:24.826+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:40:24.971+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.150 seconds
[2022-12-17T06:40:35.386+0000] {processor.py:154} INFO - Started process (PID=6733) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:35.444+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:40:35.450+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:35.447+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:35.650+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:36.005+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:36.002+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:40:36.238+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:36.237+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:40:36.431+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.094 seconds
[2022-12-17T06:40:46.904+0000] {processor.py:154} INFO - Started process (PID=6750) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:46.953+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:40:46.958+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:46.957+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:47.089+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:47.960+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:47.959+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:40:48.109+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:48.100+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:40:48.223+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.335 seconds
[2022-12-17T06:40:59.098+0000] {processor.py:154} INFO - Started process (PID=6760) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:59.147+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:40:59.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:59.151+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:59.290+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:40:59.670+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:59.669+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:40:59.849+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:40:59.848+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:41:00.011+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.928 seconds
[2022-12-17T06:41:10.376+0000] {processor.py:154} INFO - Started process (PID=6770) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:10.388+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:41:10.391+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:10.390+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:10.491+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:10.684+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:10.682+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:41:10.980+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:10.978+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:41:11.192+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.830 seconds
[2022-12-17T06:41:21.634+0000] {processor.py:154} INFO - Started process (PID=6788) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:21.639+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:41:21.643+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:21.642+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:21.746+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:21.921+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:21.920+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:41:22.043+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:22.042+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:41:22.180+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.562 seconds
[2022-12-17T06:41:32.609+0000] {processor.py:154} INFO - Started process (PID=6798) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:32.629+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:41:32.634+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:32.632+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:32.717+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:33.397+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:33.396+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:41:33.507+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:33.506+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:41:33.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.086 seconds
[2022-12-17T06:41:43.975+0000] {processor.py:154} INFO - Started process (PID=6808) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:44.018+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:41:44.023+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:44.022+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:44.144+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:45.097+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:41:55.457+0000] {processor.py:154} INFO - Started process (PID=6818) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:55.499+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:41:55.504+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:55.502+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:55.594+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:41:55.831+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:55.830+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:41:55.971+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:41:55.970+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:41:56.109+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.665 seconds
[2022-12-17T06:42:06.682+0000] {processor.py:154} INFO - Started process (PID=6836) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:06.711+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:42:06.716+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:06.715+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:06.829+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:07.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:07.001+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:42:07.145+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:07.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:42:07.302+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.663 seconds
[2022-12-17T06:42:17.628+0000] {processor.py:154} INFO - Started process (PID=6846) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:17.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:42:17.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:17.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:17.768+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:17.904+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:17.903+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:42:18.013+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:18.012+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:42:18.126+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.512 seconds
[2022-12-17T06:42:28.998+0000] {processor.py:154} INFO - Started process (PID=6856) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:29.040+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:42:29.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:29.044+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:29.153+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:29.297+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:29.296+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:42:29.415+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:29.414+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:42:29.710+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.725 seconds
[2022-12-17T06:42:40.162+0000] {processor.py:154} INFO - Started process (PID=6873) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:40.187+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:42:40.191+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:40.190+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:40.340+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:40.500+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:40.499+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:42:40.630+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:40.629+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:42:40.749+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.647 seconds
[2022-12-17T06:42:51.143+0000] {processor.py:154} INFO - Started process (PID=6884) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:51.146+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:42:51.150+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:51.149+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:51.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:42:51.797+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:51.796+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:42:51.966+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:42:51.965+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:42:52.078+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.947 seconds
[2022-12-17T06:43:02.375+0000] {processor.py:154} INFO - Started process (PID=6894) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:02.416+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:43:02.422+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:02.419+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:02.508+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:02.651+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:02.650+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:43:02.762+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:02.761+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:43:02.893+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.531 seconds
[2022-12-17T06:43:13.201+0000] {processor.py:154} INFO - Started process (PID=6904) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:13.231+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:43:13.236+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:13.235+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:13.321+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:13.453+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:13.452+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:43:13.581+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:13.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:43:13.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.540 seconds
[2022-12-17T06:43:24.144+0000] {processor.py:154} INFO - Started process (PID=6922) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:24.161+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:43:24.164+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:24.163+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:24.339+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:24.528+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:24.527+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:43:24.697+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:24.696+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:43:24.881+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.770 seconds
[2022-12-17T06:43:35.436+0000] {processor.py:154} INFO - Started process (PID=6932) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:35.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:43:35.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:35.463+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:35.547+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:35.706+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:35.705+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:43:35.831+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:35.830+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:43:35.945+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.523 seconds
[2022-12-17T06:43:46.303+0000] {processor.py:154} INFO - Started process (PID=6942) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:46.307+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:43:46.314+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:46.313+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:46.478+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:46.726+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:46.725+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:43:46.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:46.972+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:43:47.134+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.846 seconds
[2022-12-17T06:43:57.434+0000] {processor.py:154} INFO - Started process (PID=6952) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:57.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:43:57.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:57.481+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:57.579+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:43:58.309+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:58.308+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:43:58.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:43:58.425+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:43:58.622+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.202 seconds
[2022-12-17T06:44:09.079+0000] {processor.py:154} INFO - Started process (PID=6969) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:09.122+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:44:09.139+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:09.126+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:09.376+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:10.149+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:10.147+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:44:10.279+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:10.278+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:44:10.661+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.600 seconds
[2022-12-17T06:44:21.741+0000] {processor.py:154} INFO - Started process (PID=6979) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:21.764+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:44:21.769+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:21.768+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:22.060+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:22.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:22.232+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:44:22.377+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:22.376+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:44:22.488+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.782 seconds
[2022-12-17T06:44:32.943+0000] {processor.py:154} INFO - Started process (PID=6989) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:32.967+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:44:32.971+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:32.970+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:33.054+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:33.204+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:33.203+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:44:33.610+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:33.609+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:44:33.796+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.870 seconds
[2022-12-17T06:44:44.229+0000] {processor.py:154} INFO - Started process (PID=7007) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:44.272+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:44:44.281+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:44.279+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:44.423+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:44.890+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:44.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:44:45.283+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:45.282+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:44:45.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.447 seconds
[2022-12-17T06:44:56.536+0000] {processor.py:154} INFO - Started process (PID=7017) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:56.579+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:44:56.586+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:56.585+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:56.776+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:44:56.929+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:56.928+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:44:57.064+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:44:57.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:44:57.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.664 seconds
[2022-12-17T06:45:07.978+0000] {processor.py:154} INFO - Started process (PID=7027) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:08.003+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:45:08.018+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:08.011+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:08.235+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:08.974+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:08.973+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:45:09.085+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:09.084+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:45:09.234+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.286 seconds
[2022-12-17T06:45:19.576+0000] {processor.py:154} INFO - Started process (PID=7037) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:19.626+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:45:19.630+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:19.629+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:19.741+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:20.176+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:20.175+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:45:20.392+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:20.391+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:45:20.510+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.988 seconds
[2022-12-17T06:45:30.715+0000] {processor.py:154} INFO - Started process (PID=7056) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:30.785+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:45:30.790+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:30.789+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:30.884+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:31.022+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:31.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:45:31.135+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:31.134+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:45:31.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.597 seconds
[2022-12-17T06:45:42.165+0000] {processor.py:154} INFO - Started process (PID=7066) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:42.177+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:45:42.181+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:42.180+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:42.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:43.140+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:45:53.443+0000] {processor.py:154} INFO - Started process (PID=7076) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:53.493+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:45:53.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:53.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:53.601+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:45:53.907+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:53.906+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:45:54.020+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:45:54.019+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:45:54.156+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.740 seconds
[2022-12-17T06:46:04.446+0000] {processor.py:154} INFO - Started process (PID=7094) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:04.471+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:46:04.475+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:04.474+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:04.599+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:05.526+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:46:15.773+0000] {processor.py:154} INFO - Started process (PID=7104) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:15.824+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:46:15.828+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:15.827+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:15.937+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:16.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:16.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:46:16.364+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:16.363+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:46:16.542+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.783 seconds
[2022-12-17T06:46:27.090+0000] {processor.py:154} INFO - Started process (PID=7114) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:27.117+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:46:27.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:27.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:27.204+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:27.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:27.337+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:46:27.455+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:27.454+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:46:27.593+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.516 seconds
[2022-12-17T06:46:37.945+0000] {processor.py:154} INFO - Started process (PID=7124) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:37.975+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:46:37.979+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:37.978+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:38.062+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:38.575+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:38.573+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:46:38.704+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:38.703+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:46:38.854+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.925 seconds
[2022-12-17T06:46:49.390+0000] {processor.py:154} INFO - Started process (PID=7143) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:49.410+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:46:49.417+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:46:49.417+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:49.652+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:46:50.820+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:47:01.193+0000] {processor.py:154} INFO - Started process (PID=7153) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:01.238+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:47:01.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:01.241+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:01.323+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:02.261+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:02.260+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:47:02.392+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:02.391+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:47:02.494+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.317 seconds
[2022-12-17T06:47:12.785+0000] {processor.py:154} INFO - Started process (PID=7163) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:12.845+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:47:12.849+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:12.848+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:12.948+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:13.423+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:13.422+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:47:13.534+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:13.534+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:47:13.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.888 seconds
[2022-12-17T06:47:24.485+0000] {processor.py:154} INFO - Started process (PID=7181) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:24.493+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:47:24.498+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:24.497+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:24.615+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:24.775+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:24.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:47:24.951+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:24.950+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:47:25.091+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.641 seconds
[2022-12-17T06:47:35.305+0000] {processor.py:154} INFO - Started process (PID=7191) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:35.328+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:47:35.333+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:35.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:35.414+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:35.548+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:35.547+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:47:35.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:35.664+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:47:35.801+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.510 seconds
[2022-12-17T06:47:46.140+0000] {processor.py:154} INFO - Started process (PID=7201) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:46.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:47:46.192+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:46.191+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:46.294+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:46.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:46.873+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:47:46.998+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:46.997+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:47:47.138+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.012 seconds
[2022-12-17T06:47:57.526+0000] {processor.py:154} INFO - Started process (PID=7211) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:57.572+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:47:57.577+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:57.576+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:57.672+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:47:58.178+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:58.177+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:47:58.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:47:58.303+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:47:58.462+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.949 seconds
[2022-12-17T06:48:09.023+0000] {processor.py:154} INFO - Started process (PID=7229) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:09.028+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:48:09.032+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:09.031+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:09.173+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:09.617+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:09.616+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:48:09.763+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:09.762+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:48:09.969+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.963 seconds
[2022-12-17T06:48:20.172+0000] {processor.py:154} INFO - Started process (PID=7239) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:20.221+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:48:20.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:20.224+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:20.313+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:20.457+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:20.456+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:48:20.607+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:20.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:48:20.947+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.793 seconds
[2022-12-17T06:48:31.259+0000] {processor.py:154} INFO - Started process (PID=7249) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:31.501+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:48:31.505+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:31.504+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:31.611+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:32.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:32.425+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:48:32.535+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:32.534+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:48:32.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.437 seconds
[2022-12-17T06:48:43.459+0000] {processor.py:154} INFO - Started process (PID=7266) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:43.463+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:48:43.468+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:43.467+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:43.585+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:44.460+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:44.458+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:48:44.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:44.674+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:48:44.800+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.356 seconds
[2022-12-17T06:48:55.207+0000] {processor.py:154} INFO - Started process (PID=7277) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:55.230+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:48:55.245+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:55.244+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:55.383+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:48:55.530+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:55.529+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:48:55.653+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:48:55.652+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:48:55.779+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.613 seconds
[2022-12-17T06:49:05.936+0000] {processor.py:154} INFO - Started process (PID=7287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:06.060+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:49:06.065+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:06.064+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:06.290+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:06.824+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:06.823+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:49:06.979+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:06.978+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:49:07.168+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.248 seconds
[2022-12-17T06:49:17.521+0000] {processor.py:154} INFO - Started process (PID=7297) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:17.538+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:49:17.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:17.541+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:17.707+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:18.869+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:49:29.576+0000] {processor.py:154} INFO - Started process (PID=7315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:29.646+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:49:29.651+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:29.650+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:29.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:30.953+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:49:41.409+0000] {processor.py:154} INFO - Started process (PID=7325) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:41.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:49:41.463+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:41.462+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:41.595+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:41.732+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:41.731+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:49:41.853+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:41.851+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:49:41.993+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.609 seconds
[2022-12-17T06:49:52.409+0000] {processor.py:154} INFO - Started process (PID=7335) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:52.429+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:49:52.433+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:52.432+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:52.929+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:49:53.383+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:53.382+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:49:53.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:49:53.536+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:49:53.666+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.271 seconds
[2022-12-17T06:50:03.942+0000] {processor.py:154} INFO - Started process (PID=7345) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:03.974+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:50:03.980+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:03.978+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:04.064+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:04.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:04.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:50:04.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:04.466+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:50:04.782+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.854 seconds
[2022-12-17T06:50:15.646+0000] {processor.py:154} INFO - Started process (PID=7363) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:15.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:50:15.713+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:15.712+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:15.820+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:15.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:15.972+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:50:16.089+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:16.088+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:50:16.192+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.563 seconds
[2022-12-17T06:50:26.478+0000] {processor.py:154} INFO - Started process (PID=7373) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:26.502+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:50:26.506+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:26.505+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:26.617+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:27.485+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:27.484+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:50:27.633+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:27.632+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:50:27.852+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.388 seconds
[2022-12-17T06:50:38.198+0000] {processor.py:154} INFO - Started process (PID=7383) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:38.258+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:50:38.263+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:38.262+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:38.346+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:38.524+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:38.522+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:50:38.655+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:38.654+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:50:38.765+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.580 seconds
[2022-12-17T06:50:49.212+0000] {processor.py:154} INFO - Started process (PID=7400) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:49.229+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:50:49.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:49.232+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:49.415+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:50:49.588+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:49.587+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:50:49.767+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:50:49.766+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:50:49.983+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.786 seconds
[2022-12-17T06:51:00.396+0000] {processor.py:154} INFO - Started process (PID=7410) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:00.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:51:00.447+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:00.446+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:00.531+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:01.214+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:51:11.836+0000] {processor.py:154} INFO - Started process (PID=7420) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:11.877+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:51:11.881+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:11.880+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:11.989+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:12.127+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:12.126+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:51:12.239+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:12.238+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:51:12.341+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.520 seconds
[2022-12-17T06:51:22.622+0000] {processor.py:154} INFO - Started process (PID=7430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:22.671+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:51:22.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:22.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:22.761+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:22.907+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:22.906+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:51:23.018+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:23.017+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:51:23.133+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.524 seconds
[2022-12-17T06:51:33.597+0000] {processor.py:154} INFO - Started process (PID=7448) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:33.648+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:51:33.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:33.659+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:33.783+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:33.964+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:33.963+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:51:34.145+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:34.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:51:34.284+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.750 seconds
[2022-12-17T06:51:45.185+0000] {processor.py:154} INFO - Started process (PID=7458) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:45.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:51:45.214+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:45.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:45.297+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:46.615+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:46.614+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:51:46.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:46.733+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:51:46.876+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.706 seconds
[2022-12-17T06:51:57.255+0000] {processor.py:154} INFO - Started process (PID=7468) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:57.275+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:51:57.280+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:57.279+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:57.362+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:51:57.500+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:57.499+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:51:57.619+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:51:57.618+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:51:57.751+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.510 seconds
[2022-12-17T06:52:08.090+0000] {processor.py:154} INFO - Started process (PID=7484) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:08.113+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:52:08.117+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:08.116+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:08.229+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:08.381+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:08.379+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:52:08.508+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:08.507+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:52:08.684+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.608 seconds
[2022-12-17T06:52:18.940+0000] {processor.py:154} INFO - Started process (PID=7495) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:18.961+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:52:18.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:18.968+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:19.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:19.335+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:19.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:52:19.448+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:19.447+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:52:19.586+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-17T06:52:29.984+0000] {processor.py:154} INFO - Started process (PID=7505) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:30.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:52:30.043+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:30.042+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:30.146+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:30.497+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:30.496+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:52:30.618+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:30.617+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:52:30.756+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.786 seconds
[2022-12-17T06:52:41.496+0000] {processor.py:154} INFO - Started process (PID=7515) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:41.544+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:52:41.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:41.548+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:41.646+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:41.775+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:41.774+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:52:41.896+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:41.896+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:52:42.024+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.542 seconds
[2022-12-17T06:52:52.962+0000] {processor.py:154} INFO - Started process (PID=7534) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:52.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:52:53.008+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:53.007+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:53.216+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:52:53.406+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:53.405+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:52:53.621+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:52:53.620+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:52:53.754+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.809 seconds
[2022-12-17T06:53:04.410+0000] {processor.py:154} INFO - Started process (PID=7544) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:04.448+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:53:04.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:04.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:04.554+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:04.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:04.709+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:53:04.825+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:04.824+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:53:04.942+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.546 seconds
[2022-12-17T06:53:15.238+0000] {processor.py:154} INFO - Started process (PID=7554) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:15.307+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:53:15.320+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:15.311+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:15.411+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:15.543+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:15.542+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:53:15.672+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:15.671+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:53:15.781+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.560 seconds
[2022-12-17T06:53:25.948+0000] {processor.py:154} INFO - Started process (PID=7564) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:25.967+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:53:25.972+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:25.971+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:26.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:27.552+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:53:38.129+0000] {processor.py:154} INFO - Started process (PID=7583) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:38.166+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:53:38.176+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:38.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:38.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:38.858+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:38.857+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:53:39.267+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:39.266+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:53:39.441+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.338 seconds
[2022-12-17T06:53:49.713+0000] {processor.py:154} INFO - Started process (PID=7593) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:49.757+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:53:49.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:49.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:49.852+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:53:50.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:50.792+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:53:50.971+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:53:50.970+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:53:51.126+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.427 seconds
[2022-12-17T06:54:01.437+0000] {processor.py:154} INFO - Started process (PID=7603) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:01.457+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:54:01.462+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:01.461+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:01.544+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:01.685+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:01.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:54:01.797+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:01.796+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:54:01.956+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.533 seconds
[2022-12-17T06:54:12.250+0000] {processor.py:154} INFO - Started process (PID=7621) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:12.372+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:54:12.380+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:12.375+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:12.617+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:12.867+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:12.866+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:54:13.081+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:13.080+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:54:13.300+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.070 seconds
[2022-12-17T06:54:23.462+0000] {processor.py:154} INFO - Started process (PID=7631) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:23.475+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:54:23.479+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:23.478+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:23.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:23.757+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:23.756+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:54:23.887+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:23.886+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:54:24.015+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.573 seconds
[2022-12-17T06:54:34.279+0000] {processor.py:154} INFO - Started process (PID=7641) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:34.324+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:54:34.328+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:34.327+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:34.416+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:34.952+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:34.951+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:54:35.066+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:35.065+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:54:35.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.918 seconds
[2022-12-17T06:54:45.472+0000] {processor.py:154} INFO - Started process (PID=7651) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:45.515+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:54:45.520+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:45.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:45.629+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:45.977+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:45.976+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:54:46.094+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:46.093+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:54:46.202+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.744 seconds
[2022-12-17T06:54:56.717+0000] {processor.py:154} INFO - Started process (PID=7669) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:56.740+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:54:56.748+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:56.743+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:56.926+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:54:58.073+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:58.072+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:54:58.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:54:58.208+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:54:58.471+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.787 seconds
[2022-12-17T06:55:08.930+0000] {processor.py:154} INFO - Started process (PID=7679) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:08.951+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:55:08.955+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:08.954+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:09.038+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:09.444+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:09.443+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:55:09.570+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:09.569+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:55:09.678+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.762 seconds
[2022-12-17T06:55:19.951+0000] {processor.py:154} INFO - Started process (PID=7689) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:19.998+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:55:20.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:20.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:20.127+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:20.297+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:20.295+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:55:20.423+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:20.422+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:55:20.606+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.678 seconds
[2022-12-17T06:55:30.947+0000] {processor.py:154} INFO - Started process (PID=7706) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:30.968+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:55:30.976+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:30.975+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:31.090+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:32.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:32.002+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:55:32.178+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:32.177+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:55:32.374+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.443 seconds
[2022-12-17T06:55:42.782+0000] {processor.py:154} INFO - Started process (PID=7717) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:42.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:55:42.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:42.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:42.961+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:44.046+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:55:54.531+0000] {processor.py:154} INFO - Started process (PID=7727) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:54.580+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:55:54.585+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:54.584+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:54.689+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:55:54.893+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:54.892+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:55:55.089+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:55:55.082+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:55:55.384+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.909 seconds
[2022-12-17T06:56:05.994+0000] {processor.py:154} INFO - Started process (PID=7737) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:06.082+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:56:06.086+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:06.085+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:06.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:06.572+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:06.570+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:56:06.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:06.710+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:56:06.831+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.879 seconds
[2022-12-17T06:56:17.273+0000] {processor.py:154} INFO - Started process (PID=7755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:17.283+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:56:17.289+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:17.288+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:17.587+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:18.119+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:18.118+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:56:18.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:18.304+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:56:18.508+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.251 seconds
[2022-12-17T06:56:28.888+0000] {processor.py:154} INFO - Started process (PID=7765) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:28.891+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:56:28.895+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:28.894+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:29.000+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:29.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:29.164+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:56:29.281+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:29.280+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:56:29.533+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.660 seconds
[2022-12-17T06:56:39.912+0000] {processor.py:154} INFO - Started process (PID=7775) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:39.978+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:56:39.982+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:39.981+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:40.077+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:41.617+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:41.616+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:56:41.740+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:41.739+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:56:41.907+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.009 seconds
[2022-12-17T06:56:52.413+0000] {processor.py:154} INFO - Started process (PID=7792) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:52.465+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:56:52.469+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:52.468+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:52.662+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:56:52.939+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:52.938+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:56:53.252+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:56:53.251+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:56:53.403+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.007 seconds
[2022-12-17T06:57:03.667+0000] {processor.py:154} INFO - Started process (PID=7802) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:03.800+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:57:03.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:03.804+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:03.899+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:04.087+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:04.086+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:57:04.219+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:04.218+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:57:04.361+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.719 seconds
[2022-12-17T06:57:14.667+0000] {processor.py:154} INFO - Started process (PID=7812) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:14.715+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:57:14.718+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:14.717+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:14.807+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:15.425+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:15.424+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:57:15.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:15.546+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:57:15.697+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.044 seconds
[2022-12-17T06:57:26.054+0000] {processor.py:154} INFO - Started process (PID=7822) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:26.087+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:57:26.091+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:26.090+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:26.185+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:26.332+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:26.330+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:57:26.453+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:26.452+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:57:26.578+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.544 seconds
[2022-12-17T06:57:36.902+0000] {processor.py:154} INFO - Started process (PID=7839) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:36.935+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:57:36.939+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:36.938+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:37.130+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:37.928+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:37.927+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:57:38.203+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:38.202+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:57:38.359+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.526 seconds
[2022-12-17T06:57:48.740+0000] {processor.py:154} INFO - Started process (PID=7849) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:48.769+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:57:48.774+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:48.773+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:48.968+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:57:49.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:49.321+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:57:49.434+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:57:49.433+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:57:49.562+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.896 seconds
[2022-12-17T06:58:00.074+0000] {processor.py:154} INFO - Started process (PID=7859) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:58:00.106+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:58:00.122+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:58:00.121+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:58:00.397+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:58:01.160+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:58:01.159+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:58:01.272+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:58:01.271+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:58:01.388+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.407 seconds
[2022-12-17T06:58:57.684+0000] {processor.py:154} INFO - Started process (PID=182) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:58:57.693+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:58:57.697+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:58:57.696+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:58:58.166+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:58:58.953+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T06:59:09.473+0000] {processor.py:154} INFO - Started process (PID=192) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:09.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:59:09.481+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:09.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:09.612+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:09.791+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:09.790+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:59:09.949+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:09.948+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:59:10.098+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.639 seconds
[2022-12-17T06:59:20.770+0000] {processor.py:154} INFO - Started process (PID=199) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:20.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:59:20.777+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:20.776+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:20.938+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:21.123+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:21.122+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:59:21.277+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:21.276+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:59:21.497+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.740 seconds
[2022-12-17T06:59:31.837+0000] {processor.py:154} INFO - Started process (PID=209) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:31.885+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:59:31.888+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:31.888+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:31.988+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:32.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:32.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:59:32.378+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:32.377+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:59:32.724+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.902 seconds
[2022-12-17T06:59:43.055+0000] {processor.py:154} INFO - Started process (PID=227) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:43.085+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:59:43.089+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:43.088+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:43.318+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:43.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:43.587+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:59:43.774+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:43.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:59:44.081+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.040 seconds
[2022-12-17T06:59:54.418+0000] {processor.py:154} INFO - Started process (PID=237) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:54.476+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T06:59:54.481+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:54.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:54.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T06:59:54.738+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:54.737+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T06:59:54.924+0000] {logging_mixin.py:137} INFO - [2022-12-17T06:59:54.922+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T06:59:55.072+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.669 seconds
[2022-12-17T07:00:05.375+0000] {processor.py:154} INFO - Started process (PID=247) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:05.402+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:00:05.407+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:05.406+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:05.490+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:06.245+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:00:16.772+0000] {processor.py:154} INFO - Started process (PID=267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:16.781+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:00:16.785+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:16.784+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:16.987+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:17.517+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:17.516+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:00:17.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:17.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:00:18.081+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.345 seconds
[2022-12-17T07:00:28.417+0000] {processor.py:154} INFO - Started process (PID=278) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:28.427+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:00:28.431+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:28.430+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:28.516+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:28.673+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:28.672+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:00:28.789+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:28.788+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:00:28.974+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.572 seconds
[2022-12-17T07:00:39.101+0000] {processor.py:154} INFO - Started process (PID=285) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:39.143+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:00:39.147+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:39.146+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:39.230+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:40.112+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:00:50.501+0000] {processor.py:154} INFO - Started process (PID=295) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:50.514+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:00:50.519+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:50.518+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:50.611+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:00:50.745+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:50.745+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:00:50.875+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:00:50.874+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:00:51.023+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.535 seconds
[2022-12-17T07:01:01.366+0000] {processor.py:154} INFO - Started process (PID=313) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:01.417+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:01:01.424+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:01.420+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:01.613+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:02.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:02.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:01:02.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:02.259+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:01:02.574+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.263 seconds
[2022-12-17T07:01:12.757+0000] {processor.py:154} INFO - Started process (PID=323) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:12.798+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:01:12.803+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:12.802+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:12.893+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:13.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:13.033+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:01:13.170+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:13.169+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:01:13.272+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.528 seconds
[2022-12-17T07:01:23.544+0000] {processor.py:154} INFO - Started process (PID=333) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:23.558+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:01:23.566+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:23.565+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:23.727+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:24.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:24.147+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:01:24.258+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:24.257+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:01:24.385+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.858 seconds
[2022-12-17T07:01:34.767+0000] {processor.py:154} INFO - Started process (PID=343) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:34.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:01:34.810+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:34.809+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:34.930+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:35.945+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:35.944+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:01:36.127+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:36.126+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:01:36.275+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.558 seconds
[2022-12-17T07:01:46.774+0000] {processor.py:154} INFO - Started process (PID=361) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:46.841+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:01:46.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:46.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:46.976+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:47.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:47.743+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:01:47.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:47.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:01:48.070+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.339 seconds
[2022-12-17T07:01:58.374+0000] {processor.py:154} INFO - Started process (PID=374) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:58.394+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:01:58.398+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:58.397+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:58.495+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:01:58.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:58.659+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:01:58.787+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:01:58.786+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:01:58.935+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.575 seconds
[2022-12-17T07:02:09.425+0000] {processor.py:154} INFO - Started process (PID=384) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:09.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:02:09.446+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:09.445+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:09.722+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:09.875+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:09.874+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:02:09.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:09.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:02:10.147+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.792 seconds
[2022-12-17T07:02:20.692+0000] {processor.py:154} INFO - Started process (PID=400) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:20.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:02:20.720+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:20.719+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:20.921+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:21.275+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:21.274+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:02:21.416+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:21.415+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:02:21.589+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.959 seconds
[2022-12-17T07:02:31.926+0000] {processor.py:154} INFO - Started process (PID=410) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:31.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:02:31.953+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:31.952+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:32.040+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:32.649+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:32.648+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:02:32.896+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:32.895+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:02:33.030+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.117 seconds
[2022-12-17T07:02:43.296+0000] {processor.py:154} INFO - Started process (PID=420) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:43.300+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:02:43.304+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:43.303+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:43.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:43.624+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:43.623+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:02:43.901+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:43.897+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:02:44.104+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.824 seconds
[2022-12-17T07:02:54.427+0000] {processor.py:154} INFO - Started process (PID=430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:54.430+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:02:54.434+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:54.433+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:54.517+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:02:54.813+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:54.812+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:02:54.932+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:02:54.931+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:02:55.071+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.658 seconds
[2022-12-17T07:03:05.542+0000] {processor.py:154} INFO - Started process (PID=448) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:05.568+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:03:05.576+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:05.575+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:05.777+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:06.713+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:03:17.366+0000] {processor.py:154} INFO - Started process (PID=458) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:17.413+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:03:17.418+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:17.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:17.499+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:17.644+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:17.643+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:03:17.754+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:17.753+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:03:17.906+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.554 seconds
[2022-12-17T07:03:28.792+0000] {processor.py:154} INFO - Started process (PID=468) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:28.815+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:03:28.828+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:28.827+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:28.935+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:29.075+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:29.074+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:03:29.185+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:29.184+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:03:29.327+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.551 seconds
[2022-12-17T07:03:39.727+0000] {processor.py:154} INFO - Started process (PID=483) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:39.752+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:03:39.756+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:39.755+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:39.872+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:40.409+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:03:51.060+0000] {processor.py:154} INFO - Started process (PID=495) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:51.075+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:03:51.080+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:51.079+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:51.165+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:03:51.348+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:51.347+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:03:51.474+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:03:51.473+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:03:51.593+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.546 seconds
[2022-12-17T07:04:01.811+0000] {processor.py:154} INFO - Started process (PID=505) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:01.848+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:04:01.858+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:01.857+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:02.015+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:02.594+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:04:13.081+0000] {processor.py:154} INFO - Started process (PID=515) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:13.104+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:04:13.109+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:13.108+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:13.192+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:13.825+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:13.824+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:04:13.949+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:13.948+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:04:14.112+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.045 seconds
[2022-12-17T07:04:24.440+0000] {processor.py:154} INFO - Started process (PID=533) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:24.484+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:04:24.489+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:24.488+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:24.663+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:25.210+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:25.209+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:04:25.479+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:25.478+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:04:25.682+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.261 seconds
[2022-12-17T07:04:36.069+0000] {processor.py:154} INFO - Started process (PID=543) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:36.096+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:04:36.100+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:36.099+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:36.210+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:36.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:36.367+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:04:36.486+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:36.485+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:04:36.624+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.591 seconds
[2022-12-17T07:04:47.214+0000] {processor.py:154} INFO - Started process (PID=553) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:47.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:04:47.249+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:47.247+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:47.331+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:47.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:47.465+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:04:47.585+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:47.584+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:04:47.718+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.519 seconds
[2022-12-17T07:04:58.129+0000] {processor.py:154} INFO - Started process (PID=563) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:58.158+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:04:58.163+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:58.162+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:58.373+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:04:58.970+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:58.969+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:04:59.107+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:04:59.106+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:04:59.242+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.160 seconds
[2022-12-17T07:05:09.885+0000] {processor.py:154} INFO - Started process (PID=581) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:09.903+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:05:09.908+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:09.906+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:10.245+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:11.205+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:05:21.696+0000] {processor.py:154} INFO - Started process (PID=591) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:21.740+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:05:21.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:21.743+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:21.997+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:23.214+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:05:34.277+0000] {processor.py:154} INFO - Started process (PID=604) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:34.280+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:05:34.284+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:34.283+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:34.381+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:34.532+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:34.531+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:05:34.650+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:34.649+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:05:34.791+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.527 seconds
[2022-12-17T07:05:45.661+0000] {processor.py:154} INFO - Started process (PID=622) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:45.666+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:05:45.679+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:45.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:45.788+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:46.100+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:46.099+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:05:46.246+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:46.245+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:05:46.398+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.781 seconds
[2022-12-17T07:05:57.006+0000] {processor.py:154} INFO - Started process (PID=632) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:57.017+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:05:57.021+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:57.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:57.128+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:05:57.272+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:57.271+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:05:57.383+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:05:57.382+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:05:57.522+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-17T07:06:08.352+0000] {processor.py:154} INFO - Started process (PID=642) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:08.356+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:06:08.359+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:08.358+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:08.510+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:08.999+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:06:19.630+0000] {processor.py:154} INFO - Started process (PID=649) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:19.656+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:06:19.661+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:19.660+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:19.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:19.885+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:19.884+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:06:19.998+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:19.997+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:06:20.175+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.559 seconds
[2022-12-17T07:06:30.624+0000] {processor.py:154} INFO - Started process (PID=667) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:30.642+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:06:30.648+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:30.647+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:30.981+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:31.602+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:31.601+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:06:31.809+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:31.808+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:06:32.023+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.418 seconds
[2022-12-17T07:06:42.887+0000] {processor.py:154} INFO - Started process (PID=677) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:42.914+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:06:42.918+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:42.917+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:43.000+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:43.708+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:43.707+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:06:43.970+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:43.969+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:06:44.292+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.419 seconds
[2022-12-17T07:06:54.626+0000] {processor.py:154} INFO - Started process (PID=687) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:54.648+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:06:54.652+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:54.652+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:54.763+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:06:54.902+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:54.901+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:06:55.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:06:55.013+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:06:55.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.578 seconds
[2022-12-17T07:07:05.687+0000] {processor.py:154} INFO - Started process (PID=706) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:05.744+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:07:05.760+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:05.747+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:05.922+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:06.095+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:06.094+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:07:06.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:06.232+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:07:06.362+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.697 seconds
[2022-12-17T07:07:16.607+0000] {processor.py:154} INFO - Started process (PID=716) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:16.610+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:07:16.615+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:16.614+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:16.697+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:16.958+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:16.957+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:07:17.327+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:17.326+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:07:17.468+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-17T07:07:28.340+0000] {processor.py:154} INFO - Started process (PID=726) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:28.389+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:07:28.393+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:28.392+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:28.748+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:29.008+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:29.007+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:07:29.140+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:29.139+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:07:29.250+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.926 seconds
[2022-12-17T07:07:39.832+0000] {processor.py:154} INFO - Started process (PID=736) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:39.890+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:07:40.008+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:40.001+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:40.298+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:40.960+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:40.960+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:07:41.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:41.165+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:07:41.480+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.664 seconds
[2022-12-17T07:07:51.727+0000] {processor.py:154} INFO - Started process (PID=757) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:51.747+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:07:51.751+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:51.750+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:51.881+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:07:52.728+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:52.727+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:07:52.914+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:07:52.914+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:07:53.044+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.332 seconds
[2022-12-17T07:08:03.342+0000] {processor.py:154} INFO - Started process (PID=767) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:03.358+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:08:03.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:03.362+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:03.465+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:03.687+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:03.686+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:08:04.093+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:04.092+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:08:04.315+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.999 seconds
[2022-12-17T07:08:14.633+0000] {processor.py:154} INFO - Started process (PID=777) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:14.637+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:08:14.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:14.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:14.739+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:14.934+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:14.933+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:08:15.071+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:15.070+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:08:15.221+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.617 seconds
[2022-12-17T07:08:25.584+0000] {processor.py:154} INFO - Started process (PID=796) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:25.631+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:08:25.635+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:25.634+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:25.798+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:26.028+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:26.026+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:08:26.231+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:26.230+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:08:26.383+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.842 seconds
[2022-12-17T07:08:36.851+0000] {processor.py:154} INFO - Started process (PID=806) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:36.889+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:08:36.893+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:36.892+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:37.008+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:37.173+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:37.171+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:08:37.294+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:37.293+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:08:37.416+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.597 seconds
[2022-12-17T07:08:47.710+0000] {processor.py:154} INFO - Started process (PID=816) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:47.771+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:08:47.775+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:47.774+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:47.912+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:48.211+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:48.210+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:08:48.354+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:48.353+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:08:48.543+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.852 seconds
[2022-12-17T07:08:58.924+0000] {processor.py:154} INFO - Started process (PID=826) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:58.940+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:08:58.945+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:58.944+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:59.050+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:08:59.192+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:59.191+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:08:59.302+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:08:59.302+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:08:59.439+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.527 seconds
[2022-12-17T07:09:09.658+0000] {processor.py:154} INFO - Started process (PID=841) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:09.719+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:09:09.731+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:09.730+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:09.956+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:10.223+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:10.222+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:09:10.499+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:10.498+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:09:10.776+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.135 seconds
[2022-12-17T07:09:21.122+0000] {processor.py:154} INFO - Started process (PID=851) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:21.169+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:09:21.174+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:21.173+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:21.267+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:21.400+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:21.399+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:09:21.513+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:21.512+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:09:21.654+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.548 seconds
[2022-12-17T07:09:31.933+0000] {processor.py:154} INFO - Started process (PID=861) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:31.952+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:09:31.956+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:31.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:32.048+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:32.592+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:32.591+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:09:32.701+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:32.700+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:09:32.880+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.961 seconds
[2022-12-17T07:09:43.168+0000] {processor.py:154} INFO - Started process (PID=877) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:43.198+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:09:43.207+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:43.206+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:43.315+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:44.369+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:44.368+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:09:44.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:44.674+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:09:44.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.731 seconds
[2022-12-17T07:09:55.084+0000] {processor.py:154} INFO - Started process (PID=888) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:55.121+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:09:55.125+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:55.124+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:55.209+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:09:55.340+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:55.340+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:09:55.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:09:55.450+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:09:55.602+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.533 seconds
[2022-12-17T07:10:06.279+0000] {processor.py:154} INFO - Started process (PID=898) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:06.312+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:10:06.317+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:06.316+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:06.400+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:06.764+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:10:16.970+0000] {processor.py:154} INFO - Started process (PID=908) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:16.995+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:10:17.007+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:17.006+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:17.124+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:17.471+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:17.470+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:10:17.593+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:17.593+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:10:17.735+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.781 seconds
[2022-12-17T07:10:28.041+0000] {processor.py:154} INFO - Started process (PID=926) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:28.072+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:10:28.081+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:28.080+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:28.241+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:28.634+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:28.633+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:10:28.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:28.944+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:10:29.070+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.050 seconds
[2022-12-17T07:10:39.486+0000] {processor.py:154} INFO - Started process (PID=936) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:39.577+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:10:39.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:39.579+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:39.685+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:39.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:39.833+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:10:40.156+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:40.156+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:10:40.364+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.901 seconds
[2022-12-17T07:10:50.741+0000] {processor.py:154} INFO - Started process (PID=946) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:50.745+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:10:50.749+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:50.748+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:50.840+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:10:50.977+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:50.976+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:10:51.095+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:10:51.094+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:10:51.239+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.512 seconds
[2022-12-17T07:11:01.577+0000] {processor.py:154} INFO - Started process (PID=956) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:01.629+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:11:01.640+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:01.639+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:01.740+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:02.241+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:02.240+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:11:02.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:02.356+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:11:02.491+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.933 seconds
[2022-12-17T07:11:12.671+0000] {processor.py:154} INFO - Started process (PID=974) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:12.702+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:11:12.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:12.711+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:12.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:13.975+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:13.974+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:11:14.208+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:14.207+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:11:14.366+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.710 seconds
[2022-12-17T07:11:24.956+0000] {processor.py:154} INFO - Started process (PID=984) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:25.008+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:11:25.013+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:25.012+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:25.098+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:25.246+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:25.246+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:11:25.360+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:25.359+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:11:25.517+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.574 seconds
[2022-12-17T07:11:35.808+0000] {processor.py:154} INFO - Started process (PID=994) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:35.835+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:11:35.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:35.841+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:35.934+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:36.516+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:36.515+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:11:36.637+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:36.636+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:11:36.772+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.978 seconds
[2022-12-17T07:11:47.141+0000] {processor.py:154} INFO - Started process (PID=1012) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:47.196+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:11:47.204+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:47.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:47.313+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:47.557+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:47.555+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:11:47.696+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:47.695+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:11:47.849+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.727 seconds
[2022-12-17T07:11:58.201+0000] {processor.py:154} INFO - Started process (PID=1022) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:58.251+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:11:58.256+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:58.255+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:58.343+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:11:59.692+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:59.691+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:11:59.808+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:11:59.807+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:11:59.920+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.734 seconds
[2022-12-17T07:12:10.658+0000] {processor.py:154} INFO - Started process (PID=1032) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:10.705+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:12:10.709+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:10.708+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:10.795+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:10.933+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:10.932+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:12:11.063+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:11.062+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:12:11.200+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.557 seconds
[2022-12-17T07:12:21.339+0000] {processor.py:154} INFO - Started process (PID=1042) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:21.365+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:12:21.370+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:21.369+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:21.458+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:21.600+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:21.599+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:12:21.709+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:21.708+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:12:21.868+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.545 seconds
[2022-12-17T07:12:32.741+0000] {processor.py:154} INFO - Started process (PID=1060) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:32.756+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:12:32.760+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:32.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:32.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:33.070+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:33.069+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:12:33.207+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:33.206+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:12:33.654+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.931 seconds
[2022-12-17T07:12:44.019+0000] {processor.py:154} INFO - Started process (PID=1070) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:44.058+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:12:44.064+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:44.063+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:44.155+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:44.478+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:44.477+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:12:44.604+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:44.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:12:44.754+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.747 seconds
[2022-12-17T07:12:54.967+0000] {processor.py:154} INFO - Started process (PID=1080) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:54.989+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:12:54.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:54.993+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:55.078+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:12:55.365+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:55.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:12:55.499+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:12:55.498+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:12:55.630+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.678 seconds
[2022-12-17T07:13:05.971+0000] {processor.py:154} INFO - Started process (PID=1097) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:05.988+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:13:05.995+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:05.994+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:06.112+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:06.831+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:13:17.515+0000] {processor.py:154} INFO - Started process (PID=1108) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:17.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:13:17.541+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:17.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:17.632+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:17.764+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:17.763+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:13:17.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:17.878+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:13:17.997+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.497 seconds
[2022-12-17T07:13:28.291+0000] {processor.py:154} INFO - Started process (PID=1118) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:28.305+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:13:28.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:28.309+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:28.393+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:28.525+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:28.524+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:13:28.656+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:28.655+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:13:28.790+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.513 seconds
[2022-12-17T07:13:39.118+0000] {processor.py:154} INFO - Started process (PID=1128) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:39.131+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:13:39.134+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:39.133+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:39.295+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:39.600+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:39.599+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:13:39.720+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:39.719+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:13:39.840+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.740 seconds
[2022-12-17T07:13:50.202+0000] {processor.py:154} INFO - Started process (PID=1146) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:50.267+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:13:50.270+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:50.269+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:50.418+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:13:50.729+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:50.722+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:13:51.058+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:13:51.057+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:13:51.360+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.179 seconds
[2022-12-17T07:14:01.755+0000] {processor.py:154} INFO - Started process (PID=1156) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:01.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:14:01.806+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:01.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:01.905+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:02.102+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:02.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:14:02.245+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:02.244+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:14:02.465+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.725 seconds
[2022-12-17T07:14:12.895+0000] {processor.py:154} INFO - Started process (PID=1166) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:12.938+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:14:12.942+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:12.941+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:13.031+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:13.174+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:13.173+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:14:13.289+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:13.288+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:14:13.422+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.544 seconds
[2022-12-17T07:14:24.173+0000] {processor.py:154} INFO - Started process (PID=1176) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:24.218+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:14:24.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:24.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:24.314+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:25.218+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:25.216+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:14:25.332+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:25.331+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:14:25.446+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.287 seconds
[2022-12-17T07:14:35.677+0000] {processor.py:154} INFO - Started process (PID=1194) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:35.698+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:14:35.707+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:35.706+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:35.952+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:37.094+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:37.093+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:14:37.208+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:37.207+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:14:37.380+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.720 seconds
[2022-12-17T07:14:47.791+0000] {processor.py:154} INFO - Started process (PID=1204) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:47.806+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:14:47.810+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:47.809+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:47.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:48.037+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:48.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:14:48.146+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:48.146+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:14:48.296+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.519 seconds
[2022-12-17T07:14:58.611+0000] {processor.py:154} INFO - Started process (PID=1214) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:58.634+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:14:58.638+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:14:58.638+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:58.739+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:14:59.520+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:15:09.980+0000] {processor.py:154} INFO - Started process (PID=1232) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:10.003+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:15:10.007+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:10.006+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:10.183+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:10.752+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:10.751+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:15:10.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:10.878+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:15:11.122+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.165 seconds
[2022-12-17T07:15:21.538+0000] {processor.py:154} INFO - Started process (PID=1242) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:21.549+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:15:21.564+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:21.563+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:21.772+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:21.935+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:21.934+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:15:22.057+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:22.056+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:15:22.199+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.677 seconds
[2022-12-17T07:15:32.468+0000] {processor.py:154} INFO - Started process (PID=1252) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:32.471+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:15:32.475+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:32.474+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:32.569+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:32.721+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:32.720+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:15:32.860+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:32.859+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:15:32.985+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-17T07:15:43.721+0000] {processor.py:154} INFO - Started process (PID=1262) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:43.766+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:15:43.776+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:43.771+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:43.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:44.207+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:44.206+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:15:44.319+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:44.319+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:15:44.441+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.750 seconds
[2022-12-17T07:15:54.808+0000] {processor.py:154} INFO - Started process (PID=1279) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:54.825+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:15:54.842+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:54.841+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:55.130+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:15:56.340+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:56.339+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:15:56.544+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:15:56.543+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:15:56.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.955 seconds
[2022-12-17T07:16:07.080+0000] {processor.py:154} INFO - Started process (PID=1289) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:07.102+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:16:07.107+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:07.106+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:07.188+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:07.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:07.321+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:16:07.437+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:07.436+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:16:07.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.533 seconds
[2022-12-17T07:16:17.920+0000] {processor.py:154} INFO - Started process (PID=1299) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:17.939+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:16:17.944+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:17.943+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:18.031+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:18.668+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:18.667+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:16:18.782+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:18.781+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:16:18.935+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.031 seconds
[2022-12-17T07:16:29.218+0000] {processor.py:154} INFO - Started process (PID=1316) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:29.264+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:16:29.276+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:29.274+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:29.396+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:29.590+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:29.589+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:16:29.727+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:29.726+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:16:29.954+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.752 seconds
[2022-12-17T07:16:40.184+0000] {processor.py:154} INFO - Started process (PID=1327) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:40.234+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:16:40.238+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:40.237+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:40.328+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:41.835+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:41.834+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:16:41.962+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:41.961+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:16:42.084+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.917 seconds
[2022-12-17T07:16:52.621+0000] {processor.py:154} INFO - Started process (PID=1337) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:52.677+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:16:52.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:52.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:52.767+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:16:52.910+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:52.909+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:16:53.028+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:16:53.028+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:16:53.140+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.532 seconds
[2022-12-17T07:17:03.384+0000] {processor.py:154} INFO - Started process (PID=1347) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:03.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:17:03.442+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:03.441+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:03.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:04.075+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:04.074+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:17:04.186+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:04.186+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:17:04.320+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.949 seconds
[2022-12-17T07:17:14.877+0000] {processor.py:154} INFO - Started process (PID=1365) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:14.905+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:17:14.912+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:14.911+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:15.092+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:16.229+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:16.228+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:17:16.434+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:16.434+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:17:16.646+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.796 seconds
[2022-12-17T07:17:27.066+0000] {processor.py:154} INFO - Started process (PID=1375) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:27.070+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:17:27.074+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:27.073+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:27.162+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:28.418+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:28.418+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:17:28.530+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:28.529+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:17:28.653+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.601 seconds
[2022-12-17T07:17:39.426+0000] {processor.py:154} INFO - Started process (PID=1385) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:39.447+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:17:39.451+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:39.450+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:39.537+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:39.868+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:39.867+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:17:39.980+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:39.979+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:17:40.122+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.717 seconds
[2022-12-17T07:17:50.992+0000] {processor.py:154} INFO - Started process (PID=1402) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:51.041+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:17:51.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:51.044+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:51.307+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:17:51.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:51.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:17:51.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:17:51.681+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:17:51.866+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.889 seconds
[2022-12-17T07:18:02.321+0000] {processor.py:154} INFO - Started process (PID=1413) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:02.325+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:18:02.329+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:02.328+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:02.410+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:02.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:02.549+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:18:02.678+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:02.677+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:18:02.875+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.570 seconds
[2022-12-17T07:18:13.190+0000] {processor.py:154} INFO - Started process (PID=1423) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:13.221+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:18:13.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:13.224+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:13.311+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:14.799+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:18:25.298+0000] {processor.py:154} INFO - Started process (PID=1433) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:25.357+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:18:25.381+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:25.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:25.643+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:25.801+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:25.800+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:18:25.933+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:25.932+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:18:26.235+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.958 seconds
[2022-12-17T07:18:36.770+0000] {processor.py:154} INFO - Started process (PID=1451) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:36.804+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:18:36.807+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:36.806+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:36.979+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:37.145+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:37.143+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:18:37.318+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:37.317+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:18:37.437+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.727 seconds
[2022-12-17T07:18:47.991+0000] {processor.py:154} INFO - Started process (PID=1461) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:48.021+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:18:48.025+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:48.024+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:48.114+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:48.273+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:48.272+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:18:48.396+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:48.396+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:18:48.534+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.557 seconds
[2022-12-17T07:18:58.815+0000] {processor.py:154} INFO - Started process (PID=1471) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:58.829+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:18:58.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:58.843+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:59.013+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:18:59.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:59.147+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:18:59.269+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:18:59.268+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:18:59.421+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.647 seconds
[2022-12-17T07:19:09.633+0000] {processor.py:154} INFO - Started process (PID=1481) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:09.656+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:19:09.661+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:09.660+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:09.769+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:10.624+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:10.623+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:19:10.788+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:10.787+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:19:10.945+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.326 seconds
[2022-12-17T07:19:21.597+0000] {processor.py:154} INFO - Started process (PID=1499) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:21.621+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:19:21.625+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:21.624+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:21.716+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:22.073+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:22.072+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:19:22.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:22.454+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:19:22.805+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.227 seconds
[2022-12-17T07:19:33.153+0000] {processor.py:154} INFO - Started process (PID=1509) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:33.178+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:19:33.182+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:33.181+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:33.292+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:33.490+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:33.489+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:19:33.709+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:33.708+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:19:34.441+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.304 seconds
[2022-12-17T07:19:45.491+0000] {processor.py:154} INFO - Started process (PID=1519) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:45.511+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:19:45.516+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:45.514+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:45.652+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:46.425+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:46.424+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:19:46.691+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:46.690+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:19:47.065+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.588 seconds
[2022-12-17T07:19:57.850+0000] {processor.py:154} INFO - Started process (PID=1537) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:57.912+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:19:57.937+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:57.915+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:58.418+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:19:59.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:59.044+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:19:59.856+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:19:59.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:20:00.521+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.773 seconds
[2022-12-17T07:20:10.866+0000] {processor.py:154} INFO - Started process (PID=1547) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:10.894+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:20:10.898+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:10.897+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:10.991+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:11.911+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:11.910+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:20:12.064+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:12.063+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:20:12.492+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.647 seconds
[2022-12-17T07:20:22.818+0000] {processor.py:154} INFO - Started process (PID=1557) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:22.850+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:20:22.854+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:22.853+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:22.942+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:23.077+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:23.076+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:20:23.188+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:23.187+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:20:23.325+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.520 seconds
[2022-12-17T07:20:34.261+0000] {processor.py:154} INFO - Started process (PID=1567) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:34.306+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:20:34.311+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:34.310+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:34.395+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:34.533+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:34.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:20:34.662+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:34.661+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:20:34.808+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.577 seconds
[2022-12-17T07:20:45.806+0000] {processor.py:154} INFO - Started process (PID=1585) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:45.831+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:20:45.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:45.833+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:46.176+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:47.869+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:20:58.410+0000] {processor.py:154} INFO - Started process (PID=1595) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:58.456+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:20:58.461+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:58.460+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:58.545+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:20:58.694+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:58.693+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:20:58.855+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:20:58.854+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:20:59.102+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.708 seconds
[2022-12-17T07:21:09.599+0000] {processor.py:154} INFO - Started process (PID=1605) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:09.641+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:21:09.649+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:09.648+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:09.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:09.902+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:09.901+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:21:10.026+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:10.025+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:21:10.173+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.592 seconds
[2022-12-17T07:21:20.508+0000] {processor.py:154} INFO - Started process (PID=1615) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:20.556+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:21:20.560+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:20.559+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:20.821+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:21.060+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:21.058+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:21:21.486+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:21.485+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:21:21.747+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.253 seconds
[2022-12-17T07:21:32.167+0000] {processor.py:154} INFO - Started process (PID=1632) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:32.213+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:21:32.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:32.224+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:32.409+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:32.630+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:32.628+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:21:32.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:32.869+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:21:32.998+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.865 seconds
[2022-12-17T07:21:43.126+0000] {processor.py:154} INFO - Started process (PID=1642) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:43.151+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:21:43.156+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:43.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:43.239+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:43.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:43.367+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:21:43.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:43.482+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:21:43.619+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.508 seconds
[2022-12-17T07:21:53.925+0000] {processor.py:154} INFO - Started process (PID=1652) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:53.976+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:21:53.980+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:21:53.979+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:54.085+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:21:55.353+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:22:06.204+0000] {processor.py:154} INFO - Started process (PID=1671) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:06.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:22:06.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:06.221+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:06.326+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:06.479+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:06.478+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:22:06.612+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:06.611+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:22:06.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.695 seconds
[2022-12-17T07:22:17.289+0000] {processor.py:154} INFO - Started process (PID=1681) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:17.293+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:22:17.299+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:17.297+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:17.413+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:17.603+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:17.601+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:22:17.768+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:17.767+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:22:17.957+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.684 seconds
[2022-12-17T07:22:28.424+0000] {processor.py:154} INFO - Started process (PID=1691) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:28.477+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:22:28.481+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:28.480+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:28.594+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:29.388+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:29.387+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:22:29.554+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:29.553+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:22:29.851+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.451 seconds
[2022-12-17T07:22:40.292+0000] {processor.py:154} INFO - Started process (PID=1701) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:40.322+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:22:40.326+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:40.325+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:40.459+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:41.011+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:41.010+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:22:41.168+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:41.165+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:22:41.460+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.188 seconds
[2022-12-17T07:22:52.216+0000] {processor.py:154} INFO - Started process (PID=1719) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:52.221+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:22:52.236+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:52.228+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:52.479+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:22:52.827+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:52.826+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:22:53.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:22:53.068+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:22:53.385+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.200 seconds
[2022-12-17T07:23:03.994+0000] {processor.py:154} INFO - Started process (PID=1729) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:04.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:23:04.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:04.014+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:04.198+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:04.446+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:04.445+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:23:04.582+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:04.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:23:04.703+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.724 seconds
[2022-12-17T07:23:15.696+0000] {processor.py:154} INFO - Started process (PID=1739) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:15.722+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:23:15.729+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:15.728+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:15.882+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:16.282+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:16.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:23:16.737+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:16.736+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:23:16.882+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.217 seconds
[2022-12-17T07:23:27.605+0000] {processor.py:154} INFO - Started process (PID=1749) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:27.633+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:23:27.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:27.640+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:27.833+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:28.055+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:28.053+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:23:28.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:28.208+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:23:28.349+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.820 seconds
[2022-12-17T07:23:38.826+0000] {processor.py:154} INFO - Started process (PID=1767) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:38.870+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:23:38.881+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:38.877+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:38.991+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:39.618+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:39.617+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:23:40.006+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:40.005+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:23:40.234+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-17T07:23:50.627+0000] {processor.py:154} INFO - Started process (PID=1777) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:50.670+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:23:50.674+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:50.673+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:50.786+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:23:51.178+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:51.177+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:23:51.557+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:23:51.555+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:23:51.727+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.120 seconds
[2022-12-17T07:24:02.050+0000] {processor.py:154} INFO - Started process (PID=1787) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:02.094+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:24:02.098+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:02.097+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:02.188+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:02.344+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:02.343+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:24:02.510+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:02.509+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:24:02.691+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.661 seconds
[2022-12-17T07:24:13.294+0000] {processor.py:154} INFO - Started process (PID=1804) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:13.341+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:24:13.353+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:13.353+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:13.481+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:13.688+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:13.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:24:13.843+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:13.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:24:14.042+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.763 seconds
[2022-12-17T07:24:24.426+0000] {processor.py:154} INFO - Started process (PID=1815) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:24.455+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:24:24.459+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:24.458+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:24.597+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:24.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:24.898+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:24:25.126+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:25.125+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:24:25.363+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.981 seconds
[2022-12-17T07:24:35.779+0000] {processor.py:154} INFO - Started process (PID=1825) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:35.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:24:35.806+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:35.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:35.898+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:36.174+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:36.173+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:24:36.298+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:36.298+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:24:36.407+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.642 seconds
[2022-12-17T07:24:46.681+0000] {processor.py:154} INFO - Started process (PID=1835) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:46.732+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:24:46.738+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:46.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:46.880+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:47.506+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:47.505+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:24:47.722+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:47.721+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:24:47.911+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.246 seconds
[2022-12-17T07:24:58.297+0000] {processor.py:154} INFO - Started process (PID=1853) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:58.334+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:24:58.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:58.337+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:58.649+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:24:59.042+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:59.041+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:24:59.407+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:24:59.400+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:24:59.761+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.534 seconds
[2022-12-17T07:25:10.493+0000] {processor.py:154} INFO - Started process (PID=1863) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:10.497+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:25:10.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:10.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:10.612+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:10.799+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:10.798+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:25:10.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:10.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:25:11.100+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.626 seconds
[2022-12-17T07:25:21.515+0000] {processor.py:154} INFO - Started process (PID=1873) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:21.521+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:25:21.525+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:21.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:21.673+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:21.883+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:21.882+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:25:22.079+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:22.078+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:25:22.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.820 seconds
[2022-12-17T07:25:32.618+0000] {processor.py:154} INFO - Started process (PID=1883) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:32.647+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:25:32.651+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:32.650+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:32.761+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:34.219+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:34.218+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:25:34.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:34.367+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:25:34.524+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.925 seconds
[2022-12-17T07:25:44.809+0000] {processor.py:154} INFO - Started process (PID=1901) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:44.825+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:25:44.829+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:44.828+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:45.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:45.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:45.769+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:25:46.017+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:46.017+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:25:46.283+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.512 seconds
[2022-12-17T07:25:57.648+0000] {processor.py:154} INFO - Started process (PID=1911) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:57.660+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:25:57.686+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:57.681+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:57.920+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:25:58.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:58.657+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:25:58.829+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:25:58.828+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:25:59.085+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.500 seconds
[2022-12-17T07:26:09.638+0000] {processor.py:154} INFO - Started process (PID=1921) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:09.653+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:26:09.657+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:09.656+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:09.828+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:10.199+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:10.198+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:26:10.380+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:10.378+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:26:10.592+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.977 seconds
[2022-12-17T07:26:21.079+0000] {processor.py:154} INFO - Started process (PID=1931) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:21.094+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:26:21.106+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:21.105+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:21.357+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:22.514+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:22.512+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:26:22.724+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:22.718+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:26:23.084+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.045 seconds
[2022-12-17T07:26:33.789+0000] {processor.py:154} INFO - Started process (PID=1948) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:33.805+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:26:33.853+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:33.852+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:34.246+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:34.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:34.658+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:26:34.903+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:34.902+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:26:35.207+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.491 seconds
[2022-12-17T07:26:45.688+0000] {processor.py:154} INFO - Started process (PID=1958) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:45.919+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:26:45.923+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:45.922+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:46.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:46.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:46.760+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:26:46.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:46.967+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:26:47.144+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.472 seconds
[2022-12-17T07:26:57.703+0000] {processor.py:154} INFO - Started process (PID=1968) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:57.706+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:26:57.714+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:57.713+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:57.836+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:26:58.037+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:58.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:26:58.213+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:26:58.212+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:26:58.403+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.723 seconds
[2022-12-17T07:27:08.869+0000] {processor.py:154} INFO - Started process (PID=1978) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:08.872+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:27:08.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:08.875+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:09.079+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:09.511+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:09.510+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:27:09.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:09.674+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:27:09.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.049 seconds
[2022-12-17T07:27:20.344+0000] {processor.py:154} INFO - Started process (PID=1996) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:20.356+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:27:20.360+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:20.359+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:20.593+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:21.473+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:27:32.095+0000] {processor.py:154} INFO - Started process (PID=2006) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:32.100+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:27:32.104+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:32.103+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:32.231+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:32.586+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:32.581+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:27:32.786+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:32.785+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:27:32.971+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.897 seconds
[2022-12-17T07:27:43.245+0000] {processor.py:154} INFO - Started process (PID=2016) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:43.248+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:27:43.254+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:43.252+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:43.384+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:43.629+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:43.628+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:27:43.822+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:43.821+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:27:43.976+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.771 seconds
[2022-12-17T07:27:54.397+0000] {processor.py:154} INFO - Started process (PID=2026) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:54.407+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:27:54.414+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:27:54.413+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:54.572+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:27:55.597+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:28:06.292+0000] {processor.py:154} INFO - Started process (PID=2044) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:06.306+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:28:06.310+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:06.310+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:06.614+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:06.989+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:06.988+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:28:07.291+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:07.290+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:28:07.648+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.405 seconds
[2022-12-17T07:28:18.059+0000] {processor.py:154} INFO - Started process (PID=2054) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:18.063+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:28:18.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:18.068+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:18.198+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:19.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:19.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:28:19.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:19.588+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:28:19.732+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.701 seconds
[2022-12-17T07:28:30.021+0000] {processor.py:154} INFO - Started process (PID=2064) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:30.087+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:28:30.091+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:30.090+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:30.274+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:30.601+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:30.600+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:28:30.770+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:30.769+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:28:30.979+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.004 seconds
[2022-12-17T07:28:41.389+0000] {processor.py:154} INFO - Started process (PID=2074) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:41.411+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:28:41.415+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:41.414+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:41.549+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:42.146+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:42.145+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:28:42.382+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:42.381+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:28:42.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.195 seconds
[2022-12-17T07:28:53.368+0000] {processor.py:154} INFO - Started process (PID=2092) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:53.394+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:28:53.405+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:53.404+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:53.668+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:28:54.244+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:54.242+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:28:54.465+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:28:54.463+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:28:54.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.445 seconds
[2022-12-17T07:29:05.461+0000] {processor.py:154} INFO - Started process (PID=2102) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:05.492+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:29:05.499+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:05.498+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:05.719+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:06.673+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:29:17.258+0000] {processor.py:154} INFO - Started process (PID=2112) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:17.264+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:29:17.297+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:17.267+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:17.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:17.954+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:17.953+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:29:18.183+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:18.182+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:29:18.359+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.193 seconds
[2022-12-17T07:29:28.797+0000] {processor.py:154} INFO - Started process (PID=2122) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:28.801+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:29:28.810+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:28.809+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:29.037+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:29.562+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:29.561+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:29:29.822+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:29.821+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:29:30.091+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.323 seconds
[2022-12-17T07:29:40.443+0000] {processor.py:154} INFO - Started process (PID=2140) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:40.450+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:29:40.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:40.453+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:40.689+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:41.129+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:41.110+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:29:41.573+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:41.572+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:29:42.087+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.688 seconds
[2022-12-17T07:29:52.649+0000] {processor.py:154} INFO - Started process (PID=2150) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:52.654+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:29:52.661+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:52.660+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:52.793+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:29:53.078+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:53.077+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:29:53.301+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:29:53.300+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:29:53.480+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.855 seconds
[2022-12-17T07:30:03.987+0000] {processor.py:154} INFO - Started process (PID=2160) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:03.996+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:30:04.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:04.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:04.131+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:04.400+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:04.399+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:30:04.696+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:04.695+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:30:04.927+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.972 seconds
[2022-12-17T07:30:15.457+0000] {processor.py:154} INFO - Started process (PID=2170) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:15.461+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:30:15.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:15.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:15.595+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:15.808+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:15.806+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:30:15.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:15.945+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:30:16.118+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.724 seconds
[2022-12-17T07:30:26.393+0000] {processor.py:154} INFO - Started process (PID=2187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:26.405+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:30:26.415+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:26.414+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:26.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:28.174+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:28.173+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:30:28.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:28.710+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:30:29.145+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.796 seconds
[2022-12-17T07:30:39.783+0000] {processor.py:154} INFO - Started process (PID=2197) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:39.791+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:30:39.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:39.793+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:39.988+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:40.685+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:40.682+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:30:40.842+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:40.842+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:30:41.039+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.288 seconds
[2022-12-17T07:30:51.525+0000] {processor.py:154} INFO - Started process (PID=2207) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:51.532+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:30:51.536+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:51.535+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:51.680+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:30:51.910+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:51.908+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:30:52.093+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:30:52.092+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:30:52.323+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.821 seconds
[2022-12-17T07:31:02.574+0000] {processor.py:154} INFO - Started process (PID=2217) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:02.580+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:31:02.586+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:02.585+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:02.703+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:02.940+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:02.938+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:31:03.229+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:03.228+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:31:03.665+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.136 seconds
[2022-12-17T07:31:14.459+0000] {processor.py:154} INFO - Started process (PID=2235) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:14.484+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:31:14.494+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:14.493+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:14.765+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:15.959+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:31:26.306+0000] {processor.py:154} INFO - Started process (PID=2245) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:26.309+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:31:26.313+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:26.312+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:26.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:27.044+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:27.043+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:31:27.295+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:27.294+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:31:27.488+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.197 seconds
[2022-12-17T07:31:37.886+0000] {processor.py:154} INFO - Started process (PID=2255) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:37.890+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:31:37.898+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:37.897+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:38.029+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:38.391+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:38.390+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:31:38.558+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:38.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:31:38.740+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.881 seconds
[2022-12-17T07:31:49.603+0000] {processor.py:154} INFO - Started process (PID=2270) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:49.641+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:31:49.672+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:49.665+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:49.958+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:31:50.258+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:50.257+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:31:50.522+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:31:50.521+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:31:50.794+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.279 seconds
[2022-12-17T07:32:01.662+0000] {processor.py:154} INFO - Started process (PID=2281) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:01.700+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:32:01.727+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:01.725+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:02.048+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:02.662+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:02.661+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:32:02.972+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:02.969+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:32:03.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.677 seconds
[2022-12-17T07:32:13.887+0000] {processor.py:154} INFO - Started process (PID=2291) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:13.897+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:32:13.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:13.904+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:14.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:15.248+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:15.247+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:32:15.493+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:15.492+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:32:15.742+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.922 seconds
[2022-12-17T07:32:26.298+0000] {processor.py:154} INFO - Started process (PID=2301) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:26.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:32:26.446+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:26.445+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:26.849+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:28.253+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:28.252+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:32:28.425+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:28.424+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:32:28.643+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.405 seconds
[2022-12-17T07:32:39.349+0000] {processor.py:154} INFO - Started process (PID=2318) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:39.390+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:32:39.399+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:39.398+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:39.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:40.737+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:32:51.533+0000] {processor.py:154} INFO - Started process (PID=2329) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:51.542+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:32:51.546+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:32:51.545+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:51.686+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:32:53.152+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:33:03.665+0000] {processor.py:154} INFO - Started process (PID=2339) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:03.669+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:33:03.679+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:03.677+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:03.969+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:04.284+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:04.283+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:33:04.430+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:04.429+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:33:04.645+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.008 seconds
[2022-12-17T07:33:15.074+0000] {processor.py:154} INFO - Started process (PID=2349) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:15.089+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:33:15.093+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:15.092+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:15.266+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:16.494+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:16.493+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:33:16.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:16.804+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:33:17.091+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.056 seconds
[2022-12-17T07:33:27.804+0000] {processor.py:154} INFO - Started process (PID=2367) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:27.826+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:33:27.834+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:27.833+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:28.117+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:29.008+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:29.003+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:33:29.491+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:29.490+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:33:29.944+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.218 seconds
[2022-12-17T07:33:40.555+0000] {processor.py:154} INFO - Started process (PID=2377) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:40.569+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:33:40.585+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:40.584+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:40.753+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:42.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:42.225+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:33:42.605+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:42.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:33:42.829+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.298 seconds
[2022-12-17T07:33:53.107+0000] {processor.py:154} INFO - Started process (PID=2387) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:53.117+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:33:53.129+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:53.128+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:53.493+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:33:53.707+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:53.706+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:33:53.935+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:33:53.934+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:33:54.248+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.181 seconds
[2022-12-17T07:34:05.020+0000] {processor.py:154} INFO - Started process (PID=2397) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:05.034+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:34:05.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:05.037+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:05.593+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:06.293+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:06.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:34:06.882+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:06.866+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:34:07.564+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.593 seconds
[2022-12-17T07:34:18.803+0000] {processor.py:154} INFO - Started process (PID=2416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:18.806+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:34:18.823+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:18.822+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:19.295+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:21.204+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:21.203+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:34:21.653+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:21.652+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:34:22.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.463 seconds
[2022-12-17T07:34:32.837+0000] {processor.py:154} INFO - Started process (PID=2426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:32.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:34:32.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:32.877+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:33.099+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:33.721+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:34:44.463+0000] {processor.py:154} INFO - Started process (PID=2436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:44.472+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:34:44.479+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:44.478+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:44.752+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:45.542+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:34:56.088+0000] {processor.py:154} INFO - Started process (PID=2454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:56.097+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:34:56.109+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:56.108+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:56.599+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:34:57.642+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:57.641+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:34:58.044+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:34:58.034+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:34:58.400+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.346 seconds
[2022-12-17T07:35:09.006+0000] {processor.py:154} INFO - Started process (PID=2463) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:09.028+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:35:09.045+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:09.039+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:09.553+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:09.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:09.898+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:35:10.193+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:10.190+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:35:10.484+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.551 seconds
[2022-12-17T07:35:21.310+0000] {processor.py:154} INFO - Started process (PID=2474) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:21.321+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:35:21.336+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:21.335+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:21.670+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:22.422+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:22.421+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:35:22.615+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:22.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:35:22.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.648 seconds
[2022-12-17T07:35:33.479+0000] {processor.py:154} INFO - Started process (PID=2484) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:33.487+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:35:33.496+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:33.495+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:33.681+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:35.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:35.337+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:35:35.554+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:35.550+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:35:35.997+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.559 seconds
[2022-12-17T07:35:46.815+0000] {processor.py:154} INFO - Started process (PID=2504) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:46.829+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:35:46.839+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:46.834+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:47.221+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:47.585+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:47.584+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:35:48.211+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:48.210+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:35:48.877+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.106 seconds
[2022-12-17T07:35:59.226+0000] {processor.py:154} INFO - Started process (PID=2514) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:59.230+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:35:59.238+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:35:59.237+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:35:59.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:00.858+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:36:11.399+0000] {processor.py:154} INFO - Started process (PID=2524) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:11.404+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:36:11.408+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:11.407+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:11.525+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:12.005+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:12.002+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:36:12.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:12.804+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:36:13.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.835 seconds
[2022-12-17T07:36:24.204+0000] {processor.py:154} INFO - Started process (PID=2534) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:24.259+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:36:24.271+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:24.270+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:24.497+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:25.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:25.165+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:36:25.346+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:25.345+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:36:25.578+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.451 seconds
[2022-12-17T07:36:36.159+0000] {processor.py:154} INFO - Started process (PID=2552) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:36.163+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:36:36.167+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:36.166+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:36.503+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:37.605+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:36:48.194+0000] {processor.py:154} INFO - Started process (PID=2562) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:48.199+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:36:48.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:48.208+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:48.517+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:49.086+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:36:59.741+0000] {processor.py:154} INFO - Started process (PID=2572) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:59.790+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:36:59.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:36:59.793+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:36:59.962+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:00.936+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:00.926+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:37:01.376+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:01.374+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:37:01.768+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.089 seconds
[2022-12-17T07:37:12.367+0000] {processor.py:154} INFO - Started process (PID=2589) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:12.376+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:37:12.385+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:12.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:12.736+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:13.350+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:13.349+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:37:13.912+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:13.895+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:37:14.214+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.882 seconds
[2022-12-17T07:37:25.170+0000] {processor.py:154} INFO - Started process (PID=2600) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:25.173+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:37:25.193+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:25.192+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:25.627+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:26.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:26.068+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:37:26.405+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:26.404+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:37:26.573+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.447 seconds
[2022-12-17T07:37:37.122+0000] {processor.py:154} INFO - Started process (PID=2610) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:37.127+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:37:37.131+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:37.130+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:37.316+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:39.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:39.008+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:37:39.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:39.224+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:37:39.401+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.339 seconds
[2022-12-17T07:37:49.665+0000] {processor.py:154} INFO - Started process (PID=2620) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:49.669+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:37:49.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:49.673+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:49.788+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:37:50.533+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:50.532+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:37:51.081+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:37:51.080+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:37:51.379+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.742 seconds
[2022-12-17T07:38:01.960+0000] {processor.py:154} INFO - Started process (PID=2638) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:01.963+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:38:01.976+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:01.966+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:02.202+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:02.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:02.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:38:02.784+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:02.782+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:38:03.162+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.237 seconds
[2022-12-17T07:38:13.889+0000] {processor.py:154} INFO - Started process (PID=2648) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:13.894+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:38:13.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:13.898+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:14.026+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:15.421+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:38:26.608+0000] {processor.py:154} INFO - Started process (PID=2658) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:26.653+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:38:26.661+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:26.660+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:26.892+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:27.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:27.123+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:38:27.309+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:27.309+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:38:27.490+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.906 seconds
[2022-12-17T07:38:37.814+0000] {processor.py:154} INFO - Started process (PID=2668) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:37.819+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:38:37.823+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:37.822+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:38.017+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:39.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:39.657+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:38:40.122+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:40.121+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:38:40.434+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.653 seconds
[2022-12-17T07:38:50.917+0000] {processor.py:154} INFO - Started process (PID=2686) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:50.929+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:38:50.932+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:50.931+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:51.335+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:38:51.915+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:51.906+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:38:52.535+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:38:52.506+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:38:53.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.212 seconds
[2022-12-17T07:39:03.630+0000] {processor.py:154} INFO - Started process (PID=2696) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:03.636+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:39:03.649+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:03.648+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:03.780+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:04.521+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:04.520+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:39:04.767+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:04.766+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:39:05.127+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.540 seconds
[2022-12-17T07:39:15.807+0000] {processor.py:154} INFO - Started process (PID=2706) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:15.813+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:39:15.821+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:15.820+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:16.010+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:16.765+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:16.764+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:39:17.029+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:17.028+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:39:17.384+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.602 seconds
[2022-12-17T07:39:28.076+0000] {processor.py:154} INFO - Started process (PID=2716) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:28.217+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:39:28.236+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:28.227+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:28.562+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:29.567+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:39:40.673+0000] {processor.py:154} INFO - Started process (PID=2735) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:40.681+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:39:40.700+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:40.694+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:40.934+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:41.189+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:41.187+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:39:41.647+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:41.646+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:39:42.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.934 seconds
[2022-12-17T07:39:53.100+0000] {processor.py:154} INFO - Started process (PID=2745) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:53.105+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:39:53.113+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:53.112+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:53.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:39:53.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:53.934+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:39:54.380+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:39:54.370+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:39:54.808+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.733 seconds
[2022-12-17T07:40:05.293+0000] {processor.py:154} INFO - Started process (PID=2755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:05.304+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:40:05.307+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:05.306+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:05.433+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:05.790+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:05.789+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:40:06.263+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:06.262+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:40:06.744+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.484 seconds
[2022-12-17T07:40:17.223+0000] {processor.py:154} INFO - Started process (PID=2771) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:17.226+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:40:17.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:17.238+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:17.469+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:18.914+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:18.913+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:40:19.303+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:19.302+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:40:19.697+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.523 seconds
[2022-12-17T07:40:30.426+0000] {processor.py:154} INFO - Started process (PID=2782) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:30.447+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:40:30.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:30.454+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:30.777+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:31.223+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:31.222+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:40:31.733+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:31.732+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:40:32.325+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.937 seconds
[2022-12-17T07:40:42.744+0000] {processor.py:154} INFO - Started process (PID=2792) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:42.858+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:40:42.863+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:42.861+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:43.300+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:43.996+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:43.994+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:40:44.121+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:44.120+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:40:44.259+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.592 seconds
[2022-12-17T07:40:55.217+0000] {processor.py:154} INFO - Started process (PID=2802) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:55.288+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:40:55.304+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:55.291+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:55.630+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:40:55.911+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:55.909+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:40:56.240+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:40:56.235+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:40:56.528+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.379 seconds
[2022-12-17T07:41:07.217+0000] {processor.py:154} INFO - Started process (PID=2820) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:07.229+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:41:07.237+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:07.236+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:07.573+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:08.507+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:08.506+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:41:08.742+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:08.741+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:41:09.025+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.853 seconds
[2022-12-17T07:41:19.320+0000] {processor.py:154} INFO - Started process (PID=2830) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:19.365+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:41:19.369+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:19.368+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:19.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:20.587+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:20.580+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:41:20.851+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:20.850+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:41:21.074+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.777 seconds
[2022-12-17T07:41:31.364+0000] {processor.py:154} INFO - Started process (PID=2840) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:31.369+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:41:31.373+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:31.372+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:31.505+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:32.107+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:32.106+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:41:32.345+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:32.344+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:41:32.739+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.394 seconds
[2022-12-17T07:41:43.106+0000] {processor.py:154} INFO - Started process (PID=2850) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:43.110+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:41:43.117+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:43.116+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:43.251+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:44.248+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:44.233+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:41:44.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:44.801+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:41:45.393+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.322 seconds
[2022-12-17T07:41:56.094+0000] {processor.py:154} INFO - Started process (PID=2871) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:56.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:41:56.119+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:56.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:56.440+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:41:57.602+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:57.601+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:41:58.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:41:58.182+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:41:58.929+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.910 seconds
[2022-12-17T07:42:09.953+0000] {processor.py:154} INFO - Started process (PID=2881) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:09.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:42:09.967+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:09.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:10.203+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:10.540+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:10.539+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:42:10.932+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:10.931+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:42:11.260+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.367 seconds
[2022-12-17T07:42:21.620+0000] {processor.py:154} INFO - Started process (PID=2891) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:21.627+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:42:21.632+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:21.630+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:21.857+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:22.184+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:22.177+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:42:22.504+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:22.502+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:42:22.872+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.280 seconds
[2022-12-17T07:42:33.318+0000] {processor.py:154} INFO - Started process (PID=2904) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:33.348+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:42:33.356+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:33.350+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:33.553+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:34.084+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:34.083+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:42:34.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:34.400+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:42:34.784+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.496 seconds
[2022-12-17T07:42:45.721+0000] {processor.py:154} INFO - Started process (PID=2919) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:45.743+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:42:45.760+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:45.746+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:46.103+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:47.064+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:47.063+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:42:47.676+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:47.675+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:42:48.050+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.401 seconds
[2022-12-17T07:42:58.508+0000] {processor.py:154} INFO - Started process (PID=2929) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:58.529+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:42:58.533+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:58.532+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:58.785+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:42:59.608+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:59.603+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:42:59.858+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:42:59.856+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:43:00.033+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.552 seconds
[2022-12-17T07:43:10.395+0000] {processor.py:154} INFO - Started process (PID=2939) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:10.398+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:43:10.402+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:10.401+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:10.556+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:10.861+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:10.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:43:11.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:11.136+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:43:11.446+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.099 seconds
[2022-12-17T07:43:21.892+0000] {processor.py:154} INFO - Started process (PID=2955) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:21.900+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:43:21.904+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:21.903+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:22.086+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:22.463+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:22.462+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:43:22.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:22.921+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:43:23.282+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.406 seconds
[2022-12-17T07:43:33.727+0000] {processor.py:154} INFO - Started process (PID=2966) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:33.744+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:43:33.764+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:33.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:33.938+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:34.891+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:43:45.289+0000] {processor.py:154} INFO - Started process (PID=2973) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:45.312+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:43:45.319+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:45.317+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:45.419+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:45.608+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:45.607+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:43:45.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:45.802+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:43:46.150+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.877 seconds
[2022-12-17T07:43:56.808+0000] {processor.py:154} INFO - Started process (PID=2983) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:56.865+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:43:56.869+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:56.868+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:57.279+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:43:57.627+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:57.626+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:43:57.948+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:43:57.947+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:43:58.241+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.476 seconds
[2022-12-17T07:44:08.918+0000] {processor.py:154} INFO - Started process (PID=3001) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:08.965+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:44:08.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:08.968+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:09.316+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:09.866+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:09.865+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:44:10.334+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:10.331+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:44:10.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.075 seconds
[2022-12-17T07:44:21.502+0000] {processor.py:154} INFO - Started process (PID=3014) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:21.529+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:44:21.541+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:21.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:21.702+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:22.424+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:22.423+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:44:22.561+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:22.559+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:44:22.880+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.393 seconds
[2022-12-17T07:44:33.298+0000] {processor.py:154} INFO - Started process (PID=3021) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:33.329+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:44:33.333+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:33.332+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:33.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:34.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:34.225+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:44:34.543+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:34.542+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:44:34.793+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.521 seconds
[2022-12-17T07:44:45.261+0000] {processor.py:154} INFO - Started process (PID=3034) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:45.312+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:44:45.324+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:45.315+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:45.517+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:46.202+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:44:57.040+0000] {processor.py:154} INFO - Started process (PID=3051) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:57.076+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:44:57.108+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:57.079+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:57.527+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:44:59.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:59.013+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:44:59.403+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:44:59.402+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:44:59.818+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.884 seconds
[2022-12-17T07:45:10.386+0000] {processor.py:154} INFO - Started process (PID=3062) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:10.418+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:45:10.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:10.425+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:10.554+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:11.241+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:11.240+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:45:11.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:11.383+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:45:11.652+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.310 seconds
[2022-12-17T07:45:22.050+0000] {processor.py:154} INFO - Started process (PID=3072) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:22.055+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:45:22.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:22.061+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:22.384+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:23.120+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:23.119+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:45:23.266+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:23.265+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:45:23.446+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.429 seconds
[2022-12-17T07:45:34.114+0000] {processor.py:154} INFO - Started process (PID=3089) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:34.158+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:45:34.172+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:34.171+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:34.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:34.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:34.730+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:45:35.047+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:35.046+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:45:35.540+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.485 seconds
[2022-12-17T07:45:46.184+0000] {processor.py:154} INFO - Started process (PID=3100) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:46.237+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:45:46.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:46.242+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:46.446+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:46.720+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:46.719+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:45:46.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:46.993+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:45:47.176+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.015 seconds
[2022-12-17T07:45:57.464+0000] {processor.py:154} INFO - Started process (PID=3109) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:57.474+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:45:57.478+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:57.477+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:57.610+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:45:57.859+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:57.858+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:45:58.095+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:45:58.094+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:45:59.327+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.876 seconds
[2022-12-17T07:46:09.947+0000] {processor.py:154} INFO - Started process (PID=3120) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:09.973+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:46:09.983+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:09.978+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:10.229+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:10.729+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:10.727+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:46:11.090+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:11.086+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:46:11.356+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.434 seconds
[2022-12-17T07:46:21.906+0000] {processor.py:154} INFO - Started process (PID=3138) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:21.929+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:46:21.941+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:21.940+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:22.117+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:22.413+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:22.412+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:46:22.756+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:22.754+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:46:23.184+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.298 seconds
[2022-12-17T07:46:33.772+0000] {processor.py:154} INFO - Started process (PID=3148) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:33.779+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:46:33.783+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:33.782+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:34.106+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:34.365+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:34.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:46:34.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:34.536+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:46:34.821+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.088 seconds
[2022-12-17T07:46:45.243+0000] {processor.py:154} INFO - Started process (PID=3155) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:45.264+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:46:45.269+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:45.268+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:45.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:45.525+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:45.524+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:46:45.664+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:45.663+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:46:45.886+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.665 seconds
[2022-12-17T07:46:56.146+0000] {processor.py:154} INFO - Started process (PID=3165) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:56.152+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:46:56.158+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:56.156+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:56.251+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:46:56.595+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:56.594+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:46:56.720+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:46:56.719+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:46:56.847+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.723 seconds
[2022-12-17T07:47:07.621+0000] {processor.py:154} INFO - Started process (PID=3183) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:07.632+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:47:07.636+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:07.635+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:07.880+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:08.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:08.425+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:47:08.693+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:08.692+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:47:09.022+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.440 seconds
[2022-12-17T07:47:19.400+0000] {processor.py:154} INFO - Started process (PID=3193) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:19.426+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:47:19.430+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:19.429+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:19.535+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:20.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:20.124+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:47:20.241+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:20.240+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:47:20.391+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.005 seconds
[2022-12-17T07:47:30.844+0000] {processor.py:154} INFO - Started process (PID=3203) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:30.848+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:47:30.857+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:30.856+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:31.020+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:32.194+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:32.193+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:47:32.437+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:32.437+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:47:32.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.924 seconds
[2022-12-17T07:47:43.114+0000] {processor.py:154} INFO - Started process (PID=3213) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:43.144+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:47:43.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:43.147+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:43.265+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:43.503+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:43.502+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:47:43.800+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:43.790+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:47:44.401+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.308 seconds
[2022-12-17T07:47:55.287+0000] {processor.py:154} INFO - Started process (PID=3231) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:55.322+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:47:55.328+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:55.327+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:55.466+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:47:55.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:55.877+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:47:56.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:47:56.242+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:47:56.513+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.273 seconds
[2022-12-17T07:48:07.086+0000] {processor.py:154} INFO - Started process (PID=3241) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:07.133+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:48:07.141+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:07.140+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:07.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:07.583+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:07.582+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:48:07.819+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:07.818+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:48:08.075+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.017 seconds
[2022-12-17T07:48:18.452+0000] {processor.py:154} INFO - Started process (PID=3251) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:18.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:48:18.517+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:18.516+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:18.627+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:18.814+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:18.813+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:48:18.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:18.968+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:48:19.190+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.757 seconds
[2022-12-17T07:48:29.716+0000] {processor.py:154} INFO - Started process (PID=3267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:29.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:48:29.795+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:29.794+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:29.955+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:30.520+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:30.513+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:48:30.897+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:30.885+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:48:31.131+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.476 seconds
[2022-12-17T07:48:41.968+0000] {processor.py:154} INFO - Started process (PID=3278) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:42.025+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:48:42.029+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:42.028+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:42.151+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:42.370+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:42.369+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:48:42.672+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:42.671+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:48:42.892+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.022 seconds
[2022-12-17T07:48:53.347+0000] {processor.py:154} INFO - Started process (PID=3288) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:53.405+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:48:53.410+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:53.408+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:53.686+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:48:53.925+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:53.924+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:48:54.072+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:48:54.071+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:48:54.312+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.990 seconds
[2022-12-17T07:49:04.656+0000] {processor.py:154} INFO - Started process (PID=3298) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:04.707+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:49:04.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:04.710+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:04.853+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:05.514+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:05.513+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:49:06.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:06.013+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:49:07.086+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.442 seconds
[2022-12-17T07:49:17.413+0000] {processor.py:154} INFO - Started process (PID=3316) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:17.429+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:49:17.436+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:17.434+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:17.637+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:17.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:17.993+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:49:18.218+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:18.217+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:49:19.992+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.594 seconds
[2022-12-17T07:49:30.887+0000] {processor.py:154} INFO - Started process (PID=3326) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:30.931+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:49:30.935+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:30.934+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:31.080+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:31.778+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:31.777+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:49:31.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:31.998+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:49:32.297+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-17T07:49:43.516+0000] {processor.py:154} INFO - Started process (PID=3339) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:43.550+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:49:43.554+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:43.553+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:43.692+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:44.383+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:44.381+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:49:44.569+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:44.568+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:49:44.811+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.349 seconds
[2022-12-17T07:49:55.299+0000] {processor.py:154} INFO - Started process (PID=3349) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:55.316+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:49:55.320+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:55.319+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:55.498+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:49:55.693+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:55.692+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:49:55.847+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:49:55.845+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:49:56.016+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.749 seconds
[2022-12-17T07:50:06.642+0000] {processor.py:154} INFO - Started process (PID=3368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:06.651+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:50:06.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:06.658+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:06.839+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:07.181+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:07.180+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:50:07.418+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:07.417+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:50:07.563+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.971 seconds
[2022-12-17T07:50:17.908+0000] {processor.py:154} INFO - Started process (PID=3375) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:17.939+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:50:17.943+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:17.942+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:18.083+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:18.250+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:18.249+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:50:18.380+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:18.379+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:50:18.533+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.679 seconds
[2022-12-17T07:50:28.928+0000] {processor.py:154} INFO - Started process (PID=3385) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:28.930+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:50:28.934+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:28.933+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:29.027+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:29.725+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:50:40.679+0000] {processor.py:154} INFO - Started process (PID=3402) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:40.744+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:50:40.765+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:40.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:41.202+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:41.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:41.746+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:50:42.109+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:42.108+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:50:42.501+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.909 seconds
[2022-12-17T07:50:53.088+0000] {processor.py:154} INFO - Started process (PID=3413) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:53.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:50:53.128+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:53.122+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:53.303+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:50:53.678+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:53.677+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:50:53.943+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:50:53.942+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:50:54.220+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.173 seconds
[2022-12-17T07:51:04.614+0000] {processor.py:154} INFO - Started process (PID=3426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:04.643+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:51:04.647+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:04.646+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:04.768+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:06.054+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:06.053+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:51:06.276+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:06.266+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:51:06.612+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.018 seconds
[2022-12-17T07:51:17.051+0000] {processor.py:154} INFO - Started process (PID=3438) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:17.056+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:51:17.060+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:17.059+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:17.191+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:17.393+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:17.392+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:51:17.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:17.526+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:51:17.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.669 seconds
[2022-12-17T07:51:28.310+0000] {processor.py:154} INFO - Started process (PID=3454) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:28.332+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:51:28.336+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:28.335+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:28.687+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:29.620+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:29.619+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:51:29.936+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:29.936+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:51:30.148+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.875 seconds
[2022-12-17T07:51:40.582+0000] {processor.py:154} INFO - Started process (PID=3466) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:40.654+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:51:40.674+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:40.673+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:40.901+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:41.081+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:41.080+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:51:41.248+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:41.247+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:51:41.443+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.898 seconds
[2022-12-17T07:51:51.949+0000] {processor.py:154} INFO - Started process (PID=3474) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:51.955+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:51:51.972+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:51.971+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:52.120+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:51:52.361+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:52.360+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:51:52.516+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:51:52.515+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:51:52.711+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.819 seconds
[2022-12-17T07:52:03.104+0000] {processor.py:154} INFO - Started process (PID=3484) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:03.107+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:52:03.115+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:03.114+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:03.292+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:03.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:03.985+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:52:04.115+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:04.114+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:52:04.333+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.248 seconds
[2022-12-17T07:52:15.177+0000] {processor.py:154} INFO - Started process (PID=3503) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:15.189+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:52:15.198+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:15.197+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:15.634+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:16.864+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:52:27.398+0000] {processor.py:154} INFO - Started process (PID=3513) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:27.421+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:52:27.425+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:27.424+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:27.601+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:29.156+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.2), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T07:52:39.375+0000] {processor.py:154} INFO - Started process (PID=3523) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:39.407+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:52:39.420+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:39.415+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:39.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:40.277+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:40.276+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:52:40.395+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:40.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:52:40.526+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.180 seconds
[2022-12-17T07:52:50.984+0000] {processor.py:154} INFO - Started process (PID=3541) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:50.988+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:52:51.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:51.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:51.223+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:52:51.488+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:51.486+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:52:51.649+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:52:51.648+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:52:51.881+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.944 seconds
[2022-12-17T07:53:02.745+0000] {processor.py:154} INFO - Started process (PID=3551) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:53:02.820+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T07:53:02.825+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:53:02.824+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:53:03.403+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T07:53:03.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:53:03.657+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T07:53:03.808+0000] {logging_mixin.py:137} INFO - [2022-12-17T07:53:03.807+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T07:53:03.937+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.266 seconds
[2022-12-17T11:59:01.789+0000] {processor.py:154} INFO - Started process (PID=203) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:01.963+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T11:59:01.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:01.966+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:04.269+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:04.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:04.872+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T11:59:05.193+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:05.192+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T11:59:05.530+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.762 seconds
[2022-12-17T11:59:15.809+0000] {processor.py:154} INFO - Started process (PID=214) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:15.813+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T11:59:15.817+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:15.816+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:15.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:17.261+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:17.260+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T11:59:17.487+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:17.486+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T11:59:17.858+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.084 seconds
[2022-12-17T11:59:28.281+0000] {processor.py:154} INFO - Started process (PID=223) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:28.284+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T11:59:28.288+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:28.287+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:28.425+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:28.615+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:28.614+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T11:59:28.727+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:28.726+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T11:59:28.894+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.640 seconds
[2022-12-17T11:59:39.104+0000] {processor.py:154} INFO - Started process (PID=239) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:39.157+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T11:59:39.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:39.164+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:39.311+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:40.282+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:40.281+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T11:59:40.570+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:40.569+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T11:59:40.855+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.793 seconds
[2022-12-17T11:59:51.093+0000] {processor.py:154} INFO - Started process (PID=249) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:51.097+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T11:59:51.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:51.100+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:51.192+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T11:59:51.355+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:51.354+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T11:59:51.605+0000] {logging_mixin.py:137} INFO - [2022-12-17T11:59:51.604+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T11:59:51.871+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.792 seconds
[2022-12-17T12:00:02.777+0000] {processor.py:154} INFO - Started process (PID=259) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:02.779+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:00:02.783+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:02.782+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:02.996+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:04.373+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:04.372+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:00:04.594+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:04.593+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:00:04.834+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.072 seconds
[2022-12-17T12:00:15.233+0000] {processor.py:154} INFO - Started process (PID=277) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:15.280+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:00:15.287+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:15.286+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:15.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:15.877+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:15.876+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:00:16.058+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:16.057+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:00:16.328+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.117 seconds
[2022-12-17T12:00:26.813+0000] {processor.py:154} INFO - Started process (PID=287) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:26.820+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:00:26.839+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:26.825+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:26.984+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:27.504+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:27.503+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:00:27.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:27.659+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:00:27.800+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.023 seconds
[2022-12-17T12:00:38.200+0000] {processor.py:154} INFO - Started process (PID=297) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:38.212+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:00:38.220+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:38.219+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:38.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:38.631+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:38.630+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:00:38.781+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:38.780+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:00:39.007+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.839 seconds
[2022-12-17T12:00:49.280+0000] {processor.py:154} INFO - Started process (PID=307) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:49.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:00:49.303+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:49.302+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:49.411+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:00:49.783+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:49.782+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:00:50.095+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:00:50.094+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:00:50.334+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.071 seconds
[2022-12-17T12:01:00.730+0000] {processor.py:154} INFO - Started process (PID=324) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:00.752+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:01:00.756+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:00.755+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:01.028+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:01.289+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:01.288+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:01:01.825+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:01.814+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:01:02.160+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.456 seconds
[2022-12-17T12:01:12.520+0000] {processor.py:154} INFO - Started process (PID=334) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:12.590+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:01:12.599+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:12.597+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:12.778+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:13.106+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:01:23.506+0000] {processor.py:154} INFO - Started process (PID=344) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:23.541+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:01:23.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:23.548+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:23.648+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:23.795+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:23.794+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:01:23.920+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:23.919+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:01:24.048+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.587 seconds
[2022-12-17T12:01:34.380+0000] {processor.py:154} INFO - Started process (PID=354) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:34.405+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:01:34.409+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:34.408+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:34.503+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:35.049+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:35.048+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:01:35.177+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:35.176+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:01:35.400+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.035 seconds
[2022-12-17T12:01:46.146+0000] {processor.py:154} INFO - Started process (PID=372) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:46.183+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:01:46.199+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:46.186+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:46.572+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:47.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:47.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:01:47.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:47.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:01:47.912+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.833 seconds
[2022-12-17T12:01:58.419+0000] {processor.py:154} INFO - Started process (PID=382) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:58.438+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:01:58.445+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:58.444+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:58.712+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:01:59.293+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:59.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:01:59.722+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:01:59.721+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:02:00.154+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.769 seconds
[2022-12-17T12:02:10.607+0000] {processor.py:154} INFO - Started process (PID=392) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:10.636+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:02:10.640+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:10.639+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:10.726+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:10.886+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:10.885+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:02:11.000+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:10.999+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:02:11.138+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.548 seconds
[2022-12-17T12:02:21.371+0000] {processor.py:154} INFO - Started process (PID=402) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:21.398+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:02:21.403+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:21.402+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:21.482+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:21.703+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:02:32.645+0000] {processor.py:154} INFO - Started process (PID=419) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:32.667+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:02:32.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:32.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:32.799+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:33.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:33.017+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:02:33.141+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:33.140+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:02:33.293+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.667 seconds
[2022-12-17T12:02:43.669+0000] {processor.py:154} INFO - Started process (PID=430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:43.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:02:43.708+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:43.707+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:43.851+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:44.465+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:44.464+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:02:44.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:44.579+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:02:44.702+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.067 seconds
[2022-12-17T12:02:55.024+0000] {processor.py:154} INFO - Started process (PID=440) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:55.072+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:02:55.076+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:55.075+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:55.232+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:02:55.891+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:55.890+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:02:56.001+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:02:56.000+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:02:56.137+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.133 seconds
[2022-12-17T12:03:06.543+0000] {processor.py:154} INFO - Started process (PID=450) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:06.577+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:03:06.581+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:06.580+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:06.711+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:06.860+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:06.859+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:03:06.978+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:06.977+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:03:07.147+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.645 seconds
[2022-12-17T12:03:17.326+0000] {processor.py:154} INFO - Started process (PID=468) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:17.355+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:03:17.359+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:17.358+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:17.454+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:17.608+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:17.607+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:03:18.109+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:18.108+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:03:18.544+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.232 seconds
[2022-12-17T12:03:28.854+0000] {processor.py:154} INFO - Started process (PID=478) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:28.879+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:03:28.884+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:28.883+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:28.965+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:29.227+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:29.226+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:03:29.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:29.337+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:03:29.512+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.671 seconds
[2022-12-17T12:03:39.841+0000] {processor.py:154} INFO - Started process (PID=488) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:39.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:03:39.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:39.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:39.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:40.175+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:40.174+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:03:40.625+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:40.624+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:03:40.844+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.031 seconds
[2022-12-17T12:03:51.328+0000] {processor.py:154} INFO - Started process (PID=498) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:51.348+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:03:51.362+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:51.361+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:51.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:03:53.037+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:53.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:03:53.217+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:03:53.216+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:03:53.445+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.155 seconds
[2022-12-17T12:04:04.218+0000] {processor.py:154} INFO - Started process (PID=517) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:04.299+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:04:04.323+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:04.302+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:04.742+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:05.657+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:05.656+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:04:06.102+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:06.101+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:04:06.848+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.662 seconds
[2022-12-17T12:04:17.628+0000] {processor.py:154} INFO - Started process (PID=527) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:17.674+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:04:17.681+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:17.680+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:17.854+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:18.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:18.304+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:04:18.564+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:18.564+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:04:18.853+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.280 seconds
[2022-12-17T12:04:29.209+0000] {processor.py:154} INFO - Started process (PID=537) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:29.224+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:04:29.228+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:29.227+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:29.447+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:30.537+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:30.536+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:04:31.084+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:31.070+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:04:31.599+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.427 seconds
[2022-12-17T12:04:42.427+0000] {processor.py:154} INFO - Started process (PID=547) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:42.432+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:04:42.448+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:42.442+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:42.671+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:43.078+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:43.077+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:04:43.409+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:43.408+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:04:44.056+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.687 seconds
[2022-12-17T12:04:54.691+0000] {processor.py:154} INFO - Started process (PID=564) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:54.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:04:54.800+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:54.790+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:55.264+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:04:55.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:55.588+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:04:55.884+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:04:55.883+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:04:56.104+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.536 seconds
[2022-12-17T12:05:06.819+0000] {processor.py:154} INFO - Started process (PID=574) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:06.825+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:05:06.828+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:06.827+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:06.912+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:07.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:07.060+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:05:07.175+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:07.174+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:05:07.328+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.525 seconds
[2022-12-17T12:05:18.060+0000] {processor.py:154} INFO - Started process (PID=584) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:18.065+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:05:18.069+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:18.068+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:18.160+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:18.335+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:18.335+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:05:18.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:18.453+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:05:18.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.534 seconds
[2022-12-17T12:05:29.158+0000] {processor.py:154} INFO - Started process (PID=594) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:29.235+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:05:29.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:29.263+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:29.480+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:31.135+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:05:42.213+0000] {processor.py:154} INFO - Started process (PID=612) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:42.257+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:05:42.273+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:42.272+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:42.821+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:43.979+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:43.952+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:05:44.531+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:44.526+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:05:44.828+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.673 seconds
[2022-12-17T12:05:55.924+0000] {processor.py:154} INFO - Started process (PID=623) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:55.968+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:05:56.010+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:56.009+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:56.932+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:05:57.575+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:57.562+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:05:58.022+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:05:58.021+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:05:58.900+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.227 seconds
[2022-12-17T12:06:10.719+0000] {processor.py:154} INFO - Started process (PID=631) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:10.754+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:06:10.762+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:10.761+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:11.853+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:12.795+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:12.762+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:06:13.632+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:13.631+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:06:14.331+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.801 seconds
[2022-12-17T12:06:25.009+0000] {processor.py:154} INFO - Started process (PID=643) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:25.029+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:06:25.046+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:25.045+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:25.404+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:26.121+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:26.120+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:06:26.256+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:26.256+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:06:26.421+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.487 seconds
[2022-12-17T12:06:37.110+0000] {processor.py:154} INFO - Started process (PID=660) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:37.121+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:06:37.131+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:37.130+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:37.457+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:38.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:38.208+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:06:38.577+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:38.576+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:06:39.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.144 seconds
[2022-12-17T12:06:49.750+0000] {processor.py:154} INFO - Started process (PID=671) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:49.779+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:06:49.783+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:49.782+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:49.879+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:06:50.067+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:50.065+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:06:50.349+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:06:50.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:06:50.603+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.869 seconds
[2022-12-17T12:07:00.997+0000] {processor.py:154} INFO - Started process (PID=681) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:01.000+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:07:01.017+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:01.016+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:01.172+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:02.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:02.480+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:07:02.642+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:02.641+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:07:02.853+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.889 seconds
[2022-12-17T12:07:13.544+0000] {processor.py:154} INFO - Started process (PID=691) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:13.552+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:07:13.556+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:13.555+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:13.680+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:14.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:14.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:07:14.560+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:14.560+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:07:14.769+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.253 seconds
[2022-12-17T12:07:25.428+0000] {processor.py:154} INFO - Started process (PID=708) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:25.479+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:07:25.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:25.481+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:25.682+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:26.241+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:26.240+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:07:26.616+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:26.616+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:07:26.840+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.455 seconds
[2022-12-17T12:07:37.125+0000] {processor.py:154} INFO - Started process (PID=718) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:37.137+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:07:37.141+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:37.140+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:37.367+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:37.691+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:37.689+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:07:37.998+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:37.997+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:07:38.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.194 seconds
[2022-12-17T12:07:48.583+0000] {processor.py:154} INFO - Started process (PID=728) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:48.587+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:07:48.594+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:48.593+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:48.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:48.930+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:48.929+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:07:49.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:49.100+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:07:49.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.763 seconds
[2022-12-17T12:07:59.770+0000] {processor.py:154} INFO - Started process (PID=738) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:59.795+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:07:59.800+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:07:59.798+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:07:59.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:00.441+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:00.439+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:08:00.568+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:00.568+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:08:00.716+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.975 seconds
[2022-12-17T12:08:11.228+0000] {processor.py:154} INFO - Started process (PID=753) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:11.231+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:08:11.234+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:11.233+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:11.516+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:11.872+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:11.871+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:08:12.091+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:12.090+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:08:12.269+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.068 seconds
[2022-12-17T12:08:22.517+0000] {processor.py:154} INFO - Started process (PID=763) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:22.538+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:08:22.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:22.541+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:22.661+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:22.837+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:22.837+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:08:22.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:22.964+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:08:23.112+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.615 seconds
[2022-12-17T12:08:33.394+0000] {processor.py:154} INFO - Started process (PID=774) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:33.448+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:08:33.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:33.451+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:33.546+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:33.695+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:33.693+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:08:33.806+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:33.805+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:08:33.947+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.567 seconds
[2022-12-17T12:08:44.265+0000] {processor.py:154} INFO - Started process (PID=781) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:44.296+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:08:44.304+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:44.303+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:44.419+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:45.374+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:45.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:08:45.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:45.500+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:08:45.629+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.385 seconds
[2022-12-17T12:08:55.911+0000] {processor.py:154} INFO - Started process (PID=799) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:55.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:08:55.949+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:55.948+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:56.045+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:08:56.196+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:56.195+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:08:56.317+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:08:56.316+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:08:56.490+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.594 seconds
[2022-12-17T12:09:07.023+0000] {processor.py:154} INFO - Started process (PID=809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:07.051+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:09:07.068+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:07.054+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:07.211+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:07.365+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:07.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:09:07.545+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:07.544+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:09:07.666+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.699 seconds
[2022-12-17T12:09:18.038+0000] {processor.py:154} INFO - Started process (PID=819) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:18.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:09:18.199+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:18.198+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:18.315+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:18.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:18.466+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:09:18.606+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:18.605+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:09:18.725+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.736 seconds
[2022-12-17T12:09:28.981+0000] {processor.py:154} INFO - Started process (PID=829) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:29.003+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:09:29.008+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:29.007+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:29.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:29.238+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:29.237+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:09:29.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:29.453+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:09:29.650+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.682 seconds
[2022-12-17T12:09:39.790+0000] {processor.py:154} INFO - Started process (PID=848) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:39.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:09:39.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:39.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:40.012+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:40.184+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:40.183+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:09:40.317+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:40.316+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:09:40.490+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.719 seconds
[2022-12-17T12:09:51.284+0000] {processor.py:154} INFO - Started process (PID=858) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:51.312+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:09:51.317+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:51.316+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:51.400+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:09:51.528+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:51.527+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:09:51.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:09:51.640+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:09:51.810+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.539 seconds
[2022-12-17T12:10:02.062+0000] {processor.py:154} INFO - Started process (PID=868) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:02.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:10:02.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:02.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:02.204+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:02.345+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:02.344+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:10:02.463+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:02.462+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:10:02.571+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.522 seconds
[2022-12-17T12:10:13.021+0000] {processor.py:154} INFO - Started process (PID=885) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:13.075+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:10:13.092+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:13.078+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:13.288+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:14.624+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:14.617+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:10:14.820+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:14.819+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:10:15.137+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.157 seconds
[2022-12-17T12:10:25.640+0000] {processor.py:154} INFO - Started process (PID=895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:25.662+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:10:25.667+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:25.666+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:25.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:26.112+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:26.111+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:10:26.253+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:26.252+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:10:26.431+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.805 seconds
[2022-12-17T12:10:36.757+0000] {processor.py:154} INFO - Started process (PID=905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:36.761+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:10:36.765+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:36.764+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:36.851+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:36.980+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:36.979+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:10:37.094+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:37.094+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:10:37.233+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.491 seconds
[2022-12-17T12:10:47.511+0000] {processor.py:154} INFO - Started process (PID=915) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:47.531+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:10:47.536+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:47.535+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:47.616+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:47.908+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:47.908+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:10:48.023+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:48.022+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:10:48.158+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.660 seconds
[2022-12-17T12:10:58.477+0000] {processor.py:154} INFO - Started process (PID=933) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:58.481+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:10:58.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:58.483+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:58.578+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:10:59.262+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:59.261+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:10:59.386+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:10:59.385+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:10:59.583+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.123 seconds
[2022-12-17T12:11:09.720+0000] {processor.py:154} INFO - Started process (PID=943) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:09.826+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:11:09.830+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:09.829+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:09.926+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:10.293+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:10.292+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:11:10.402+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:10.401+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:11:10.532+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.825 seconds
[2022-12-17T12:11:20.819+0000] {processor.py:154} INFO - Started process (PID=953) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:20.863+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:11:20.867+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:20.866+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:20.947+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:21.376+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:21.375+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:11:21.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:21.481+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:11:21.635+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.829 seconds
[2022-12-17T12:11:32.016+0000] {processor.py:154} INFO - Started process (PID=969) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:32.020+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:11:32.032+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:32.031+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:32.155+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:32.324+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:32.322+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:11:32.454+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:32.453+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:11:32.608+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.630 seconds
[2022-12-17T12:11:42.813+0000] {processor.py:154} INFO - Started process (PID=981) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:42.816+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:11:42.820+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:42.819+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:42.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:43.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:43.029+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:11:43.138+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:43.137+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:11:43.276+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.477 seconds
[2022-12-17T12:11:53.558+0000] {processor.py:154} INFO - Started process (PID=991) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:53.585+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:11:53.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:53.588+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:53.672+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:11:53.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:53.801+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:11:53.929+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:11:53.928+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:11:54.096+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.553 seconds
[2022-12-17T12:12:04.470+0000] {processor.py:154} INFO - Started process (PID=1001) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:04.488+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:12:04.495+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:04.494+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:04.596+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:04.756+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:04.752+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:12:04.870+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:04.869+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:12:04.974+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.545 seconds
[2022-12-17T12:12:15.445+0000] {processor.py:154} INFO - Started process (PID=1018) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:15.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:12:15.471+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:15.462+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:15.655+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:16.757+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:16.756+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:12:17.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:17.003+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:12:17.214+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.841 seconds
[2022-12-17T12:12:27.477+0000] {processor.py:154} INFO - Started process (PID=1028) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:27.525+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:12:27.529+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:27.528+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:27.609+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:27.817+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:27.816+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:12:27.924+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:27.923+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:12:28.067+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.603 seconds
[2022-12-17T12:12:38.364+0000] {processor.py:154} INFO - Started process (PID=1038) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:38.407+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:12:38.412+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:38.411+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:38.494+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:38.668+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:38.667+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:12:38.779+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:38.778+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:12:38.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.541 seconds
[2022-12-17T12:12:49.162+0000] {processor.py:154} INFO - Started process (PID=1048) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:49.178+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:12:49.182+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:49.181+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:49.273+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:12:49.925+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:49.924+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:12:50.036+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:12:50.035+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:12:50.173+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.026 seconds
[2022-12-17T12:13:00.446+0000] {processor.py:154} INFO - Started process (PID=1066) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:00.460+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:13:00.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:00.463+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:00.580+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:00.720+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:00.719+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:13:00.840+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:00.839+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:13:00.989+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.575 seconds
[2022-12-17T12:13:11.309+0000] {processor.py:154} INFO - Started process (PID=1076) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:11.312+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:13:11.316+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:11.316+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:11.405+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:11.647+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:11.646+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:13:11.777+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:11.777+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:13:11.891+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.597 seconds
[2022-12-17T12:13:22.186+0000] {processor.py:154} INFO - Started process (PID=1086) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:22.244+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:13:22.248+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:22.247+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:22.328+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:23.732+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:13:34.049+0000] {processor.py:154} INFO - Started process (PID=1096) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:34.103+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:13:34.107+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:34.106+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:34.185+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:34.689+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:34.688+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:13:34.803+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:34.802+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:13:34.959+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.924 seconds
[2022-12-17T12:13:45.623+0000] {processor.py:154} INFO - Started process (PID=1114) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:45.678+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:13:45.691+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:45.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:45.877+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:46.723+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:46.718+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:13:46.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:46.993+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:13:47.152+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.556 seconds
[2022-12-17T12:13:57.423+0000] {processor.py:154} INFO - Started process (PID=1124) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:57.457+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:13:57.462+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:57.460+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:57.556+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:13:58.080+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:58.079+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:13:58.194+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:13:58.193+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:13:58.330+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.921 seconds
[2022-12-17T12:14:08.673+0000] {processor.py:154} INFO - Started process (PID=1134) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:08.727+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:14:08.731+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:08.730+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:08.813+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:09.144+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:09.143+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:14:09.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:09.263+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:14:09.401+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.741 seconds
[2022-12-17T12:14:19.822+0000] {processor.py:154} INFO - Started process (PID=1151) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:19.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:14:19.878+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:19.877+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:19.973+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:20.199+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:20.197+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:14:20.342+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:20.341+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:14:20.446+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.640 seconds
[2022-12-17T12:14:30.812+0000] {processor.py:154} INFO - Started process (PID=1161) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:30.855+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:14:30.861+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:30.858+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:30.950+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:31.086+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:31.085+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:14:31.198+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:31.197+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:14:31.375+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.582 seconds
[2022-12-17T12:14:41.638+0000] {processor.py:154} INFO - Started process (PID=1171) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:41.659+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:14:41.663+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:41.662+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:41.747+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:42.627+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:42.626+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:14:42.750+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:42.749+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:14:42.852+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.228 seconds
[2022-12-17T12:14:53.231+0000] {processor.py:154} INFO - Started process (PID=1181) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:53.268+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:14:53.271+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:53.271+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:53.356+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:14:53.524+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:53.523+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:14:53.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:14:53.664+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:14:53.798+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.582 seconds
[2022-12-17T12:15:04.673+0000] {processor.py:154} INFO - Started process (PID=1198) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:04.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:15:04.715+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:04.710+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:04.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:05.503+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:05.502+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:15:05.631+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:05.631+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:15:05.780+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.172 seconds
[2022-12-17T12:15:16.168+0000] {processor.py:154} INFO - Started process (PID=1208) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:16.193+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:15:16.197+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:16.196+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:16.285+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:16.450+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:16.449+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:15:16.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:16.640+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:15:16.816+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-17T12:15:27.168+0000] {processor.py:154} INFO - Started process (PID=1218) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:27.192+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:15:27.196+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:27.195+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:27.419+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:27.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:27.548+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:15:27.668+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:27.667+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:15:27.788+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.652 seconds
[2022-12-17T12:15:38.196+0000] {processor.py:154} INFO - Started process (PID=1236) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:38.222+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:15:38.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:38.225+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:38.451+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:39.018+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:39.017+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:15:39.194+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:39.193+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:15:39.323+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.142 seconds
[2022-12-17T12:15:49.789+0000] {processor.py:154} INFO - Started process (PID=1247) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:49.813+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:15:49.817+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:49.816+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:49.908+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:15:50.493+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:50.492+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:15:50.603+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:15:50.602+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:15:50.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.118 seconds
[2022-12-17T12:16:01.225+0000] {processor.py:154} INFO - Started process (PID=1257) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:01.271+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:16:01.275+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:01.274+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:01.356+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:01.489+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:01.488+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:16:01.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:01.597+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:16:01.751+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.541 seconds
[2022-12-17T12:16:12.442+0000] {processor.py:154} INFO - Started process (PID=1267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:12.462+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:16:12.467+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:12.466+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:12.556+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:12.961+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:12.960+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:16:13.072+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:13.071+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:16:13.206+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.777 seconds
[2022-12-17T12:16:23.921+0000] {processor.py:154} INFO - Started process (PID=1284) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:23.963+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:16:23.979+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:23.966+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:24.179+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:24.636+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:24.635+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:16:24.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:24.760+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:16:24.930+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.039 seconds
[2022-12-17T12:16:35.213+0000] {processor.py:154} INFO - Started process (PID=1294) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:35.255+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:16:35.260+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:35.259+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:35.341+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:35.470+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:35.469+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:16:35.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:35.579+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:16:35.722+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.525 seconds
[2022-12-17T12:16:46.371+0000] {processor.py:154} INFO - Started process (PID=1304) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:46.413+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:16:46.417+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:46.416+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:46.497+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:47.470+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:16:57.787+0000] {processor.py:154} INFO - Started process (PID=1314) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:57.827+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:16:57.831+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:57.830+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:57.917+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:16:58.096+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:58.090+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:16:58.354+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:16:58.353+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:16:58.498+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.730 seconds
[2022-12-17T12:17:08.929+0000] {processor.py:154} INFO - Started process (PID=1332) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:08.985+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:17:08.988+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:08.988+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:09.074+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:09.232+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:09.231+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:17:09.348+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:09.347+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:17:09.484+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.585 seconds
[2022-12-17T12:17:19.753+0000] {processor.py:154} INFO - Started process (PID=1342) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:19.778+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:17:19.783+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:19.782+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:19.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:20.508+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:20.507+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:17:20.624+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:20.623+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:17:20.743+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.004 seconds
[2022-12-17T12:17:30.984+0000] {processor.py:154} INFO - Started process (PID=1352) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:31.025+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:17:31.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:31.028+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:31.108+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:31.240+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:31.239+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:17:31.349+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:31.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:17:31.480+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.512 seconds
[2022-12-17T12:17:41.805+0000] {processor.py:154} INFO - Started process (PID=1370) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:41.836+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:17:41.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:41.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:41.950+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:42.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:42.457+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:17:42.581+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:42.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:17:42.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.939 seconds
[2022-12-17T12:17:53.338+0000] {processor.py:154} INFO - Started process (PID=1381) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:53.361+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:17:53.365+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:17:53.364+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:53.444+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:17:54.826+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:18:05.148+0000] {processor.py:154} INFO - Started process (PID=1391) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:05.195+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:18:05.199+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:05.198+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:05.282+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:05.410+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:05.409+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:18:05.519+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:05.518+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:18:05.666+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.532 seconds
[2022-12-17T12:18:15.942+0000] {processor.py:154} INFO - Started process (PID=1401) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:15.999+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:18:16.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:16.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:16.082+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:16.600+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:18:27.150+0000] {processor.py:154} INFO - Started process (PID=1418) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:27.179+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:18:27.183+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:27.182+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:27.426+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:27.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:27.587+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:18:27.717+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:27.717+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:18:28.068+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.944 seconds
[2022-12-17T12:18:38.440+0000] {processor.py:154} INFO - Started process (PID=1428) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:38.459+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:18:38.463+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:38.462+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:38.553+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:39.005+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:39.004+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:18:39.278+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:39.277+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:18:39.468+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.043 seconds
[2022-12-17T12:18:49.727+0000] {processor.py:154} INFO - Started process (PID=1438) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:49.751+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:18:49.754+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:49.753+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:49.869+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:18:50.820+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:50.819+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:18:51.006+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:18:51.005+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:18:51.178+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.466 seconds
[2022-12-17T12:19:01.436+0000] {processor.py:154} INFO - Started process (PID=1455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:01.496+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:19:01.500+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:01.499+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:01.623+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:02.125+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:02.124+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:19:02.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:02.500+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:19:02.712+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.291 seconds
[2022-12-17T12:19:12.935+0000] {processor.py:154} INFO - Started process (PID=1466) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:12.987+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:19:12.992+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:12.991+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:13.089+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:13.232+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:13.231+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:19:13.341+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:13.341+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:19:13.446+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.525 seconds
[2022-12-17T12:19:23.667+0000] {processor.py:154} INFO - Started process (PID=1476) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:23.703+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:19:23.707+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:23.706+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:23.786+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:23.913+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:23.912+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:19:24.025+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:24.024+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:19:24.161+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.508 seconds
[2022-12-17T12:19:34.388+0000] {processor.py:154} INFO - Started process (PID=1486) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:34.391+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:19:34.395+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:34.394+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:34.477+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:34.626+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:34.625+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:19:34.753+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:34.752+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:19:34.900+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.527 seconds
[2022-12-17T12:19:45.268+0000] {processor.py:154} INFO - Started process (PID=1505) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:45.324+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:19:45.342+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:45.341+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:45.591+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:45.750+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:45.749+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:19:45.871+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:45.870+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:19:46.016+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.783 seconds
[2022-12-17T12:19:56.401+0000] {processor.py:154} INFO - Started process (PID=1515) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:56.416+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:19:56.419+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:56.418+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:56.503+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:19:56.659+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:56.658+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:19:56.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:19:56.873+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:19:56.982+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.598 seconds
[2022-12-17T12:20:07.092+0000] {processor.py:154} INFO - Started process (PID=1525) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:07.120+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:20:07.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:07.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:07.204+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:07.333+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:07.332+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:20:07.447+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:07.446+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:20:07.579+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.501 seconds
[2022-12-17T12:20:17.812+0000] {processor.py:154} INFO - Started process (PID=1535) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:17.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:20:17.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:17.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:17.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:18.428+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:18.427+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:20:18.561+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:18.560+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:20:19.174+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.376 seconds
[2022-12-17T12:20:29.451+0000] {processor.py:154} INFO - Started process (PID=1555) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:29.648+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:20:29.664+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:29.650+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:29.892+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:31.201+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:31.200+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:20:31.335+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:31.334+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:20:31.477+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.083 seconds
[2022-12-17T12:20:41.769+0000] {processor.py:154} INFO - Started process (PID=1565) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:41.791+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:20:41.795+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:41.794+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:41.883+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:42.049+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:42.048+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:20:42.213+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:42.212+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:20:42.313+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.558 seconds
[2022-12-17T12:20:52.608+0000] {processor.py:154} INFO - Started process (PID=1575) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:52.651+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:20:52.654+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:52.653+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:52.755+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:20:52.921+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:52.920+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:20:53.058+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:20:53.057+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:20:53.173+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.588 seconds
[2022-12-17T12:21:03.402+0000] {processor.py:154} INFO - Started process (PID=1592) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:03.422+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:21:03.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:03.425+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:03.547+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:04.123+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:21:14.834+0000] {processor.py:154} INFO - Started process (PID=1603) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:14.857+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:21:14.865+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:14.865+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:14.980+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:15.229+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:15.228+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:21:15.353+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:15.352+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:21:15.472+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.674 seconds
[2022-12-17T12:21:25.765+0000] {processor.py:154} INFO - Started process (PID=1613) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:25.794+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:21:25.798+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:25.797+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:25.889+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:26.184+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:26.182+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:21:26.372+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:26.371+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:21:26.568+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.816 seconds
[2022-12-17T12:21:36.845+0000] {processor.py:154} INFO - Started process (PID=1623) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:36.875+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:21:36.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:36.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:36.975+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:37.759+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:37.758+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:21:37.941+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:37.940+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:21:38.153+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.322 seconds
[2022-12-17T12:21:48.548+0000] {processor.py:154} INFO - Started process (PID=1641) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:48.580+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:21:48.588+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:48.582+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:48.793+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:21:49.638+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:49.637+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:21:49.950+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:21:49.949+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:21:50.137+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.603 seconds
[2022-12-17T12:22:00.490+0000] {processor.py:154} INFO - Started process (PID=1651) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:00.513+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:22:00.519+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:00.517+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:00.606+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:00.975+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:00.974+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:22:01.096+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:01.095+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:22:01.256+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.778 seconds
[2022-12-17T12:22:11.701+0000] {processor.py:154} INFO - Started process (PID=1661) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:11.749+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:22:11.785+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:11.784+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:11.866+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:13.173+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:13.172+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:22:13.288+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:13.287+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:22:13.432+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.745 seconds
[2022-12-17T12:22:23.892+0000] {processor.py:154} INFO - Started process (PID=1677) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:24.047+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:22:24.059+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:24.050+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:24.200+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:25.626+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:25.625+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:22:25.775+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:25.774+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:22:25.947+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.077 seconds
[2022-12-17T12:22:36.297+0000] {processor.py:154} INFO - Started process (PID=1689) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:36.346+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:22:36.351+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:36.350+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:36.430+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:36.558+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:36.557+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:22:36.669+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:36.668+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:22:36.834+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.551 seconds
[2022-12-17T12:22:47.122+0000] {processor.py:154} INFO - Started process (PID=1699) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:47.172+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:22:47.180+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:47.179+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:47.265+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:47.481+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:47.480+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:22:47.697+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:47.696+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:22:47.893+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.840 seconds
[2022-12-17T12:22:58.658+0000] {processor.py:154} INFO - Started process (PID=1709) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:58.692+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:22:58.695+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:58.694+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:58.853+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:22:59.009+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:59.007+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:22:59.129+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:22:59.128+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:22:59.251+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.614 seconds
[2022-12-17T12:23:09.935+0000] {processor.py:154} INFO - Started process (PID=1727) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:09.998+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:23:10.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:10.001+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:10.205+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:10.662+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:10.661+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:23:10.826+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:10.825+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:23:11.047+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.149 seconds
[2022-12-17T12:23:21.463+0000] {processor.py:154} INFO - Started process (PID=1737) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:21.500+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:23:21.504+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:21.503+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:21.611+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:22.660+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:23:32.944+0000] {processor.py:154} INFO - Started process (PID=1747) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:32.969+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:23:32.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:32.972+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:33.051+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:33.183+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:33.182+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:23:33.296+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:33.295+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:23:33.455+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.525 seconds
[2022-12-17T12:23:43.768+0000] {processor.py:154} INFO - Started process (PID=1757) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:43.771+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:23:43.775+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:43.774+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:43.855+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:43.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:43.986+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:23:44.100+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:44.099+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:23:44.241+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.487 seconds
[2022-12-17T12:23:54.569+0000] {processor.py:154} INFO - Started process (PID=1775) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:54.599+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:23:54.610+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:54.609+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:54.779+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:23:56.308+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:56.307+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:23:56.432+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:23:56.432+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:23:56.538+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.989 seconds
[2022-12-17T12:24:06.952+0000] {processor.py:154} INFO - Started process (PID=1785) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:06.975+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:24:06.978+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:06.977+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:07.065+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:07.385+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:07.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:24:07.495+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:07.494+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:24:07.647+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.707 seconds
[2022-12-17T12:24:18.244+0000] {processor.py:154} INFO - Started process (PID=1795) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:18.255+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:24:18.260+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:18.259+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:18.358+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:19.220+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:24:29.622+0000] {processor.py:154} INFO - Started process (PID=1813) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:29.646+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:24:29.654+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:29.653+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:29.751+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:30.295+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:30.294+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:24:30.432+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:30.431+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:24:30.547+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.941 seconds
[2022-12-17T12:24:40.892+0000] {processor.py:154} INFO - Started process (PID=1823) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:40.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:24:40.964+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:40.963+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:41.072+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:41.226+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:41.225+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:24:41.337+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:41.336+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:24:41.436+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.555 seconds
[2022-12-17T12:24:52.137+0000] {processor.py:154} INFO - Started process (PID=1833) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:52.185+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:24:52.190+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:52.189+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:52.288+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:24:52.429+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:52.428+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:24:52.539+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:24:52.538+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:24:52.639+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.515 seconds
[2022-12-17T12:25:02.876+0000] {processor.py:154} INFO - Started process (PID=1843) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:02.901+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:25:02.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:02.904+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:02.987+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:04.234+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:04.233+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:25:04.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:04.383+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:25:04.494+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.632 seconds
[2022-12-17T12:25:14.628+0000] {processor.py:154} INFO - Started process (PID=1860) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:14.684+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:25:14.692+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:14.691+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:14.790+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:15.064+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:15.063+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:25:15.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:15.224+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:25:15.357+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.744 seconds
[2022-12-17T12:25:25.892+0000] {processor.py:154} INFO - Started process (PID=1870) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:25.937+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:25:25.940+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:25.939+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:26.025+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:26.171+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:26.171+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:25:26.354+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:26.353+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:25:26.540+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-17T12:25:36.812+0000] {processor.py:154} INFO - Started process (PID=1880) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:36.841+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:25:36.844+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:36.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:36.923+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:37.352+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:37.351+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:25:37.465+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:37.464+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:25:37.571+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.775 seconds
[2022-12-17T12:25:47.917+0000] {processor.py:154} INFO - Started process (PID=1897) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:48.010+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:25:48.234+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:48.233+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:48.348+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:48.679+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:48.672+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:25:49.023+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:49.022+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:25:49.264+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.378 seconds
[2022-12-17T12:25:59.702+0000] {processor.py:154} INFO - Started process (PID=1908) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:59.728+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:25:59.735+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:59.735+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:59.826+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:25:59.956+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:25:59.955+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:26:00.072+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:00.071+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:26:00.203+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.539 seconds
[2022-12-17T12:26:10.497+0000] {processor.py:154} INFO - Started process (PID=1918) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:10.538+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:26:10.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:10.541+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:10.626+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:11.154+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:11.153+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:26:11.267+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:11.266+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:26:11.422+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.942 seconds
[2022-12-17T12:26:21.681+0000] {processor.py:154} INFO - Started process (PID=1928) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:21.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:26:21.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:21.711+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:21.793+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:21.919+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:21.918+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:26:22.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:22.029+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:26:22.185+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.521 seconds
[2022-12-17T12:26:32.616+0000] {processor.py:154} INFO - Started process (PID=1945) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:32.667+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:26:32.704+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:32.690+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:32.861+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:33.766+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:26:44.202+0000] {processor.py:154} INFO - Started process (PID=1955) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:44.246+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:26:44.251+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:44.250+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:44.331+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:44.489+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:44.488+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:26:44.737+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:44.736+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:26:44.884+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.696 seconds
[2022-12-17T12:26:55.095+0000] {processor.py:154} INFO - Started process (PID=1965) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:55.128+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:26:55.132+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:55.131+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:55.215+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:26:55.370+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:55.369+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:26:55.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:26:55.500+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:26:55.652+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.576 seconds
[2022-12-17T12:27:05.812+0000] {processor.py:154} INFO - Started process (PID=1975) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:05.847+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:27:05.855+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:05.850+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:05.939+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:06.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:06.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:27:06.245+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:06.244+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:27:06.366+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.568 seconds
[2022-12-17T12:27:16.893+0000] {processor.py:154} INFO - Started process (PID=1993) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:16.924+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:27:16.933+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:16.932+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:17.071+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:17.374+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:17.373+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:27:17.597+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:17.597+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:27:17.777+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.907 seconds
[2022-12-17T12:27:28.389+0000] {processor.py:154} INFO - Started process (PID=2003) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:28.437+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:27:28.442+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:28.441+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:28.520+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:28.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:28.659+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:27:28.781+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:28.780+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:27:28.903+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.528 seconds
[2022-12-17T12:27:39.149+0000] {processor.py:154} INFO - Started process (PID=2013) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:39.175+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:27:39.192+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:39.191+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:39.307+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:39.676+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:39.675+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:27:39.781+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:39.780+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:27:39.897+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.764 seconds
[2022-12-17T12:27:50.246+0000] {processor.py:154} INFO - Started process (PID=2029) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:50.289+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:27:50.311+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:50.297+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:50.509+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:27:50.755+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:50.754+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:27:51.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:27:51.001+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:27:51.216+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.010 seconds
[2022-12-17T12:28:01.780+0000] {processor.py:154} INFO - Started process (PID=2040) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:01.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:28:01.806+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:01.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:01.894+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:02.065+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:02.064+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:28:02.190+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:02.189+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:28:02.326+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.560 seconds
[2022-12-17T12:28:12.568+0000] {processor.py:154} INFO - Started process (PID=2050) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:12.617+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:28:12.621+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:12.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:12.733+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:13.098+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:13.097+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:28:13.228+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:13.227+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:28:13.360+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.807 seconds
[2022-12-17T12:28:23.629+0000] {processor.py:154} INFO - Started process (PID=2060) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:23.676+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:28:23.680+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:23.678+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:23.770+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:23.920+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:23.920+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:28:24.031+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:24.029+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:28:24.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.551 seconds
[2022-12-17T12:28:34.646+0000] {processor.py:154} INFO - Started process (PID=2078) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:34.680+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:28:34.683+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:34.682+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:34.813+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:35.105+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:35.104+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:28:35.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:35.365+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:28:35.612+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.980 seconds
[2022-12-17T12:28:45.924+0000] {processor.py:154} INFO - Started process (PID=2088) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:46.026+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:28:46.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:46.029+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:46.117+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:46.485+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:46.484+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:28:46.609+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:46.609+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:28:46.729+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.819 seconds
[2022-12-17T12:28:57.612+0000] {processor.py:154} INFO - Started process (PID=2098) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:57.638+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:28:57.642+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:57.641+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:57.725+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:28:57.872+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:57.871+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:28:58.001+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:28:58.000+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:28:58.136+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.537 seconds
[2022-12-17T12:29:03.299+0000] {processor.py:154} INFO - Started process (PID=2110) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:03.303+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:29:03.307+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:03.306+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:03.423+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:03.568+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:03.567+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:29:03.681+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:03.680+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:29:03.840+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.556 seconds
[2022-12-17T12:29:14.221+0000] {processor.py:154} INFO - Started process (PID=2126) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:14.266+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:29:14.278+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:14.277+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:14.550+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:14.867+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:14.866+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:29:14.995+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:14.994+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:29:15.157+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.008 seconds
[2022-12-17T12:29:25.593+0000] {processor.py:154} INFO - Started process (PID=2136) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:25.666+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:29:25.670+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:25.669+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:25.909+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:26.083+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:26.082+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:29:26.191+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:26.190+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:29:26.301+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.766 seconds
[2022-12-17T12:29:36.556+0000] {processor.py:154} INFO - Started process (PID=2146) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:36.581+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:29:36.584+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:36.584+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:36.670+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:37.453+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:37.446+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:29:37.731+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:37.729+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:29:37.967+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.424 seconds
[2022-12-17T12:29:48.108+0000] {processor.py:154} INFO - Started process (PID=2156) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:48.160+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:29:48.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:48.164+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:48.247+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:48.431+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:48.430+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:29:48.819+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:48.818+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:29:48.941+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.848 seconds
[2022-12-17T12:29:59.562+0000] {processor.py:154} INFO - Started process (PID=2174) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:59.607+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:29:59.612+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:29:59.610+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:29:59.696+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:00.932+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:30:11.975+0000] {processor.py:154} INFO - Started process (PID=2187) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:11.994+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:30:11.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:11.997+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:12.144+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:12.331+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:12.330+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:30:12.451+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:12.450+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:30:12.590+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.626 seconds
[2022-12-17T12:30:23.009+0000] {processor.py:154} INFO - Started process (PID=2197) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:23.032+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:30:23.036+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:23.035+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:23.163+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:24.098+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:24.097+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:30:24.250+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:24.249+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:30:24.525+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.531 seconds
[2022-12-17T12:30:35.327+0000] {processor.py:154} INFO - Started process (PID=2216) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:35.351+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:30:35.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:35.354+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:35.464+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:35.989+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:35.988+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:30:36.138+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:36.137+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:30:36.404+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.179 seconds
[2022-12-17T12:30:46.583+0000] {processor.py:154} INFO - Started process (PID=2226) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:46.631+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:30:46.635+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:46.634+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:46.750+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:46.886+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:46.885+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:30:47.013+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:47.012+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:30:47.146+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.578 seconds
[2022-12-17T12:30:57.973+0000] {processor.py:154} INFO - Started process (PID=2234) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:57.989+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:30:57.993+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:57.992+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:58.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:30:58.937+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:58.936+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:30:59.066+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:30:59.065+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:30:59.248+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.299 seconds
[2022-12-17T12:31:09.703+0000] {processor.py:154} INFO - Started process (PID=2246) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:09.734+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:31:09.738+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:09.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:10.121+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:10.801+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:31:21.038+0000] {processor.py:154} INFO - Started process (PID=2266) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:21.082+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:31:21.089+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:21.085+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:21.261+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:21.473+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:21.472+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:31:21.606+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:21.605+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:31:21.764+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.740 seconds
[2022-12-17T12:31:31.926+0000] {processor.py:154} INFO - Started process (PID=2276) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:31.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:31:31.966+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:31.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:32.073+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:32.230+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:32.229+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:31:32.359+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:32.358+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:31:32.476+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.566 seconds
[2022-12-17T12:31:42.760+0000] {processor.py:154} INFO - Started process (PID=2286) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:42.780+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:31:42.783+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:42.783+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:42.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:44.275+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:31:54.597+0000] {processor.py:154} INFO - Started process (PID=2302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:54.613+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:31:54.620+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:54.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:54.715+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:31:54.864+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:54.863+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:31:55.005+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:31:55.004+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:31:55.216+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.637 seconds
[2022-12-17T12:32:05.625+0000] {processor.py:154} INFO - Started process (PID=2312) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:05.663+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:32:05.667+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:05.666+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:05.836+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:06.171+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:06.170+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:32:06.291+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:06.290+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:32:06.424+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.853 seconds
[2022-12-17T12:32:16.573+0000] {processor.py:154} INFO - Started process (PID=2322) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:16.620+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:32:16.625+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:16.624+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:16.842+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:17.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:17.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:32:17.346+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:17.345+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:32:17.471+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.923 seconds
[2022-12-17T12:32:27.731+0000] {processor.py:154} INFO - Started process (PID=2332) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:27.757+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:32:27.761+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:27.760+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:27.840+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:27.972+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:27.971+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:32:28.160+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:28.160+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:32:28.471+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.757 seconds
[2022-12-17T12:32:38.900+0000] {processor.py:154} INFO - Started process (PID=2350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:38.926+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:32:38.930+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:38.929+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:39.016+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:40.010+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:32:50.511+0000] {processor.py:154} INFO - Started process (PID=2360) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:50.539+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:32:50.543+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:50.542+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:50.623+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:32:50.791+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:50.790+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:32:50.932+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:32:50.931+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:32:51.378+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.881 seconds
[2022-12-17T12:33:01.628+0000] {processor.py:154} INFO - Started process (PID=2370) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:01.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:33:01.675+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:01.674+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:01.766+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:01.928+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:01.927+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:33:02.066+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:02.064+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:33:02.292+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.683 seconds
[2022-12-17T12:33:12.710+0000] {processor.py:154} INFO - Started process (PID=2391) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:12.734+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:33:12.738+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:12.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:12.943+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:13.133+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:13.132+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:33:13.250+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:13.249+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:33:13.414+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.729 seconds
[2022-12-17T12:33:23.788+0000] {processor.py:154} INFO - Started process (PID=2398) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:23.837+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:33:23.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:23.844+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:23.947+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:24.162+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:24.161+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:33:24.269+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:24.268+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:33:24.413+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.641 seconds
[2022-12-17T12:33:34.663+0000] {processor.py:154} INFO - Started process (PID=2408) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:34.708+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:33:34.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:34.711+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:34.790+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:34.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:34.921+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:33:35.050+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:35.049+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:33:35.289+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.642 seconds
[2022-12-17T12:33:46.209+0000] {processor.py:154} INFO - Started process (PID=2418) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:46.242+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:33:46.246+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:46.245+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:46.325+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:46.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:46.451+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:33:46.569+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:46.568+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:33:46.710+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.515 seconds
[2022-12-17T12:33:57.009+0000] {processor.py:154} INFO - Started process (PID=2436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:57.036+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:33:57.040+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:33:57.039+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:57.150+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:33:57.744+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:34:08.028+0000] {processor.py:154} INFO - Started process (PID=2446) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:08.074+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:34:08.078+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:08.077+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:08.164+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:08.789+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:08.788+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:34:08.943+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:08.942+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:34:09.081+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.067 seconds
[2022-12-17T12:34:19.357+0000] {processor.py:154} INFO - Started process (PID=2456) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:19.377+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:34:19.381+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:19.380+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:19.461+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:19.596+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:19.596+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:34:19.729+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:19.728+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:34:19.864+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.520 seconds
[2022-12-17T12:34:30.051+0000] {processor.py:154} INFO - Started process (PID=2466) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:30.100+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:34:30.105+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:30.104+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:30.200+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:30.866+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:30.864+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:34:31.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:31.003+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:34:31.136+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.098 seconds
[2022-12-17T12:34:41.750+0000] {processor.py:154} INFO - Started process (PID=2484) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:41.778+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:34:41.782+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:41.781+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:41.873+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:42.033+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:42.032+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:34:42.155+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:42.154+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:34:42.290+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.560 seconds
[2022-12-17T12:34:53.055+0000] {processor.py:154} INFO - Started process (PID=2494) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:53.070+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:34:53.075+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:53.074+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:53.162+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:34:53.890+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:53.889+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:34:54.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:34:54.033+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:34:54.136+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.094 seconds
[2022-12-17T12:35:04.436+0000] {processor.py:154} INFO - Started process (PID=2504) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:04.450+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:35:04.455+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:04.454+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:04.552+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:04.690+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:04.689+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:35:04.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:04.804+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:35:04.922+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.502 seconds
[2022-12-17T12:35:15.217+0000] {processor.py:154} INFO - Started process (PID=2522) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:15.245+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:35:15.250+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:15.249+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:15.524+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:15.739+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:15.738+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:35:15.863+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:15.862+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:35:16.019+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.847 seconds
[2022-12-17T12:35:26.160+0000] {processor.py:154} INFO - Started process (PID=2532) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:26.204+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:35:26.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:26.208+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:26.291+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:26.420+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:26.420+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:35:26.540+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:26.539+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:35:26.684+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.544 seconds
[2022-12-17T12:35:37.353+0000] {processor.py:154} INFO - Started process (PID=2542) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:37.373+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:35:37.377+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:37.376+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:37.457+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:37.750+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:37.749+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:35:37.858+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:37.857+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:35:37.998+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.662 seconds
[2022-12-17T12:35:48.260+0000] {processor.py:154} INFO - Started process (PID=2552) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:48.288+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:35:48.293+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:48.292+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:48.376+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:48.504+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:48.503+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:35:48.614+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:48.613+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:35:48.746+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.502 seconds
[2022-12-17T12:35:59.143+0000] {processor.py:154} INFO - Started process (PID=2571) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:59.171+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:35:59.175+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:59.174+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:59.270+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:35:59.449+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:59.448+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:35:59.578+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:35:59.577+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:35:59.719+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.591 seconds
[2022-12-17T12:36:10.069+0000] {processor.py:154} INFO - Started process (PID=2581) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:10.085+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:36:10.090+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:10.089+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:10.172+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:10.820+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:10.819+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:36:10.993+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:10.992+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:36:11.149+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.094 seconds
[2022-12-17T12:36:21.412+0000] {processor.py:154} INFO - Started process (PID=2591) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:21.465+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:36:21.473+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:21.472+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:21.552+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:21.682+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:21.681+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:36:21.792+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:21.791+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:36:21.910+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.512 seconds
[2022-12-17T12:36:32.067+0000] {processor.py:154} INFO - Started process (PID=2601) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:32.092+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:36:32.097+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:32.096+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:32.176+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:32.972+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:36:43.402+0000] {processor.py:154} INFO - Started process (PID=2619) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:43.428+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:36:43.432+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:43.431+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:43.522+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:43.677+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:43.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:36:43.795+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:43.795+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:36:43.937+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.549 seconds
[2022-12-17T12:36:54.181+0000] {processor.py:154} INFO - Started process (PID=2629) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:54.210+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:36:54.214+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:54.213+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:54.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:36:54.450+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:54.449+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:36:54.613+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:36:54.612+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:36:54.733+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.566 seconds
[2022-12-17T12:37:04.856+0000] {processor.py:154} INFO - Started process (PID=2639) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:04.903+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:37:04.911+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:04.906+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:05.234+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:06.382+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:37:16.872+0000] {processor.py:154} INFO - Started process (PID=2656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:17.024+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:37:17.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:17.037+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:17.240+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:17.561+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:17.561+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:37:17.756+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:17.755+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:37:17.923+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.098 seconds
[2022-12-17T12:37:28.306+0000] {processor.py:154} INFO - Started process (PID=2666) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:28.564+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:37:28.568+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:28.567+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:28.675+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:29.640+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:37:39.897+0000] {processor.py:154} INFO - Started process (PID=2676) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:39.949+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:37:39.954+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:39.953+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:40.166+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:40.846+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:40.845+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:37:40.984+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:40.983+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:37:41.092+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.246 seconds
[2022-12-17T12:37:52.096+0000] {processor.py:154} INFO - Started process (PID=2686) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:52.115+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:37:52.119+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:52.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:52.223+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:37:52.388+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:52.387+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:37:52.496+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:37:52.495+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:37:52.602+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.519 seconds
[2022-12-17T12:38:03.632+0000] {processor.py:154} INFO - Started process (PID=2704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:03.750+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:38:03.753+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:03.753+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:03.893+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:04.066+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:04.065+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:38:04.176+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:04.175+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:38:04.478+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.912 seconds
[2022-12-17T12:38:14.722+0000] {processor.py:154} INFO - Started process (PID=2714) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:14.776+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:38:14.781+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:14.780+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:14.862+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:15.623+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:15.622+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:38:15.785+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:15.784+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:38:15.912+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.203 seconds
[2022-12-17T12:38:26.165+0000] {processor.py:154} INFO - Started process (PID=2724) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:26.188+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:38:26.192+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:26.191+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:26.279+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:27.800+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:27.799+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:38:27.908+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:27.907+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:38:28.043+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.893 seconds
[2022-12-17T12:38:38.835+0000] {processor.py:154} INFO - Started process (PID=2742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:38.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:38:38.871+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:38.871+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:39.101+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:39.321+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:39.320+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:38:39.533+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:39.532+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:38:39.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.827 seconds
[2022-12-17T12:38:49.952+0000] {processor.py:154} INFO - Started process (PID=2752) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:50.000+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:38:50.004+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:50.003+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:50.167+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:38:50.395+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:50.394+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:38:50.525+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:38:50.524+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:38:50.720+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.781 seconds
[2022-12-17T12:39:01.762+0000] {processor.py:154} INFO - Started process (PID=2762) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:01.789+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:39:01.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:01.793+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:01.896+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:02.963+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:02.962+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:39:03.078+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:03.077+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:39:03.212+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.475 seconds
[2022-12-17T12:39:13.590+0000] {processor.py:154} INFO - Started process (PID=2772) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:13.693+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:39:13.696+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:13.695+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:14.025+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:14.278+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:14.277+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:39:14.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:14.425+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:39:14.582+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.015 seconds
[2022-12-17T12:39:24.824+0000] {processor.py:154} INFO - Started process (PID=2789) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:24.864+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:39:24.881+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:24.880+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:25.040+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:26.511+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:26.510+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:39:26.620+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:26.619+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:39:26.767+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.963 seconds
[2022-12-17T12:39:37.010+0000] {processor.py:154} INFO - Started process (PID=2799) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:37.052+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:39:37.060+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:37.054+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:37.281+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:37.439+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:37.438+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:39:37.550+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:37.550+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:39:37.660+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.697 seconds
[2022-12-17T12:39:47.779+0000] {processor.py:154} INFO - Started process (PID=2809) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:47.807+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:39:47.811+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:47.810+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:47.892+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:48.022+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:48.021+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:39:48.133+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:48.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:39:48.433+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.667 seconds
[2022-12-17T12:39:58.782+0000] {processor.py:154} INFO - Started process (PID=2827) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:58.810+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:39:58.813+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:58.812+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:58.902+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:39:59.054+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:59.053+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:39:59.179+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:39:59.178+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:39:59.294+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.533 seconds
[2022-12-17T12:40:10.093+0000] {processor.py:154} INFO - Started process (PID=2837) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:10.107+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:40:10.119+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:10.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:10.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:10.361+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:10.360+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:40:10.476+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:10.475+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:40:10.625+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.547 seconds
[2022-12-17T12:40:21.535+0000] {processor.py:154} INFO - Started process (PID=2847) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:21.550+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:40:21.558+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:21.557+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:21.641+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:21.771+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:21.770+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:40:21.885+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:21.884+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:40:22.000+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.478 seconds
[2022-12-17T12:40:32.261+0000] {processor.py:154} INFO - Started process (PID=2857) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:32.310+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:40:32.315+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:32.314+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:32.402+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:32.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:32.526+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:40:32.651+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:32.651+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:40:32.801+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.556 seconds
[2022-12-17T12:40:43.169+0000] {processor.py:154} INFO - Started process (PID=2875) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:43.195+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:40:43.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:43.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:43.297+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:43.461+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:43.460+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:40:43.591+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:43.590+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:40:43.864+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.710 seconds
[2022-12-17T12:40:54.289+0000] {processor.py:154} INFO - Started process (PID=2885) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:54.317+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:40:54.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:54.321+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:54.411+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:40:54.959+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:54.958+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:40:55.075+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:40:55.074+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:40:55.198+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.922 seconds
[2022-12-17T12:41:05.825+0000] {processor.py:154} INFO - Started process (PID=2895) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:05.873+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:41:05.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:05.878+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:05.957+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:06.441+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:41:16.771+0000] {processor.py:154} INFO - Started process (PID=2905) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:16.811+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:41:16.815+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:16.814+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:16.916+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:17.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:17.232+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:41:17.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:17.362+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:41:17.483+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.727 seconds
[2022-12-17T12:41:27.768+0000] {processor.py:154} INFO - Started process (PID=2923) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:27.814+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:41:27.820+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:27.819+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:27.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:28.037+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:28.036+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:41:28.145+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:28.144+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:41:28.292+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.538 seconds
[2022-12-17T12:41:38.554+0000] {processor.py:154} INFO - Started process (PID=2933) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:38.608+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:41:38.612+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:38.611+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:38.693+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:38.822+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:38.821+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:41:38.937+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:38.936+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:41:39.057+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.517 seconds
[2022-12-17T12:41:49.370+0000] {processor.py:154} INFO - Started process (PID=2943) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:49.423+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:41:49.428+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:49.426+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:49.521+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:41:49.670+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:49.669+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:41:49.781+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:41:49.781+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:41:50.006+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.649 seconds
[2022-12-17T12:42:00.352+0000] {processor.py:154} INFO - Started process (PID=2961) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:00.381+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:42:00.385+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:00.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:00.560+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:00.861+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:00.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:42:01.074+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:01.073+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:42:01.220+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.890 seconds
[2022-12-17T12:42:11.682+0000] {processor.py:154} INFO - Started process (PID=2971) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:11.717+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:42:11.722+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:11.721+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:11.865+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:13.228+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:13.227+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:42:13.353+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:13.352+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:42:13.470+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.819 seconds
[2022-12-17T12:42:24.065+0000] {processor.py:154} INFO - Started process (PID=2981) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:24.093+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:42:24.098+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:24.097+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:24.180+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:24.470+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:24.469+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:42:24.580+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:24.580+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:42:24.716+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.668 seconds
[2022-12-17T12:42:34.938+0000] {processor.py:154} INFO - Started process (PID=2991) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:34.964+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:42:34.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:34.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:35.054+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:35.395+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:42:45.986+0000] {processor.py:154} INFO - Started process (PID=3009) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:46.024+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:42:46.036+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:46.035+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:46.167+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:46.933+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:46.932+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:42:47.161+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:47.160+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:42:47.282+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.353 seconds
[2022-12-17T12:42:57.585+0000] {processor.py:154} INFO - Started process (PID=3019) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:57.624+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:42:57.628+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:57.627+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:57.719+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:42:57.866+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:57.859+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:42:58.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:42:58.152+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:42:58.258+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.691 seconds
[2022-12-17T12:43:08.526+0000] {processor.py:154} INFO - Started process (PID=3029) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:08.550+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:43:08.554+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:08.553+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:08.650+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:09.330+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:09.329+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:43:09.449+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:09.449+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:43:09.583+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.071 seconds
[2022-12-17T12:43:20.004+0000] {processor.py:154} INFO - Started process (PID=3047) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:20.036+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:43:20.051+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:20.042+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:20.425+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:20.768+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:20.767+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:43:21.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:21.061+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:43:21.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.224 seconds
[2022-12-17T12:43:31.334+0000] {processor.py:154} INFO - Started process (PID=3058) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:31.354+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:43:31.358+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:31.357+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:31.446+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:31.618+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:31.617+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:43:31.923+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:31.922+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:43:32.201+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.881 seconds
[2022-12-17T12:43:42.498+0000] {processor.py:154} INFO - Started process (PID=3068) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:42.545+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:43:42.550+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:42.549+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:42.706+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:42.845+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:42.844+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:43:43.026+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:43.025+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:43:43.186+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.702 seconds
[2022-12-17T12:43:53.552+0000] {processor.py:154} INFO - Started process (PID=3078) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:53.594+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:43:53.605+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:53.604+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:53.745+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:43:53.935+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:53.934+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:43:54.056+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:43:54.055+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:43:54.162+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.624 seconds
[2022-12-17T12:44:04.464+0000] {processor.py:154} INFO - Started process (PID=3096) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:04.480+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:44:04.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:04.483+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:04.672+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:05.624+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:05.623+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:44:05.818+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:05.817+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:44:05.980+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.552 seconds
[2022-12-17T12:44:16.307+0000] {processor.py:154} INFO - Started process (PID=3106) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:16.331+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:44:16.337+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:16.334+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:16.438+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:17.083+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:17.082+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:44:17.281+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:17.280+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:44:17.388+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.103 seconds
[2022-12-17T12:44:27.656+0000] {processor.py:154} INFO - Started process (PID=3116) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:27.700+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:44:27.704+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:27.703+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:27.870+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:29.173+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:29.172+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:44:29.287+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:29.286+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:44:29.409+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.766 seconds
[2022-12-17T12:44:39.814+0000] {processor.py:154} INFO - Started process (PID=3133) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:39.843+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:44:39.855+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:39.846+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:40.086+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:40.279+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:40.278+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:44:40.399+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:40.398+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:44:40.584+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.796 seconds
[2022-12-17T12:44:51.105+0000] {processor.py:154} INFO - Started process (PID=3144) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:51.135+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:44:51.167+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:51.138+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:51.257+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:44:51.384+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:51.383+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:44:51.492+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:44:51.491+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:44:51.626+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.560 seconds
[2022-12-17T12:45:01.880+0000] {processor.py:154} INFO - Started process (PID=3154) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:01.906+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:45:01.920+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:01.919+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:02.007+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:02.688+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:02.687+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:45:02.805+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:02.805+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:45:02.917+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.052 seconds
[2022-12-17T12:45:13.170+0000] {processor.py:154} INFO - Started process (PID=3164) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:13.197+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:45:13.204+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:13.203+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:13.286+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:13.960+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:13.959+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:45:14.071+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:14.070+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:45:14.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.056 seconds
[2022-12-17T12:45:24.564+0000] {processor.py:154} INFO - Started process (PID=3182) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:24.610+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:45:24.618+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:24.617+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:24.746+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:25.135+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:25.134+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:45:25.606+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:25.598+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:45:25.759+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.211 seconds
[2022-12-17T12:45:36.197+0000] {processor.py:154} INFO - Started process (PID=3192) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:36.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:45:36.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:36.223+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:36.312+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:36.677+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:36.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:45:36.787+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:36.786+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:45:36.902+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.720 seconds
[2022-12-17T12:45:47.138+0000] {processor.py:154} INFO - Started process (PID=3202) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:47.182+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:45:47.187+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:47.186+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:47.267+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:48.384+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:45:58.686+0000] {processor.py:154} INFO - Started process (PID=3212) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:58.706+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:45:58.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:58.709+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:58.791+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:45:58.920+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:58.919+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:45:59.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:45:59.029+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:45:59.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.492 seconds
[2022-12-17T12:46:10.095+0000] {processor.py:154} INFO - Started process (PID=3230) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:10.141+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:46:10.156+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:10.155+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:10.392+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:10.573+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:10.572+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:46:10.709+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:10.707+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:46:11.012+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.939 seconds
[2022-12-17T12:46:21.173+0000] {processor.py:154} INFO - Started process (PID=3240) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:21.176+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:46:21.180+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:21.179+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:21.268+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:21.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:21.465+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:46:21.628+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:21.627+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:46:21.780+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.621 seconds
[2022-12-17T12:46:32.069+0000] {processor.py:154} INFO - Started process (PID=3250) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:32.072+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:46:32.077+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:32.076+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:32.169+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:32.315+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:32.314+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:46:32.422+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:32.421+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:46:32.541+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.486 seconds
[2022-12-17T12:46:42.929+0000] {processor.py:154} INFO - Started process (PID=3267) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:42.999+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:46:43.003+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:43.002+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:43.222+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:43.893+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:46:54.103+0000] {processor.py:154} INFO - Started process (PID=3278) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:54.132+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:46:54.136+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:54.135+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:54.256+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:46:55.290+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:55.289+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:46:55.405+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:46:55.405+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:46:55.545+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.456 seconds
[2022-12-17T12:47:05.887+0000] {processor.py:154} INFO - Started process (PID=3288) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:05.891+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:47:05.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:05.894+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:05.998+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:06.154+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:06.153+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:47:06.267+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:06.266+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:47:06.388+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.516 seconds
[2022-12-17T12:47:16.753+0000] {processor.py:154} INFO - Started process (PID=3298) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:16.765+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:47:16.769+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:16.768+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:16.872+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:17.181+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:17.180+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:47:17.402+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:17.401+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:47:17.542+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.808 seconds
[2022-12-17T12:47:27.947+0000] {processor.py:154} INFO - Started process (PID=3315) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:27.964+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:47:27.968+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:27.967+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:28.120+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:28.284+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:28.283+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:47:28.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:28.463+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:47:28.642+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.733 seconds
[2022-12-17T12:47:39.089+0000] {processor.py:154} INFO - Started process (PID=3325) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:39.092+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:47:39.096+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:39.095+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:39.181+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:39.654+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:39.653+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:47:39.764+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:39.763+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:47:39.902+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.827 seconds
[2022-12-17T12:47:50.264+0000] {processor.py:154} INFO - Started process (PID=3335) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:50.268+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:47:50.271+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:50.270+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:50.355+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:47:51.412+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:51.411+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:47:51.541+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:47:51.540+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:47:51.695+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.446 seconds
[2022-12-17T12:48:01.945+0000] {processor.py:154} INFO - Started process (PID=3345) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:01.972+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:48:01.978+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:01.977+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:02.060+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:02.483+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:48:12.928+0000] {processor.py:154} INFO - Started process (PID=3363) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:12.940+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:48:12.944+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:12.943+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:13.103+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:13.833+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:13.833+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:48:14.032+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:14.031+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:48:14.191+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.307 seconds
[2022-12-17T12:48:24.557+0000] {processor.py:154} INFO - Started process (PID=3373) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:24.560+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:48:24.564+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:24.563+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:24.645+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:24.850+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:24.849+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:48:24.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:24.968+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:48:25.111+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.568 seconds
[2022-12-17T12:48:35.371+0000] {processor.py:154} INFO - Started process (PID=3383) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:35.375+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:48:35.379+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:35.378+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:35.472+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:35.603+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:35.602+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:48:35.710+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:35.710+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:48:35.815+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.464 seconds
[2022-12-17T12:48:46.278+0000] {processor.py:154} INFO - Started process (PID=3399) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:46.281+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:48:46.294+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:46.293+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:46.436+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:46.787+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:46.786+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:48:46.913+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:46.912+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:48:47.040+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.799 seconds
[2022-12-17T12:48:57.619+0000] {processor.py:154} INFO - Started process (PID=3410) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:57.673+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:48:57.680+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:57.679+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:57.770+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:48:58.206+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:58.205+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:48:58.399+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:48:58.398+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:48:58.514+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.909 seconds
[2022-12-17T12:49:08.944+0000] {processor.py:154} INFO - Started process (PID=3420) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:08.986+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:49:09.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:09.033+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:09.163+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:09.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:09.640+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:49:09.750+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:09.749+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:49:09.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.038 seconds
[2022-12-17T12:49:20.197+0000] {processor.py:154} INFO - Started process (PID=3430) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:20.205+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:49:20.212+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:20.211+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:20.295+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:20.425+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:20.424+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:49:20.534+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:20.533+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:49:20.671+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.488 seconds
[2022-12-17T12:49:31.506+0000] {processor.py:154} INFO - Started process (PID=3448) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:31.539+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:49:31.547+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:31.542+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:31.637+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:31.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:31.801+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:49:31.923+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:31.922+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:49:32.052+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.560 seconds
[2022-12-17T12:49:43.000+0000] {processor.py:154} INFO - Started process (PID=3458) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:43.026+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:49:43.030+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:43.029+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:43.116+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:43.242+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:43.241+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:49:43.349+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:43.348+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:49:43.489+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.504 seconds
[2022-12-17T12:49:53.721+0000] {processor.py:154} INFO - Started process (PID=3468) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:53.742+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:49:53.745+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:49:53.745+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:53.830+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:49:55.050+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:50:05.440+0000] {processor.py:154} INFO - Started process (PID=3478) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:05.491+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:50:05.520+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:05.514+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:05.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:05.880+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:05.879+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:50:06.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:06.014+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:50:06.172+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.751 seconds
[2022-12-17T12:50:16.578+0000] {processor.py:154} INFO - Started process (PID=3496) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:16.591+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:50:16.595+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:16.594+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:16.676+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:16.808+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:16.807+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:50:16.924+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:16.923+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:50:17.040+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.475 seconds
[2022-12-17T12:50:27.512+0000] {processor.py:154} INFO - Started process (PID=3506) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:27.556+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:50:27.560+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:27.559+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:27.669+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:27.847+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:27.846+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:50:27.965+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:27.964+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:50:28.077+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.620 seconds
[2022-12-17T12:50:38.338+0000] {processor.py:154} INFO - Started process (PID=3516) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:38.365+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:50:38.368+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:38.367+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:38.453+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:38.677+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:38.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:50:38.825+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:38.824+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:50:38.980+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.656 seconds
[2022-12-17T12:50:49.221+0000] {processor.py:154} INFO - Started process (PID=3533) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:49.235+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:50:49.243+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:49.238+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:49.360+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:50:49.515+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:49.515+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:50:49.704+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:50:49.703+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:50:50.074+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.868 seconds
[2022-12-17T12:51:00.647+0000] {processor.py:154} INFO - Started process (PID=3543) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:00.675+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:51:00.685+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:00.684+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:00.818+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:01.360+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:01.359+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:51:01.517+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:01.516+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:51:01.648+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.029 seconds
[2022-12-17T12:51:11.880+0000] {processor.py:154} INFO - Started process (PID=3553) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:11.940+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:51:11.943+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:11.942+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:12.051+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:12.450+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:12.449+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:51:12.644+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:12.643+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:51:12.741+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.875 seconds
[2022-12-17T12:51:23.014+0000] {processor.py:154} INFO - Started process (PID=3563) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:23.017+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:51:23.021+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:23.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:23.106+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:23.271+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:23.270+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:51:23.400+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:23.398+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:51:23.538+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.538 seconds
[2022-12-17T12:51:33.936+0000] {processor.py:154} INFO - Started process (PID=3581) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:34.013+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:51:34.021+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:34.020+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:34.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:34.860+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:51:45.317+0000] {processor.py:154} INFO - Started process (PID=3591) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:45.371+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:51:45.375+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:45.374+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:45.493+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:46.059+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:46.058+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:51:46.190+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:46.189+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:51:46.343+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.041 seconds
[2022-12-17T12:51:56.656+0000] {processor.py:154} INFO - Started process (PID=3601) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:56.762+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:51:56.767+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:56.766+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:56.848+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:51:56.978+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:56.977+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:51:57.088+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:51:57.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:51:57.434+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.792 seconds
[2022-12-17T12:52:07.779+0000] {processor.py:154} INFO - Started process (PID=3618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:07.799+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:52:07.816+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:07.815+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:07.945+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:08.158+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:08.157+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:52:08.431+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:08.430+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:52:08.953+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.204 seconds
[2022-12-17T12:52:19.677+0000] {processor.py:154} INFO - Started process (PID=3629) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:19.720+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:52:19.724+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:19.723+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:19.900+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:20.476+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:20.475+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:52:20.615+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:20.614+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:52:20.736+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.137 seconds
[2022-12-17T12:52:30.991+0000] {processor.py:154} INFO - Started process (PID=3639) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:31.001+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:52:31.006+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:31.005+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:31.087+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:31.249+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:31.246+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:52:31.377+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:31.376+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:52:31.507+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.531 seconds
[2022-12-17T12:52:41.755+0000] {processor.py:154} INFO - Started process (PID=3649) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:41.772+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:52:41.776+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:41.775+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:41.859+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:42.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:42.018+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:52:42.131+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:42.130+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:52:42.359+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.621 seconds
[2022-12-17T12:52:52.881+0000] {processor.py:154} INFO - Started process (PID=3667) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:52.897+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:52:52.901+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:52.900+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:53.066+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:52:54.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:54.122+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:52:54.390+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:52:54.389+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:52:54.612+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.770 seconds
[2022-12-17T12:53:04.929+0000] {processor.py:154} INFO - Started process (PID=3677) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:04.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:53:04.954+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:04.953+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:05.096+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:05.531+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:05.530+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:53:05.734+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:05.733+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:53:05.843+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.936 seconds
[2022-12-17T12:53:16.324+0000] {processor.py:154} INFO - Started process (PID=3687) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:16.343+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:53:16.356+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:16.346+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:16.478+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:17.212+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:17.211+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:53:17.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:17.365+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:53:17.576+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.271 seconds
[2022-12-17T12:53:27.894+0000] {processor.py:154} INFO - Started process (PID=3704) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:27.922+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:53:27.926+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:27.925+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:28.118+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:29.531+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:29.530+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:53:29.746+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:29.745+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:53:29.936+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.058 seconds
[2022-12-17T12:53:40.324+0000] {processor.py:154} INFO - Started process (PID=3715) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:40.353+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:53:40.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:40.356+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:40.441+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:41.101+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:41.100+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:53:41.214+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:41.214+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:53:41.372+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.062 seconds
[2022-12-17T12:53:51.534+0000] {processor.py:154} INFO - Started process (PID=3725) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:51.568+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:53:51.572+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:51.571+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:51.667+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:53:52.017+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:52.016+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:53:52.139+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:53:52.139+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:53:52.281+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.762 seconds
[2022-12-17T12:54:02.893+0000] {processor.py:154} INFO - Started process (PID=3735) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:02.897+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:54:02.900+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:02.900+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:02.984+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:03.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:03.136+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:54:03.253+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:03.252+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:54:03.385+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.508 seconds
[2022-12-17T12:54:13.765+0000] {processor.py:154} INFO - Started process (PID=3753) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:13.772+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:54:13.792+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:13.790+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:14.038+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:14.317+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:14.316+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:54:14.547+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:14.546+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:54:14.881+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.134 seconds
[2022-12-17T12:54:25.249+0000] {processor.py:154} INFO - Started process (PID=3763) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:25.276+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:54:25.280+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:25.279+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:25.369+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:25.517+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:25.516+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:54:25.680+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:25.679+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:54:25.824+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.589 seconds
[2022-12-17T12:54:36.647+0000] {processor.py:154} INFO - Started process (PID=3773) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:36.668+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:54:36.677+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:36.676+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:36.765+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:38.315+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:38.314+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:54:38.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:38.465+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:54:38.590+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.958 seconds
[2022-12-17T12:54:49.208+0000] {processor.py:154} INFO - Started process (PID=3786) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:49.253+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:54:49.257+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:49.256+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:49.389+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:54:49.548+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:49.547+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:54:49.690+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:54:49.689+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:54:49.827+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.666 seconds
[2022-12-17T12:54:59.998+0000] {processor.py:154} INFO - Started process (PID=3801) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:00.011+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:55:00.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:00.014+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:00.120+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:00.953+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:00.953+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:55:01.066+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:01.065+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:55:01.204+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.220 seconds
[2022-12-17T12:55:11.492+0000] {processor.py:154} INFO - Started process (PID=3811) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:11.515+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:55:11.520+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:11.519+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:11.601+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:12.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:12.232+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:55:12.358+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:12.357+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:55:12.496+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.018 seconds
[2022-12-17T12:55:22.798+0000] {processor.py:154} INFO - Started process (PID=3821) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:22.817+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:55:22.820+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:22.819+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:22.904+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:23.233+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:23.232+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:55:23.356+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:23.355+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:55:23.468+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.684 seconds
[2022-12-17T12:55:33.905+0000] {processor.py:154} INFO - Started process (PID=3838) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:33.935+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:55:33.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:33.937+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:34.089+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:34.490+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:34.489+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:55:34.665+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:34.664+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:55:34.821+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.932 seconds
[2022-12-17T12:55:45.219+0000] {processor.py:154} INFO - Started process (PID=3848) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:45.256+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:55:45.261+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:45.260+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:45.359+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:45.499+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:45.498+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:55:45.608+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:45.607+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:55:45.718+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.515 seconds
[2022-12-17T12:55:56.120+0000] {processor.py:154} INFO - Started process (PID=3858) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:56.149+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:55:56.153+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:56.152+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:56.318+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:55:57.466+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:57.465+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:55:57.592+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:55:57.591+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:55:57.717+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.631 seconds
[2022-12-17T12:56:07.973+0000] {processor.py:154} INFO - Started process (PID=3868) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:08.004+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:56:08.008+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:08.007+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:08.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:08.970+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:56:19.572+0000] {processor.py:154} INFO - Started process (PID=3886) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:19.576+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:56:19.584+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:19.583+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:19.698+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:19.844+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:19.843+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:56:19.972+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:19.971+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:56:20.118+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.561 seconds
[2022-12-17T12:56:30.355+0000] {processor.py:154} INFO - Started process (PID=3896) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:30.359+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:56:30.363+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:30.362+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:30.448+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:30.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:30.597+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:56:30.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:30.873+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:56:31.022+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.681 seconds
[2022-12-17T12:56:41.354+0000] {processor.py:154} INFO - Started process (PID=3906) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:41.394+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:56:41.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:41.400+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:41.488+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:41.652+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:41.651+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:56:41.784+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:41.782+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:56:41.978+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.640 seconds
[2022-12-17T12:56:52.417+0000] {processor.py:154} INFO - Started process (PID=3923) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:52.453+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:56:52.465+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:52.464+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:52.639+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:56:52.889+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:52.888+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:56:53.083+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:56:53.082+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:56:53.588+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.210 seconds
[2022-12-17T12:57:04.156+0000] {processor.py:154} INFO - Started process (PID=3933) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:04.198+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:57:04.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:04.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:04.287+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:04.429+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:04.428+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:57:04.565+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:04.564+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:57:04.721+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.579 seconds
[2022-12-17T12:57:14.949+0000] {processor.py:154} INFO - Started process (PID=3943) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:14.953+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:57:14.956+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:14.955+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:15.040+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:15.215+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:15.214+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:57:15.342+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:15.341+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:57:15.532+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.600 seconds
[2022-12-17T12:57:25.867+0000] {processor.py:154} INFO - Started process (PID=3953) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:25.871+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:57:25.875+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:25.874+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:26.075+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:26.272+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:26.272+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:57:26.383+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:26.382+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:57:26.485+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.633 seconds
[2022-12-17T12:57:36.752+0000] {processor.py:154} INFO - Started process (PID=3971) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:36.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:57:36.796+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:36.795+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:36.944+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:37.181+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:37.180+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:57:37.317+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:37.316+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:57:37.584+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.847 seconds
[2022-12-17T12:57:47.923+0000] {processor.py:154} INFO - Started process (PID=3981) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:47.932+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:57:47.935+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:47.934+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:48.022+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:48.479+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:48.478+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:57:48.719+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:48.718+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:57:48.872+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.966 seconds
[2022-12-17T12:57:59.146+0000] {processor.py:154} INFO - Started process (PID=3991) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:59.312+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:57:59.319+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:59.314+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:59.429+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:57:59.707+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:59.706+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:57:59.902+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:57:59.901+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:58:00.010+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.893 seconds
[2022-12-17T12:58:10.372+0000] {processor.py:154} INFO - Started process (PID=4001) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:10.548+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:58:10.552+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:10.551+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:10.718+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:10.939+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:10.934+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:58:11.154+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:11.153+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:58:11.369+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.037 seconds
[2022-12-17T12:58:21.803+0000] {processor.py:154} INFO - Started process (PID=4019) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:21.828+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:58:21.832+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:21.831+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:21.934+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:22.373+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:22.372+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:58:22.508+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:22.507+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:58:22.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.855 seconds
[2022-12-17T12:58:32.928+0000] {processor.py:154} INFO - Started process (PID=4029) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:32.975+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:58:32.979+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:32.978+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:33.111+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:33.419+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:33.418+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:58:33.553+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:33.552+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:58:33.682+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.776 seconds
[2022-12-17T12:58:43.955+0000] {processor.py:154} INFO - Started process (PID=4039) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:44.001+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:58:44.005+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:44.004+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:44.097+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:44.280+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:44.279+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:58:44.487+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:44.486+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:58:44.594+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.653 seconds
[2022-12-17T12:58:55.101+0000] {processor.py:154} INFO - Started process (PID=4057) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:55.115+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:58:55.119+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:55.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:55.338+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:58:55.717+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:55.717+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:58:55.977+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:58:55.976+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:58:56.185+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.129 seconds
[2022-12-17T12:59:04.489+0000] {processor.py:154} INFO - Started process (PID=4070) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:04.505+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:59:04.509+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:04.508+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:04.617+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:04.748+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:04.747+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:59:04.856+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:04.855+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:59:04.990+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.514 seconds
[2022-12-17T12:59:15.725+0000] {processor.py:154} INFO - Started process (PID=4077) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:15.774+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:59:15.784+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:15.783+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:15.899+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:16.525+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:16.524+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:59:16.745+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:16.744+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:59:16.950+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.240 seconds
[2022-12-17T12:59:27.171+0000] {processor.py:154} INFO - Started process (PID=4090) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:27.198+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:59:27.203+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:27.201+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:27.390+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:28.855+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:28.854+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:59:29.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:29.033+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:59:29.356+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.197 seconds
[2022-12-17T12:59:39.631+0000] {processor.py:154} INFO - Started process (PID=4109) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:39.685+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:59:39.694+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:39.693+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:39.897+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:41.091+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T12:59:51.677+0000] {processor.py:154} INFO - Started process (PID=4119) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:51.718+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T12:59:51.722+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:51.721+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:51.807+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T12:59:51.957+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:51.957+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T12:59:52.080+0000] {logging_mixin.py:137} INFO - [2022-12-17T12:59:52.079+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T12:59:52.234+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.571 seconds
[2022-12-17T13:00:02.933+0000] {processor.py:154} INFO - Started process (PID=4129) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:02.937+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:00:02.941+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:02.940+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:03.025+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:03.195+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:03.194+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:00:03.323+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:03.322+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:00:03.466+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.549 seconds
[2022-12-17T13:00:13.831+0000] {processor.py:154} INFO - Started process (PID=4146) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:13.869+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:00:13.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:13.872+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:14.120+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:14.507+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:14.506+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:00:14.743+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:14.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:00:14.972+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.178 seconds
[2022-12-17T13:00:26.109+0000] {processor.py:154} INFO - Started process (PID=4157) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:26.113+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:00:26.116+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:26.115+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:26.201+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:26.889+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:26.888+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:00:27.088+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:27.087+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:00:27.281+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.184 seconds
[2022-12-17T13:00:37.512+0000] {processor.py:154} INFO - Started process (PID=4167) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:37.526+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:00:37.530+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:37.529+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:37.611+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:37.744+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:37.744+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:00:37.853+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:37.852+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:00:38.026+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.527 seconds
[2022-12-17T13:00:48.353+0000] {processor.py:154} INFO - Started process (PID=4177) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:48.356+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:00:48.360+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:48.360+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:48.451+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:48.864+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:48.863+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:00:48.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:48.985+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:00:49.194+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.855 seconds
[2022-12-17T13:00:59.611+0000] {processor.py:154} INFO - Started process (PID=4195) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:59.616+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:00:59.624+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:00:59.623+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:00:59.909+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:00.264+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:00.263+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:01:00.457+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:00.456+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:01:00.636+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.041 seconds
[2022-12-17T13:01:10.951+0000] {processor.py:154} INFO - Started process (PID=4205) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:10.979+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:01:10.984+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:10.983+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:11.071+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:11.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:11.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:01:11.360+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:11.360+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:01:11.480+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.543 seconds
[2022-12-17T13:01:22.176+0000] {processor.py:154} INFO - Started process (PID=4215) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:22.180+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:01:22.184+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:22.183+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:22.273+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:22.509+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:22.508+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:01:22.618+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:22.617+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:01:22.757+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.595 seconds
[2022-12-17T13:01:33.034+0000] {processor.py:154} INFO - Started process (PID=4225) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:33.037+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:01:33.041+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:33.040+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:33.143+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:33.320+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:33.320+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:01:33.637+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:33.618+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:01:33.921+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.900 seconds
[2022-12-17T13:01:44.393+0000] {processor.py:154} INFO - Started process (PID=4244) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:44.397+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:01:44.401+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:44.400+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:44.490+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:45.872+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:45.871+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:01:45.988+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:45.987+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:01:46.099+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.719 seconds
[2022-12-17T13:01:56.709+0000] {processor.py:154} INFO - Started process (PID=4254) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:56.746+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:01:56.776+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:56.775+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:56.857+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:01:56.990+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:56.989+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:01:57.117+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:01:57.116+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:01:57.256+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.563 seconds
[2022-12-17T13:02:07.482+0000] {processor.py:154} INFO - Started process (PID=4264) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:07.528+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:02:07.533+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:07.532+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:07.612+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:07.969+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:02:18.389+0000] {processor.py:154} INFO - Started process (PID=4282) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:18.431+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:02:18.480+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:18.434+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:18.586+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:18.873+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:18.872+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:02:19.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:19.001+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:02:19.137+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.763 seconds
[2022-12-17T13:02:29.404+0000] {processor.py:154} INFO - Started process (PID=4292) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:29.453+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:02:29.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:29.457+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:29.538+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:29.666+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:29.665+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:02:29.778+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:29.777+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:02:29.924+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.536 seconds
[2022-12-17T13:02:40.181+0000] {processor.py:154} INFO - Started process (PID=4302) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:40.202+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:02:40.206+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:40.205+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:40.373+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:40.530+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:40.529+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:02:40.676+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:40.676+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:02:40.794+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.627 seconds
[2022-12-17T13:02:51.176+0000] {processor.py:154} INFO - Started process (PID=4312) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:51.205+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:02:51.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:51.208+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:51.340+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:02:51.536+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:51.535+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:02:51.652+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:02:51.651+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:02:51.780+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.652 seconds
[2022-12-17T13:03:02.229+0000] {processor.py:154} INFO - Started process (PID=4330) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:02.253+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:03:02.265+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:02.264+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:02.586+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:03.202+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:03.201+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:03:03.428+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:03.427+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:03:03.556+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.369 seconds
[2022-12-17T13:03:13.879+0000] {processor.py:154} INFO - Started process (PID=4340) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:13.921+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:03:13.925+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:13.924+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:14.010+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:14.529+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:14.528+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:03:14.788+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:14.788+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:03:14.975+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.111 seconds
[2022-12-17T13:03:25.267+0000] {processor.py:154} INFO - Started process (PID=4350) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:25.301+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:03:25.305+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:25.304+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:25.390+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:25.549+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:25.548+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:03:25.715+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:25.714+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:03:26.087+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.837 seconds
[2022-12-17T13:03:36.342+0000] {processor.py:154} INFO - Started process (PID=4367) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:36.393+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:03:36.414+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:36.413+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:36.715+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:36.985+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:36.984+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:03:37.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:37.337+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:03:37.800+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.488 seconds
[2022-12-17T13:03:48.061+0000] {processor.py:154} INFO - Started process (PID=4378) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:48.083+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:03:48.087+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:48.086+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:48.171+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:48.334+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:48.333+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:03:48.461+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:48.460+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:03:48.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.551 seconds
[2022-12-17T13:03:58.873+0000] {processor.py:154} INFO - Started process (PID=4388) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:58.887+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:03:58.890+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:58.889+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:58.981+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:03:59.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:59.224+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:03:59.437+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:03:59.436+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:03:59.620+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.769 seconds
[2022-12-17T13:04:09.953+0000] {processor.py:154} INFO - Started process (PID=4398) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:10.001+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:04:10.005+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:10.004+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:10.192+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:10.400+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:10.399+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:04:10.618+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:10.614+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:04:10.768+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.830 seconds
[2022-12-17T13:04:21.257+0000] {processor.py:154} INFO - Started process (PID=4417) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:21.353+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:04:21.359+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:21.356+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:21.488+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:22.149+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:22.148+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:04:22.362+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:22.362+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:04:22.616+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.423 seconds
[2022-12-17T13:04:32.911+0000] {processor.py:154} INFO - Started process (PID=4427) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:33.027+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:04:33.032+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:33.030+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:33.114+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:33.241+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:33.240+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:04:33.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:33.356+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:04:33.523+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.627 seconds
[2022-12-17T13:04:43.781+0000] {processor.py:154} INFO - Started process (PID=4437) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:43.784+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:04:43.788+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:43.787+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:43.874+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:44.025+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:44.024+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:04:44.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:44.165+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:04:44.313+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.547 seconds
[2022-12-17T13:04:55.024+0000] {processor.py:154} INFO - Started process (PID=4447) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:55.027+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:04:55.031+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:55.030+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:55.196+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:04:55.390+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:55.389+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:04:55.545+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:04:55.544+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:04:55.648+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.636 seconds
[2022-12-17T13:05:06.336+0000] {processor.py:154} INFO - Started process (PID=4465) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:06.339+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:05:06.342+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:06.342+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:06.427+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:07.202+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:05:17.559+0000] {processor.py:154} INFO - Started process (PID=4475) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:17.563+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:05:17.568+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:17.567+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:17.708+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:18.801+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:18.800+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:05:18.997+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:18.996+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:05:19.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.620 seconds
[2022-12-17T13:05:30.264+0000] {processor.py:154} INFO - Started process (PID=4485) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:30.305+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:05:30.309+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:30.308+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:30.393+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:30.777+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:30.776+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:05:30.901+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:30.900+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:05:31.036+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.799 seconds
[2022-12-17T13:05:41.609+0000] {processor.py:154} INFO - Started process (PID=4503) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:41.662+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:05:41.666+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:41.665+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:41.911+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:42.113+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:42.112+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:05:42.298+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:42.297+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:05:42.465+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.890 seconds
[2022-12-17T13:05:52.879+0000] {processor.py:154} INFO - Started process (PID=4513) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:52.898+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:05:52.902+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:52.901+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:52.988+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:05:53.823+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:53.822+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:05:53.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:05:53.972+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:05:54.141+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.276 seconds
[2022-12-17T13:06:04.386+0000] {processor.py:154} INFO - Started process (PID=4523) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:04.390+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:06:04.400+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:04.400+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:04.489+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:05.203+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:05.202+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:06:05.337+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:05.336+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:06:05.510+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.139 seconds
[2022-12-17T13:06:15.773+0000] {processor.py:154} INFO - Started process (PID=4533) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:15.802+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:06:15.806+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:15.805+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:15.891+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:16.405+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:06:26.808+0000] {processor.py:154} INFO - Started process (PID=4550) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:26.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:06:26.852+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:26.851+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:27.044+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:27.816+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:27.815+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:06:28.032+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:28.031+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:06:28.149+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.374 seconds
[2022-12-17T13:06:38.483+0000] {processor.py:154} INFO - Started process (PID=4560) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:38.508+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:06:38.512+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:38.511+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:38.597+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:39.187+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:06:50.089+0000] {processor.py:154} INFO - Started process (PID=4570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:50.153+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:06:50.157+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:50.156+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:50.254+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:06:51.117+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:51.116+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:06:51.242+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:06:51.241+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:06:51.438+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.381 seconds
[2022-12-17T13:07:01.870+0000] {processor.py:154} INFO - Started process (PID=4588) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:01.887+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:07:01.902+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:01.901+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:02.193+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:02.610+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:02.609+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:07:02.745+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:02.744+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:07:03.000+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.199 seconds
[2022-12-17T13:07:13.304+0000] {processor.py:154} INFO - Started process (PID=4598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:13.307+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:07:13.312+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:13.310+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:13.404+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:13.773+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:07:24.688+0000] {processor.py:154} INFO - Started process (PID=4608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:24.702+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:07:24.707+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:24.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:24.829+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:24.982+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:24.981+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:07:25.134+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:25.132+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:07:25.247+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.574 seconds
[2022-12-17T13:07:36.021+0000] {processor.py:154} INFO - Started process (PID=4618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:36.071+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:07:36.075+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:36.074+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:36.164+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:36.887+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:36.886+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:07:37.013+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:37.012+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:07:37.165+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.164 seconds
[2022-12-17T13:07:47.604+0000] {processor.py:154} INFO - Started process (PID=4636) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:47.636+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:07:47.644+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:47.643+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:47.835+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:48.137+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:48.136+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:07:48.432+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:48.414+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:07:48.838+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.256 seconds
[2022-12-17T13:07:59.180+0000] {processor.py:154} INFO - Started process (PID=4646) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:59.184+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:07:59.188+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:07:59.187+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:07:59.272+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:00.841+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:00.840+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:08:00.956+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:00.955+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:08:01.100+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.933 seconds
[2022-12-17T13:08:11.952+0000] {processor.py:154} INFO - Started process (PID=4656) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:11.962+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:08:11.967+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:11.965+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:12.054+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:12.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:12.640+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:08:12.767+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:12.766+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:08:12.967+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.029 seconds
[2022-12-17T13:08:23.507+0000] {processor.py:154} INFO - Started process (PID=4673) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:23.520+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:08:23.523+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:23.522+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:23.721+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:24.365+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:24.364+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:08:24.529+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:24.528+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:08:24.748+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.287 seconds
[2022-12-17T13:08:35.102+0000] {processor.py:154} INFO - Started process (PID=4683) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:35.106+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:08:35.110+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:35.109+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:35.196+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:35.347+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:35.346+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:08:35.487+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:35.486+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:08:35.653+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.564 seconds
[2022-12-17T13:08:45.974+0000] {processor.py:154} INFO - Started process (PID=4693) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:46.013+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:08:46.018+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:46.017+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:46.098+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:46.537+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:08:56.930+0000] {processor.py:154} INFO - Started process (PID=4703) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:56.961+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:08:56.965+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:56.964+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:57.049+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:08:57.221+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:57.220+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:08:57.426+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:08:57.425+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:08:57.583+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.671 seconds
[2022-12-17T13:09:08.027+0000] {processor.py:154} INFO - Started process (PID=4720) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:08.031+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:09:08.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:08.037+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:08.220+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:08.486+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:08.485+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:09:08.938+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:08.937+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:09:09.172+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.160 seconds
[2022-12-17T13:09:19.630+0000] {processor.py:154} INFO - Started process (PID=4730) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:19.688+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:09:19.693+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:19.692+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:19.793+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:19.951+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:19.950+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:09:20.077+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:20.076+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:09:20.202+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.585 seconds
[2022-12-17T13:09:30.448+0000] {processor.py:154} INFO - Started process (PID=4740) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:30.470+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:09:30.473+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:30.472+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:30.556+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:30.718+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:30.717+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:09:30.840+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:30.839+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:09:30.983+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.549 seconds
[2022-12-17T13:09:41.419+0000] {processor.py:154} INFO - Started process (PID=4755) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:41.424+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:09:41.436+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:41.435+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:41.579+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:41.728+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:41.727+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:09:41.866+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:41.865+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:09:42.111+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.729 seconds
[2022-12-17T13:09:52.314+0000] {processor.py:154} INFO - Started process (PID=4766) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:52.345+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:09:52.350+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:52.348+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:52.445+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:09:53.475+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:53.472+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:09:53.617+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:09:53.616+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:09:53.728+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.427 seconds
[2022-12-17T13:10:04.005+0000] {processor.py:154} INFO - Started process (PID=4776) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:04.009+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:10:04.013+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:04.012+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:04.149+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:04.920+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:04.919+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:10:05.053+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:05.052+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:10:05.268+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.278 seconds
[2022-12-17T13:10:15.677+0000] {processor.py:154} INFO - Started process (PID=4786) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:15.712+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:10:15.738+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:15.737+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:16.123+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:16.441+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:16.440+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:10:16.565+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:16.565+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:10:16.724+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.088 seconds
[2022-12-17T13:10:27.039+0000] {processor.py:154} INFO - Started process (PID=4805) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:27.103+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:10:27.111+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:27.106+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:27.547+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:28.013+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:28.005+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:10:28.255+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:28.254+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:10:28.429+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.415 seconds
[2022-12-17T13:10:38.758+0000] {processor.py:154} INFO - Started process (PID=4815) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:38.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:10:38.778+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:38.777+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:39.141+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:39.430+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:39.429+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:10:39.562+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:39.558+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:10:39.720+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.976 seconds
[2022-12-17T13:10:50.032+0000] {processor.py:154} INFO - Started process (PID=4822) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:50.052+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:10:50.056+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:50.055+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:50.137+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:10:50.861+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:50.860+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:10:51.087+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:10:51.086+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:10:51.196+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.177 seconds
[2022-12-17T13:11:01.462+0000] {processor.py:154} INFO - Started process (PID=4839) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:01.487+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:11:01.495+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:01.490+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:01.593+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:01.973+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:01.972+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:11:02.774+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:02.773+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:11:03.001+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.557 seconds
[2022-12-17T13:11:13.513+0000] {processor.py:154} INFO - Started process (PID=4853) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:13.534+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:11:13.538+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:13.537+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:13.650+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:14.514+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:14.513+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:11:14.740+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:14.739+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:11:14.966+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.472 seconds
[2022-12-17T13:11:25.304+0000] {processor.py:154} INFO - Started process (PID=4863) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:25.329+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:11:25.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:25.337+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:25.604+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:25.882+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:25.881+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:11:26.034+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:26.033+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:11:26.211+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.962 seconds
[2022-12-17T13:11:36.486+0000] {processor.py:154} INFO - Started process (PID=4873) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:36.490+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:11:36.494+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:36.493+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:36.583+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:37.484+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:37.483+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:11:37.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:37.793+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:11:37.908+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.435 seconds
[2022-12-17T13:11:48.461+0000] {processor.py:154} INFO - Started process (PID=4890) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:48.509+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:11:48.525+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:48.524+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:48.629+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:11:48.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:48.968+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:11:49.686+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:11:49.685+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:11:49.977+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.550 seconds
[2022-12-17T13:12:00.231+0000] {processor.py:154} INFO - Started process (PID=4900) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:00.275+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:12:00.279+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:00.278+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:00.373+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:01.181+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:01.180+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:12:01.379+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:01.378+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:12:01.526+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.309 seconds
[2022-12-17T13:12:11.810+0000] {processor.py:154} INFO - Started process (PID=4910) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:11.835+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:12:11.839+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:11.838+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:11.951+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:12.509+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:12:23.111+0000] {processor.py:154} INFO - Started process (PID=4924) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:23.161+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:12:23.164+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:23.164+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:23.263+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:24.001+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:23.999+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:12:24.643+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:24.630+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:12:25.016+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.924 seconds
[2022-12-17T13:12:35.440+0000] {processor.py:154} INFO - Started process (PID=4938) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:35.464+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:12:35.470+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:35.469+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:35.589+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:35.838+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:35.837+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:12:35.999+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:35.995+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:12:36.304+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.879 seconds
[2022-12-17T13:12:46.652+0000] {processor.py:154} INFO - Started process (PID=4948) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:46.662+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:12:46.666+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:46.665+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:46.760+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:47.390+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:47.389+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:12:47.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:47.874+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:12:48.145+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.509 seconds
[2022-12-17T13:12:58.594+0000] {processor.py:154} INFO - Started process (PID=4960) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:58.654+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:12:58.663+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:58.657+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:58.770+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:12:59.010+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:59.009+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:12:59.205+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:12:59.204+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:12:59.512+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.968 seconds
[2022-12-17T13:13:09.779+0000] {processor.py:154} INFO - Started process (PID=4975) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:09.818+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:13:09.823+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:09.822+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:09.933+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:10.387+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:10.386+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:13:10.527+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:10.526+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:13:10.829+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.065 seconds
[2022-12-17T13:13:21.192+0000] {processor.py:154} INFO - Started process (PID=4982) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:21.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:13:21.216+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:21.215+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:21.299+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:21.436+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:21.435+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:13:21.550+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:21.549+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:13:21.936+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.759 seconds
[2022-12-17T13:13:32.361+0000] {processor.py:154} INFO - Started process (PID=4992) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:32.381+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:13:32.386+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:32.384+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:32.466+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:34.165+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:34.164+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:13:34.482+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:34.481+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:13:34.758+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.410 seconds
[2022-12-17T13:13:45.095+0000] {processor.py:154} INFO - Started process (PID=5012) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:45.131+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:13:45.148+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:45.134+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:45.345+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:45.591+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:45.582+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:13:45.883+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:45.876+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:13:46.191+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.114 seconds
[2022-12-17T13:13:56.617+0000] {processor.py:154} INFO - Started process (PID=5022) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:56.701+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:13:56.706+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:13:56.705+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:56.790+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:13:57.740+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:14:08.201+0000] {processor.py:154} INFO - Started process (PID=5035) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:08.205+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:14:08.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:08.208+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:08.300+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:08.685+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:08.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:14:09.057+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:09.056+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:14:09.440+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.252 seconds
[2022-12-17T13:14:19.798+0000] {processor.py:154} INFO - Started process (PID=5045) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:19.805+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:14:19.810+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:19.809+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:20.023+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:20.855+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:20.854+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:14:21.180+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:21.179+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:14:21.312+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.551 seconds
[2022-12-17T13:14:31.749+0000] {processor.py:154} INFO - Started process (PID=5063) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:31.860+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:14:31.864+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:31.863+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:32.076+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:32.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:32.464+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:14:32.719+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:32.714+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:14:33.208+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.487 seconds
[2022-12-17T13:14:43.712+0000] {processor.py:154} INFO - Started process (PID=5075) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:43.716+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:14:43.723+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:43.722+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:43.893+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:44.314+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:44.313+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:14:44.558+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:44.557+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:14:44.766+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.075 seconds
[2022-12-17T13:14:55.221+0000] {processor.py:154} INFO - Started process (PID=5085) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:55.250+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:14:55.253+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:55.252+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:55.363+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:14:56.002+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:56.001+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:14:56.152+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:14:56.151+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:14:56.402+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.203 seconds
[2022-12-17T13:15:06.716+0000] {processor.py:154} INFO - Started process (PID=5103) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:06.751+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:15:06.775+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:06.754+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:07.142+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:07.603+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:07.602+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:15:07.975+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:07.974+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:15:08.185+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.485 seconds
[2022-12-17T13:15:18.533+0000] {processor.py:154} INFO - Started process (PID=5113) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:18.585+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:15:18.596+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:18.595+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:18.929+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:19.229+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:19.228+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:15:19.419+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:19.418+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:15:19.545+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.026 seconds
[2022-12-17T13:15:29.845+0000] {processor.py:154} INFO - Started process (PID=5123) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:29.887+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:15:29.891+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:29.890+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:29.977+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:30.205+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:30.204+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:15:30.470+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:30.469+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:15:30.754+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.924 seconds
[2022-12-17T13:15:41.076+0000] {processor.py:154} INFO - Started process (PID=5133) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:41.433+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:15:41.438+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:41.437+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:41.522+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:41.794+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:41.793+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:15:42.062+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:42.060+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:15:42.234+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.175 seconds
[2022-12-17T13:15:52.799+0000] {processor.py:154} INFO - Started process (PID=5151) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:52.840+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:15:52.844+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:52.843+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:53.007+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:15:53.222+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:53.221+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:15:53.366+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:15:53.362+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:15:53.667+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.889 seconds
[2022-12-17T13:16:04.157+0000] {processor.py:154} INFO - Started process (PID=5161) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:04.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:16:04.224+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:04.223+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:04.310+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:04.686+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:04.684+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:16:04.905+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:04.904+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:16:05.095+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.952 seconds
[2022-12-17T13:16:15.451+0000] {processor.py:154} INFO - Started process (PID=5171) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:15.490+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:16:15.493+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:15.492+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:15.672+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:16.107+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:16.106+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:16:16.382+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:16.381+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:16:16.715+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.281 seconds
[2022-12-17T13:16:27.448+0000] {processor.py:154} INFO - Started process (PID=5189) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:27.452+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:16:27.470+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:27.469+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:27.813+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:28.904+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:28.888+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:16:29.506+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:29.505+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:16:30.076+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.693 seconds
[2022-12-17T13:16:41.030+0000] {processor.py:154} INFO - Started process (PID=5199) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:41.064+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:16:41.072+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:41.066+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:41.185+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:42.487+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:16:52.916+0000] {processor.py:154} INFO - Started process (PID=5209) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:52.943+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:16:52.947+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:52.946+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:53.078+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:16:54.709+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:54.693+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:16:54.894+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:16:54.893+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:16:55.054+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.164 seconds
[2022-12-17T13:17:05.313+0000] {processor.py:154} INFO - Started process (PID=5219) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:05.330+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:17:05.334+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:05.333+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:05.775+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:06.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:06.497+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:17:06.690+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:06.688+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:17:06.932+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.657 seconds
[2022-12-17T13:17:17.623+0000] {processor.py:154} INFO - Started process (PID=5236) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:17.669+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:17:17.673+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:17.672+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:17.929+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:18.142+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:18.140+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:17:18.275+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:18.274+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:17:18.467+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.921 seconds
[2022-12-17T13:17:29.020+0000] {processor.py:154} INFO - Started process (PID=5246) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:29.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:17:29.124+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:29.123+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:29.204+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:29.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:29.337+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:17:29.452+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:29.452+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:17:29.558+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.559 seconds
[2022-12-17T13:17:39.879+0000] {processor.py:154} INFO - Started process (PID=5256) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:39.898+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:17:39.902+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:39.901+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:40.009+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:40.372+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:40.371+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:17:40.515+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:40.514+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:17:40.676+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.821 seconds
[2022-12-17T13:17:50.926+0000] {processor.py:154} INFO - Started process (PID=5274) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:50.979+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:17:50.987+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:50.982+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:51.088+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:17:52.038+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:52.037+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:17:52.322+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:17:52.321+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:17:52.499+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.602 seconds
[2022-12-17T13:18:03.050+0000] {processor.py:154} INFO - Started process (PID=5284) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:03.079+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:18:03.084+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:03.083+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:03.193+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:04.063+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:04.062+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:18:04.172+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:04.172+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:18:04.295+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.270 seconds
[2022-12-17T13:18:14.537+0000] {processor.py:154} INFO - Started process (PID=5294) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:14.558+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:18:14.562+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:14.561+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:14.659+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:14.903+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:14.896+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:18:15.110+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:15.109+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:18:15.260+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.737 seconds
[2022-12-17T13:18:25.567+0000] {processor.py:154} INFO - Started process (PID=5304) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:25.617+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:18:25.622+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:25.620+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:25.701+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:27.473+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:27.464+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:18:27.701+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:27.700+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:18:27.873+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.320 seconds
[2022-12-17T13:18:38.329+0000] {processor.py:154} INFO - Started process (PID=5321) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:38.380+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:18:38.404+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:38.401+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:38.643+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:39.141+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:39.140+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:18:39.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:39.356+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:18:39.637+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.350 seconds
[2022-12-17T13:18:50.217+0000] {processor.py:154} INFO - Started process (PID=5331) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:50.236+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:18:50.240+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:50.239+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:50.406+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:18:50.677+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:50.676+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:18:50.912+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:18:50.911+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:18:51.236+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.051 seconds
[2022-12-17T13:19:01.640+0000] {processor.py:154} INFO - Started process (PID=5341) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:01.663+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:19:01.669+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:01.667+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:01.761+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:02.296+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:02.295+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:19:02.420+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:02.419+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:19:02.597+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.973 seconds
[2022-12-17T13:19:12.997+0000] {processor.py:154} INFO - Started process (PID=5357) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:13.073+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:19:13.081+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:13.080+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:13.186+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:13.387+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:13.386+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:19:13.542+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:13.541+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:19:13.976+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.021 seconds
[2022-12-17T13:19:24.353+0000] {processor.py:154} INFO - Started process (PID=5368) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:24.409+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:19:24.413+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:24.412+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:24.523+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:24.739+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:24.738+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:19:24.992+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:24.991+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:19:25.134+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.797 seconds
[2022-12-17T13:19:35.750+0000] {processor.py:154} INFO - Started process (PID=5378) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:35.789+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:19:35.793+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:35.792+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:35.888+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:36.389+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:36.388+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:19:36.529+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:36.528+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:19:36.675+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.948 seconds
[2022-12-17T13:19:47.061+0000] {processor.py:154} INFO - Started process (PID=5388) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:47.124+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:19:47.128+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:47.127+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:47.210+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:47.377+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:47.376+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:19:47.500+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:47.500+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:19:47.610+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.564 seconds
[2022-12-17T13:19:58.066+0000] {processor.py:154} INFO - Started process (PID=5406) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:58.116+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:19:58.121+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:58.120+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:58.225+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:19:59.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:59.457+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:19:59.578+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:19:59.578+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:19:59.739+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.698 seconds
[2022-12-17T13:20:10.120+0000] {processor.py:154} INFO - Started process (PID=5416) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:10.149+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:20:10.154+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:10.153+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:10.242+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:10.531+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:10.530+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:20:10.646+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:10.645+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:20:10.787+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.682 seconds
[2022-12-17T13:20:21.138+0000] {processor.py:154} INFO - Started process (PID=5426) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:21.186+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:20:21.191+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:21.190+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:21.273+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:21.403+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:21.403+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:20:21.528+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:21.528+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:20:21.649+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.525 seconds
[2022-12-17T13:20:31.988+0000] {processor.py:154} INFO - Started process (PID=5436) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:32.013+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:20:32.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:32.016+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:32.105+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:32.246+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:32.244+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:20:32.356+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:32.356+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:20:32.499+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.529 seconds
[2022-12-17T13:20:42.636+0000] {processor.py:154} INFO - Started process (PID=5455) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:42.688+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:20:42.704+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:42.703+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:42.881+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:43.331+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:43.330+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:20:43.520+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:43.519+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:20:43.763+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.145 seconds
[2022-12-17T13:20:54.146+0000] {processor.py:154} INFO - Started process (PID=5465) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:54.208+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:20:54.213+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:54.212+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:54.302+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:20:54.976+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:54.974+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:20:55.100+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:20:55.099+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:20:55.215+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.083 seconds
[2022-12-17T13:21:05.483+0000] {processor.py:154} INFO - Started process (PID=5475) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:05.532+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:21:05.540+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:21:05.539+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:05.631+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:06.539+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:21:16.846+0000] {processor.py:154} INFO - Started process (PID=5491) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:16.871+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:21:16.875+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:21:16.874+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:17.212+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:18.394+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:21:29.289+0000] {processor.py:154} INFO - Started process (PID=5502) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:29.354+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:21:29.358+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:21:29.357+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:29.719+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:30.974+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:21:41.175+0000] {processor.py:154} INFO - Started process (PID=5512) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:41.203+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:21:41.213+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:21:41.206+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:41.366+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:41.545+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:21:41.543+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:21:41.670+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:21:41.669+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:21:41.806+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.677 seconds
[2022-12-17T13:21:52.191+0000] {processor.py:154} INFO - Started process (PID=5522) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:52.250+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:21:52.254+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:21:52.253+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:52.343+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:21:53.198+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:22:03.718+0000] {processor.py:154} INFO - Started process (PID=5540) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:03.748+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:22:03.756+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:03.755+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:03.963+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:04.968+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:22:15.591+0000] {processor.py:154} INFO - Started process (PID=5550) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:15.619+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:22:15.623+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:15.622+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:15.721+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:16.040+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:16.039+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:22:16.337+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:16.336+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:22:16.610+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.050 seconds
[2022-12-17T13:22:27.070+0000] {processor.py:154} INFO - Started process (PID=5560) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:27.095+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:22:27.099+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:27.098+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:27.258+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:27.947+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:27.946+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:22:28.297+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:28.296+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:22:28.582+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.538 seconds
[2022-12-17T13:22:39.145+0000] {processor.py:154} INFO - Started process (PID=5570) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:39.211+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:22:39.216+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:39.214+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:39.424+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:40.209+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:40.207+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:22:40.464+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:40.463+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:22:40.837+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.729 seconds
[2022-12-17T13:22:51.268+0000] {processor.py:154} INFO - Started process (PID=5588) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:51.278+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:22:51.282+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:51.281+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:51.557+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:22:51.986+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:51.982+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:22:52.469+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:22:52.446+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:22:52.896+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.674 seconds
[2022-12-17T13:23:04.000+0000] {processor.py:154} INFO - Started process (PID=5598) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:04.010+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:23:04.015+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:04.014+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:04.192+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:04.492+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:04.491+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:23:04.641+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:04.640+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:23:04.835+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.860 seconds
[2022-12-17T13:23:15.521+0000] {processor.py:154} INFO - Started process (PID=5608) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:15.537+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:23:15.541+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:15.540+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:15.669+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:16.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:16.223+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:23:16.420+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:16.419+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:23:16.605+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.120 seconds
[2022-12-17T13:23:27.081+0000] {processor.py:154} INFO - Started process (PID=5618) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:27.092+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:23:27.096+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:27.095+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:27.245+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:27.606+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:27.605+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:23:27.789+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:27.788+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:23:28.004+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.963 seconds
[2022-12-17T13:23:38.727+0000] {processor.py:154} INFO - Started process (PID=5635) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:38.741+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:23:38.757+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:38.756+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:39.125+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:40.074+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:40.073+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:23:40.723+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:40.709+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:23:41.104+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.433 seconds
[2022-12-17T13:23:51.529+0000] {processor.py:154} INFO - Started process (PID=5645) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:51.559+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:23:51.572+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:51.571+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:51.910+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:23:52.203+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:52.199+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:23:52.660+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:23:52.659+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:23:52.932+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.460 seconds
[2022-12-17T13:24:03.763+0000] {processor.py:154} INFO - Started process (PID=5655) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:03.767+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:24:03.776+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:03.771+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:03.906+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:04.299+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:04.294+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:24:04.589+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:04.588+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:24:04.764+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.033 seconds
[2022-12-17T13:24:15.101+0000] {processor.py:154} INFO - Started process (PID=5665) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:15.108+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:24:15.112+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:15.110+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:15.277+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:15.458+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:15.457+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:24:15.613+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:15.612+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:24:15.813+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.742 seconds
[2022-12-17T13:24:26.633+0000] {processor.py:154} INFO - Started process (PID=5683) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:26.676+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:24:26.693+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:26.692+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:27.273+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:29.093+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:29.090+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:24:29.444+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:29.444+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:24:29.892+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 3.379 seconds
[2022-12-17T13:24:40.258+0000] {processor.py:154} INFO - Started process (PID=5693) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:40.262+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:24:40.286+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:40.285+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:40.548+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:41.974+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:24:52.438+0000] {processor.py:154} INFO - Started process (PID=5703) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:52.498+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:24:52.502+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:52.501+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:52.764+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:24:53.032+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:53.031+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:24:53.196+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:24:53.195+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:24:53.477+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.066 seconds
[2022-12-17T13:25:03.872+0000] {processor.py:154} INFO - Started process (PID=5713) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:03.880+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:25:03.889+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:03.888+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:04.049+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:04.245+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:04.244+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:25:04.396+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:04.394+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:25:04.592+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.745 seconds
[2022-12-17T13:25:15.371+0000] {processor.py:154} INFO - Started process (PID=5732) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:15.393+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:25:15.396+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:15.395+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:15.546+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:16.073+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:16.071+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:25:16.412+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:16.411+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:25:16.640+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.301 seconds
[2022-12-17T13:25:26.972+0000] {processor.py:154} INFO - Started process (PID=5742) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:27.117+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:25:27.121+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:27.120+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:27.325+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:27.844+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:27.843+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:25:28.190+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:28.189+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:25:28.547+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.591 seconds
[2022-12-17T13:25:39.197+0000] {processor.py:154} INFO - Started process (PID=5752) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:39.241+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:25:39.250+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:39.249+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:39.498+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:39.899+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:39.898+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:25:40.136+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:40.134+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:25:40.340+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.219 seconds
[2022-12-17T13:25:50.862+0000] {processor.py:154} INFO - Started process (PID=5771) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:50.960+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:25:51.011+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:50.998+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:51.540+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:25:52.065+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:52.062+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:25:52.316+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:25:52.310+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:25:52.668+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.870 seconds
[2022-12-17T13:26:03.417+0000] {processor.py:154} INFO - Started process (PID=5781) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:03.421+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:26:03.427+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:03.426+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:03.633+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:04.697+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:26:15.272+0000] {processor.py:154} INFO - Started process (PID=5791) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:15.279+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:26:15.286+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:15.286+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:15.406+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:15.570+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:15.569+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:26:15.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:15.710+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:26:15.864+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.607 seconds
[2022-12-17T13:26:26.954+0000] {processor.py:154} INFO - Started process (PID=5801) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:26.980+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:26:26.987+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:26.986+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:27.226+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:27.514+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:27.513+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:26:27.713+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:27.712+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:26:27.902+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.996 seconds
[2022-12-17T13:26:38.497+0000] {processor.py:154} INFO - Started process (PID=5819) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:38.506+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:26:38.521+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:38.520+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:38.795+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:40.206+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:40.205+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:26:40.746+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:40.745+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:26:41.216+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.802 seconds
[2022-12-17T13:26:52.126+0000] {processor.py:154} INFO - Started process (PID=5829) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:52.141+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:26:52.150+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:52.149+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:52.263+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:26:53.177+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:53.176+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:26:53.472+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:26:53.471+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:26:53.665+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.556 seconds
[2022-12-17T13:27:03.922+0000] {processor.py:154} INFO - Started process (PID=5839) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:03.941+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:27:03.946+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:03.945+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:04.037+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:04.176+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:04.175+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:27:04.336+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:04.335+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:27:04.681+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.773 seconds
[2022-12-17T13:27:14.978+0000] {processor.py:154} INFO - Started process (PID=5849) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:15.032+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:27:15.036+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:15.035+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:15.119+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:15.278+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:15.277+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:27:15.398+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:15.397+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:27:15.558+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.597 seconds
[2022-12-17T13:27:26.478+0000] {processor.py:154} INFO - Started process (PID=5867) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:26.498+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:27:26.501+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:26.500+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:26.842+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:28.454+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:27:38.895+0000] {processor.py:154} INFO - Started process (PID=5877) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:38.948+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:27:38.953+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:38.952+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:39.042+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:39.750+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:39.749+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:27:39.981+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:39.980+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:27:40.098+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.219 seconds
[2022-12-17T13:27:50.332+0000] {processor.py:154} INFO - Started process (PID=5887) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:50.353+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:27:50.357+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:50.356+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:50.457+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:27:51.406+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:51.405+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:27:51.657+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:27:51.656+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:27:51.808+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.514 seconds
[2022-12-17T13:28:02.944+0000] {processor.py:154} INFO - Started process (PID=5897) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:02.973+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:28:02.996+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:02.978+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:03.146+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:03.380+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:03.378+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:28:03.521+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:03.520+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:28:03.656+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.846 seconds
[2022-12-17T13:28:14.288+0000] {processor.py:154} INFO - Started process (PID=5915) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:14.313+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:28:14.325+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:14.324+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:14.464+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:15.879+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:15.878+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:28:15.994+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:15.993+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:28:16.118+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.861 seconds
[2022-12-17T13:28:26.932+0000] {processor.py:154} INFO - Started process (PID=5925) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:26.957+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:28:26.961+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:26.960+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:27.146+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:27.413+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:27.412+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:28:27.598+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:27.597+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:28:27.716+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.819 seconds
[2022-12-17T13:28:37.950+0000] {processor.py:154} INFO - Started process (PID=5935) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:37.965+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:28:37.969+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:37.968+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:38.084+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:39.116+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:39.115+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:28:39.234+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:39.233+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:28:39.365+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.429 seconds
[2022-12-17T13:28:49.845+0000] {processor.py:154} INFO - Started process (PID=5952) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:49.870+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:28:49.874+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:49.873+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:50.032+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:28:50.301+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:50.300+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:28:50.522+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:28:50.521+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:28:50.644+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.839 seconds
[2022-12-17T13:29:01.136+0000] {processor.py:154} INFO - Started process (PID=5962) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:01.158+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:29:01.162+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:01.161+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:01.462+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:01.671+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:01.670+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:29:02.149+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:02.148+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:29:02.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.179 seconds
[2022-12-17T13:29:13.061+0000] {processor.py:154} INFO - Started process (PID=5972) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:13.110+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:29:13.123+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:13.114+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:13.577+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:13.935+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:13.931+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:29:14.075+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:14.074+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:29:14.202+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.273 seconds
[2022-12-17T13:29:24.367+0000] {processor.py:154} INFO - Started process (PID=5982) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:24.383+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:29:24.391+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:24.390+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:24.510+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:24.690+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:24.689+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:29:24.854+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:24.853+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:29:25.109+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.766 seconds
[2022-12-17T13:29:35.956+0000] {processor.py:154} INFO - Started process (PID=6000) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:36.092+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:29:36.102+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:36.101+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:36.373+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:36.957+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:36.956+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:29:37.143+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:37.142+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:29:37.472+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.598 seconds
[2022-12-17T13:29:47.990+0000] {processor.py:154} INFO - Started process (PID=6010) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:48.006+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:29:48.011+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:48.010+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:48.110+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:48.402+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:29:59.274+0000] {processor.py:154} INFO - Started process (PID=6020) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:59.290+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:29:59.295+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:29:59.294+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:29:59.529+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:00.712+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:30:11.235+0000] {processor.py:154} INFO - Started process (PID=6038) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:11.254+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:30:11.287+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:30:11.269+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:11.577+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:12.536+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:30:23.550+0000] {processor.py:154} INFO - Started process (PID=6048) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:23.572+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:30:23.585+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:30:23.584+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:23.718+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:25.136+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:30:35.738+0000] {processor.py:154} INFO - Started process (PID=6058) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:35.773+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:30:35.789+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:30:35.788+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:35.974+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:37.094+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:30:37.093+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:30:37.398+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:30:37.397+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:30:37.653+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.951 seconds
[2022-12-17T13:30:48.212+0000] {processor.py:154} INFO - Started process (PID=6068) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:48.220+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:30:48.225+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:30:48.223+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:48.474+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:30:49.460+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:30:49.454+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:30:49.743+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:30:49.742+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:30:50.115+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.939 seconds
[2022-12-17T13:31:00.929+0000] {processor.py:154} INFO - Started process (PID=6086) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:00.975+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:31:00.981+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:00.980+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:01.364+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:02.565+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:02.565+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:31:02.802+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:02.801+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:31:03.074+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.183 seconds
[2022-12-17T13:31:13.421+0000] {processor.py:154} INFO - Started process (PID=6096) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:13.469+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:31:13.474+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:13.473+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:13.571+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:14.900+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:31:25.372+0000] {processor.py:154} INFO - Started process (PID=6106) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:25.421+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:31:25.425+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:25.424+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:25.698+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:26.749+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:26.748+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:31:26.948+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:26.942+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:31:27.267+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.915 seconds
[2022-12-17T13:31:37.808+0000] {processor.py:154} INFO - Started process (PID=6116) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:37.868+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:31:37.875+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:37.870+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:38.092+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:38.569+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:38.568+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:31:38.881+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:38.880+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:31:39.554+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.773 seconds
[2022-12-17T13:31:50.230+0000] {processor.py:154} INFO - Started process (PID=6134) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:50.238+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:31:50.242+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:50.241+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:50.539+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:31:51.338+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:51.337+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:31:51.950+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:31:51.949+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:31:52.226+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 2.032 seconds
[2022-12-17T13:32:02.886+0000] {processor.py:154} INFO - Started process (PID=6144) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:02.934+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:32:02.955+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:02.937+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:03.449+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:04.649+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:32:15.174+0000] {processor.py:154} INFO - Started process (PID=6154) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:15.209+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:32:15.221+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:15.220+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:15.514+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:16.278+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:16.277+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:32:16.494+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:16.493+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:32:16.711+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.583 seconds
[2022-12-17T13:32:27.109+0000] {processor.py:154} INFO - Started process (PID=6171) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:27.154+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:32:27.161+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:27.160+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:27.257+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:28.089+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:28.088+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:32:28.622+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:28.607+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:32:28.894+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.816 seconds
[2022-12-17T13:32:39.565+0000] {processor.py:154} INFO - Started process (PID=6182) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:39.598+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:32:39.602+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:39.601+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:39.786+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:40.335+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:40.334+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:32:40.528+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:40.527+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:32:40.725+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.194 seconds
[2022-12-17T13:32:51.037+0000] {processor.py:154} INFO - Started process (PID=6192) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:51.083+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:32:51.087+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:51.086+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:51.322+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:32:51.705+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:51.704+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:32:51.859+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:32:51.858+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:32:52.002+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.986 seconds
[2022-12-17T13:33:02.394+0000] {processor.py:154} INFO - Started process (PID=6202) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:02.442+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:33:02.446+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:02.445+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:02.542+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:02.712+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:02.711+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:33:03.019+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:03.018+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:33:03.298+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.918 seconds
[2022-12-17T13:33:13.558+0000] {processor.py:154} INFO - Started process (PID=6221) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:13.591+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:33:13.617+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:13.616+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:13.741+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:13.912+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:13.911+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:33:14.055+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:14.054+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:33:14.280+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.738 seconds
[2022-12-17T13:33:24.734+0000] {processor.py:154} INFO - Started process (PID=6231) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:24.774+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:33:24.779+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:24.778+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:24.867+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:25.013+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:25.012+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:33:25.188+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:25.187+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:33:25.299+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.579 seconds
[2022-12-17T13:33:35.620+0000] {processor.py:154} INFO - Started process (PID=6241) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:35.670+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:33:35.674+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:35.673+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:35.771+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:35.922+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:35.921+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:33:36.073+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:36.072+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:33:36.213+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.607 seconds
[2022-12-17T13:33:46.574+0000] {processor.py:154} INFO - Started process (PID=6251) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:46.629+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:33:46.633+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:46.632+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:46.749+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:46.894+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:46.893+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:33:47.014+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:47.013+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:33:47.144+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.598 seconds
[2022-12-17T13:33:57.571+0000] {processor.py:154} INFO - Started process (PID=6270) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:57.589+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:33:57.593+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:57.592+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:57.704+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:33:57.884+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:57.883+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:33:58.070+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:33:58.069+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:33:58.293+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.755 seconds
[2022-12-17T13:34:09.082+0000] {processor.py:154} INFO - Started process (PID=6280) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:09.115+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:34:09.119+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:09.118+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:09.217+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:09.472+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:09.471+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:34:09.622+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:09.621+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:34:09.747+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.679 seconds
[2022-12-17T13:34:20.305+0000] {processor.py:154} INFO - Started process (PID=6290) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:20.355+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:34:20.360+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:20.358+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:20.444+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:20.581+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:20.580+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:34:20.701+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:20.700+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:34:20.837+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.547 seconds
[2022-12-17T13:34:31.129+0000] {processor.py:154} INFO - Started process (PID=6300) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:31.160+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:34:31.166+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:31.165+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:31.266+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:32.380+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:34:42.857+0000] {processor.py:154} INFO - Started process (PID=6318) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:42.884+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:34:42.892+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:42.891+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:42.997+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:43.160+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:43.159+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:34:43.293+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:43.292+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:34:43.473+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.631 seconds
[2022-12-17T13:34:53.755+0000] {processor.py:154} INFO - Started process (PID=6328) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:53.902+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:34:53.906+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:34:53.905+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:54.013+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:34:55.255+0000] {processor.py:179} ERROR - Got an exception! Propagating...
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 175, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 156, in _handle_dag_file_processing
    result: tuple[int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 75, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/dag_processing/processor.py", line 781, in process_file
    dagbag.sync_to_db(processor_subdir=self._dag_directory, session=session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 644, in sync_to_db
    for attempt in run_with_db_retries(logger=self.log):
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 384, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 362, in iter
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.8/site-packages/tenacity/__init__.py", line 195, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 437, in result
    return self.__get_result()
  File "/usr/local/lib/python3.8/concurrent/futures/_base.py", line 389, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 656, in sync_to_db
    serialize_errors.extend(_serialize_dag_capturing_errors(dag, session))
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 627, in _serialize_dag_capturing_errors
    dag_was_updated = SerializedDagModel.write_dag(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 72, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/serialized_dag.py", line 146, in write_dag
    session.query(literal(True))
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2810, in first
    return self.limit(1)._iter().first()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2894, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1688, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1529, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 747, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3197, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3276, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3246, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2100, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3243, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 310, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 868, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 476, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 256, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 371, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 666, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 207, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 590, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 584, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: FATAL:  password authentication failed for user "airflow"

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2022-12-17T13:35:05.660+0000] {processor.py:154} INFO - Started process (PID=6338) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:35:05.716+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:35:05.721+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:35:05.719+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:35:05.821+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:35:06.118+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:35:06.117+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:35:06.378+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:35:06.377+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:35:06.517+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 0.874 seconds
[2022-12-17T13:35:16.869+0000] {processor.py:154} INFO - Started process (PID=6356) to work on /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:35:16.938+0000] {processor.py:756} INFO - Processing file /opt/airflow/dags/spark_etl/spark_dags.py for tasks to queue
[2022-12-17T13:35:16.945+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:35:16.944+0000] {dagbag.py:537} INFO - Filling up the DagBag from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:35:17.129+0000] {processor.py:766} INFO - DAG(s) dict_keys(['spark_etl']) retrieved from /opt/airflow/dags/spark_etl/spark_dags.py
[2022-12-17T13:35:18.286+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:35:18.285+0000] {dag.py:2585} INFO - Sync 1 DAGs
[2022-12-17T13:35:18.468+0000] {logging_mixin.py:137} INFO - [2022-12-17T13:35:18.466+0000] {dag.py:3336} INFO - Setting next_dagrun for spark_etl to None, run_after=None
[2022-12-17T13:35:18.609+0000] {processor.py:176} INFO - Processing /opt/airflow/dags/spark_etl/spark_dags.py took 1.784 seconds
